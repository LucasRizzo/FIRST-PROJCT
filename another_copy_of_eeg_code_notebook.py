# -*- coding: utf-8 -*-
"""Another copy of EEG CODE_notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nvhYMYD_dPC0gVdFXoJcUwj3h2R2ABHN

source localisation: https://www.sciencedirect.com/science/article/abs/pii/B9780444640321000060

https://brain-resources.netlify.app/

mutiple resource theory (https://en.wikipedia.org/wiki/Workload#/media/File:KTS1workload.jpg)
https://nist.mni.mcgill.ca/cerebra/ Atlas
https://mindboggle.readthedocs.io/en/latest/labels.html

![image.png](data:image/png;base64,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)

https://lucalongo.eu/MRT/CR-MR-mapping/

# Imports
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

import os

import statistics
!pip install nilearn
from nilearn.plotting.cm import cold_hot
from nilearn import datasets, plotting # Import datasets from nilearn
from nilearn.image import new_img_like # Import new_img_like from nilearn.image

#!git clone https://github.com/kdotdot/cerebra_atlas_python.git
!git clone https://github.com/marowakamalaldeen/Cerebra_aatlas_python.git
!pip install -r Cerebra_aatlas_python/requirements.txt
!pip install --editable Cerebra_aatlas_python/
!apt-get update
!apt-get install texlive texlive-latex-extra texlive-fonts-recommended dvipng cm-super -y --fix-missing

"""#  Load data"""

videodata1= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/First data/video1_eLORETA.npy") #marowa's drive video1
videodata= np.load("/content/drive/MyDrive/Data TU PHD DUBLIN/First data/baseline_eLORETA.npy")  #base relax
videodata2= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/First data/video2_eLORETA.npy") #video2
videodata3= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/First data/video3_eLORETA.npy") #video3
# Load the source space atlas (mapping voxel data)
map_voxel = np.load("/content/drive/MyDrive/Data TU PHD DUBLIN/LABEL DETAILS/map_voxel.npy")
region_data = pd.read_csv("/content/drive/MyDrive/Data TU PHD DUBLIN/LABEL DETAILS/results_per_region (MY 101124).csv")
file_path = ('f"/content/drive/MyDrive/Data TU PHD DUBLIN/subjects Data/{subject_id}/evaluation/{video_type}_eLORETA.npy"')
#luca's drive
#pathLuca = "/content/drive/MyDrive/TU Dublin-NN/research/PHD CANDIDATES - AICL research LAB/CURRENT/Carlos Gomez Tapia - PB04482/dataset - full source activation space (31000x11250) for 35 subjects/"
#fileName = "video1_eLORETA.npy"
#videodata= np.load (pathLuca + fileName)
print(videodata1.shape)
print(len(videodata)) #len of points in brain (31553)
print(videodata[0])
print(len(videodata[0]))  #timestamps  (11250=125hz x 90 seconds)
print(type(videodata))
#video2data is a list of list (first list 31553 lenghth,each of this cell contains 11250 point,31553 is represented the number of the brain and the 11250 is the point in time  )

"""#All Function"""

def load_subject_video_data(subject_id, video_type):
    """
    Load brain activation data for a given subject and video type.
    """
    file_path = f"/content/drive/MyDrive/Data TU PHD DUBLIN/subjects Data/{subject_id}/evaluation/{video_type}.npy"
    print(f"🔍 Checking file path: {file_path}")  # Debugging print
    if not os.path.exists(file_path):
        print(f"❌ Warning: File not found for subject {subject_id}, video type {video_type}. Returning dummy data.")
        return np.random.randn(31335, 11250)  # Dummy fallback data
    return np.load(file_path)

def compute_rms_time_series1(videodata, map_voxel, region_data):
    """
    Compute the RMS for a given video dataset across all time locations,
    sorting 31,553 points into regions based on Cerebra ID,
    calculating RMS per time activation for each region,
    and visualizing them separately over time.
    """
    if "Cerebra_ID" not in region_data.columns or "Region_name" not in region_data.columns:
        raise ValueError("CSV file must contain 'Cerebra_ID' and 'Region_name' columns.")

    valid_cerebra_ids = set(region_data["Cerebra_ID"].unique())
    region_name_map = region_data.set_index("Cerebra_ID")["Region_name"].to_dict()

    avg_rms_time_series = {region: np.zeros(videodata.shape[1]) for region in valid_cerebra_ids}
    count_voxels = {region: np.zeros(videodata.shape[1]) for region in valid_cerebra_ids}

    for time_location in range(videodata.shape[1]):
        videodata_selected = videodata[:, time_location]
        voxel_rms = np.sqrt(videodata_selected**2)

        for region in valid_cerebra_ids:
            region_indices = np.where(map_voxel == region)[0]
            if len(region_indices) == 0:
                continue

            rms_values = voxel_rms[region_indices]
            avg_rms_time_series[region][time_location] += np.sum(rms_values)
            count_voxels[region][time_location] += len(rms_values)

    for region in valid_cerebra_ids:
        valid_indices = count_voxels[region] > 0
        avg_rms_time_series[region][valid_indices] /= count_voxels[region][valid_indices]

    return avg_rms_time_series, region_name_map

def plot_rms_time_series1(avg_rms_time_series, region_name_map, subject_id, video_type):
    """
    Visualize each region's RMS separately in 102 diagrams with both Region Name and Cerebra ID,
    and include Subject ID and Video Type in titles.
    """
    for region, rms_values in avg_rms_time_series.items():
        region_name = region_name_map.get(region, f"Region {region}")
        plt.figure(figsize=(10, 5))
        plt.plot(range(len(rms_values)), rms_values, label=f"{region_name} (ID: {region})")
        plt.xlabel("Time Activation (0 - 11250)")
        plt.ylabel("RMS Value")
        plt.title(f"{region_name} (ID: {region})\nSubject: {subject_id} | Video Type: {video_type}")
        plt.legend()
        plt.grid(axis="y", linestyle="--", alpha=0.6)
        plt.show()

def load_subject_video_data1():
    """
    Load brain activation data for a given subject and video type.
    Allows user to enter the file path manually.
    """
    file_path = input("Enter the full file path for the video data (.npy file): ").strip()

    if not os.path.exists(file_path):
        print(f"❌ Warning: File not found at {file_path}. Returning dummy data.")
        return np.random.randn(31335, 11250)  # Dummy fallback data

    # Check if the provided path is a directory. If so, list the .npy files within it.
    if os.path.isdir(file_path):
        npy_files = [f for f in os.listdir(file_path) if f.endswith(".npy")]
        if npy_files:
            print("Found the following .npy files in the directory:")
            for i, file in enumerate(npy_files):
                print(f"{i + 1}. {file}")
            file_index = int(input(f"Select the file to load (1-{len(npy_files)}): ")) - 1
            file_path = os.path.join(file_path, npy_files[file_index])
        else:
            print(f"❌ Error: No .npy files found in the directory {file_path}. Returning dummy data.")
            return np.random.randn(31335, 11250)

    return np.load(file_path)



def load_region_data1():
    """
    Load the region mapping data (CSV file) manually entered by the user.
    """
    csv_path = input("Enter the full file path for the region CSV file: ").strip()

    if not os.path.exists(csv_path):
        raise FileNotFoundError(f"❌ Error: CSV file not found at {csv_path}. Please check the path.")

    return pd.read_csv(csv_path)

def compute_rms_time_series2(videodata, map_voxel, region_data):
    """
    Sorts 31,553 points into regions based on Cerebra ID,
    Compute the RMS for a given video dataset across all time locations.
    calculating RMS per time activation for each region,
    and visualizing them separately over time.
    """
    required_columns = {"Cerebra_ID", "Region_name"}
    missing_columns = required_columns - set(region_data.columns)

    if missing_columns:
        raise ValueError(f"❌ CSV file must contain the following columns: {missing_columns}")

    valid_cerebra_ids = set(region_data["Cerebra_ID"].unique())
    region_name_map = region_data.set_index("Cerebra_ID")["Region_name"].to_dict()

    avg_rms_time_series = {region: np.zeros(videodata.shape[1]) for region in valid_cerebra_ids}
    count_voxels = {region: np.zeros(videodata.shape[1]) for region in valid_cerebra_ids}

    for time_location in range(videodata.shape[1]):
        videodata_selected = videodata[:, time_location]
        voxel_rms = np.sqrt(videodata_selected**2)

        for region in valid_cerebra_ids:
            region_indices = np.where(map_voxel == region)[0]
            if len(region_indices) == 0:
                continue

            rms_values = voxel_rms[region_indices]
            avg_rms_time_series[region][time_location] += np.sum(rms_values)
            count_voxels[region][time_location] += len(rms_values)

    for region in valid_cerebra_ids:
        valid_indices = count_voxels[region] > 0
        avg_rms_time_series[region][valid_indices] /= count_voxels[region][valid_indices]

    return avg_rms_time_series, region_name_map

def plot_rms_time_series2(avg_rms_time_series, region_name_map):
    """
    Visualize each region's RMS separately in 102 diagrams with both Region Name and Cerebra ID.
    """
    for region, rms_values in avg_rms_time_series.items():
        region_name = region_name_map.get(region, f"Region {region}")
        plt.figure(figsize=(10, 5))
        plt.plot(range(len(rms_values)), rms_values, label=f"{region_name} (ID: {region})")
        plt.xlabel("Time Activation (0 - 11250)")
        plt.ylabel("RMS Value")
        plt.title(f"{region_name} (ID: {region})")
        plt.legend()
        plt.grid(axis="y", linestyle="--", alpha=0.6)
        plt.show()




def load_video_data():
    """
    Manually enter the file path to load brain activation data.
    """
    file_path = input("Enter the full file path for the video data (.npy file): ").strip()
    if not os.path.exists(file_path):
        print(f"❌ Warning: File not found at {file_path}. Returning dummy data.")
        return np.random.randn(31335, 11250)  # Dummy fallback data
    return np.load(file_path)

def load_csv_data():
    """
    Manually enter the file path to load the CSV file.
    """
    file_path = input("Enter the full file path for the CSV file: ").strip()
    if not os.path.exists(file_path):
        print(f"❌ Warning: File not found at {file_path}. Please enter a valid path.")
        return None
    return pd.read_csv(file_path)

def compute_rms_time_series3(videodata, map_voxel, region_data):
    """
    Compute the RMS for each region along all 11250 time activations,
    sort each region to its corresponding MRT based on the CSV file,
    compute the average RMS per MRT, and prepare data for visualization.
    """
    required_columns = {"Cerebra_ID", "Region_name", "Multiple resource theory ID", "MRT ID Name"}
    missing_columns = required_columns - set(region_data.columns)
    if missing_columns:
        raise ValueError(f"CSV file must contain the following columns: {missing_columns}")

    valid_cerebra_ids = set(region_data["Cerebra_ID"].unique())
    region_name_map = region_data.set_index("Cerebra_ID")["Region_name"].to_dict()
    mrt_id_map = region_data.set_index("Cerebra_ID")["Multiple resource theory ID"].to_dict()
    mrt_name_map = region_data.set_index("Multiple resource theory ID")["MRT ID Name"].to_dict()

    mrt_regions_map = {}
    rms_per_mrt = {}
    avg_rms_per_mrt = {}

    for cerebra_id, mrt_id in mrt_id_map.items():
        region_name = region_name_map.get(cerebra_id, f"Region {cerebra_id}")
        if mrt_id not in mrt_regions_map:
            mrt_regions_map[mrt_id] = []
        mrt_regions_map[mrt_id].append(region_name)

    rms_per_region = {cerebra_id: np.sqrt(np.mean(videodata[map_voxel == cerebra_id]**2, axis=0)) for cerebra_id in valid_cerebra_ids}

    for cerebra_id, mrt_id in mrt_id_map.items():
        if mrt_id not in rms_per_mrt:
            rms_per_mrt[mrt_id] = []
        rms_per_mrt[mrt_id].append(rms_per_region.get(cerebra_id, np.zeros(11250)))

    for mrt_id, rms_values_list in rms_per_mrt.items():
        avg_rms_per_mrt[mrt_id] = np.mean(rms_values_list, axis=0)

    mwl_index = np.sum(list(avg_rms_per_mrt.values()), axis=0)

    return avg_rms_per_mrt, mwl_index, mrt_regions_map, mrt_name_map

def plot_mrt_time_series3(rms_per_mrt, mwl_index, mrt_regions_map, mrt_name_map):
    """
    Plot the average time series for each MRT ID separately in red,
    and plot the overall average RMS in green.
    """
    for mrt_id, avg_rms_values in rms_per_mrt.items():
        mrt_name = mrt_name_map.get(mrt_id, "Unknown MRT Name")
        regions_str = ", ".join(mrt_regions_map.get(mrt_id, []))

        plt.figure(figsize=(16, 5))
        plt.plot(range(11250), avg_rms_values, color='red', label=f"{mrt_name} (MRT ID {mrt_id})")
        plt.xlabel("Time (0 - 11250)")
        plt.ylabel("Average RMS Value")
        plt.title(f"Average RMS for {mrt_name} (MRT ID {mrt_id})\nRegions: {regions_str}")
        plt.legend(loc='upper right', fontsize=8)
        plt.grid(axis="y", linestyle="--", alpha=0.6)
        plt.show()

    plt.figure(figsize=(10, 5))
    plt.plot(range(11250), mwl_index, color='green', label="MWL Index ")
    plt.xlabel("Time (0 - 11250)")
    plt.ylabel("Sum of Averages of RMS of cortical regions")
    plt.title("Mental Workload Index")
    plt.legend()
    plt.grid(axis="y", linestyle="--", alpha=0.6)
    plt.show()

"""# Details of Data all subject

Subject Loop
Cerebra ID Loop
Time voxel activation Loop
"""

#Subject Loop Cerebra ID Loop Time voxel activation Loop

# Validate required columns exist
if "Cerebra_ID" not in region_data.columns or "Region_name" not in region_data.columns:
    raise ValueError("CSV file must contain 'Cerebra_ID' and 'Region_name' columns.")

# Create mapping from Cerebra_ID to Region_name
mapping_region_id_to_name = region_data.set_index("Cerebra_ID")["Region_name"].to_dict()
# --------------------- Helper Functions ---------------------

def load_subject_video_data(subject_id, video_type):
    """
    Load brain activation data for a given subject and video type.

    Parameters:
        subject_id (str): Subject ID (e.g., 'NDARZY502FAG')
        video_type (str): Video type (e.g., "baseline", "video1", "video2", "video3")

    Returns:
        np.ndarray: Brain voxel activation data.
    """
    file_path = f"/content/drive/MyDrive/Data TU PHD DUBLIN/subjects Data/{subject_id}/evaluation/{video_type}_eLORETA.npy"
    return np.load(file_path)


def get_number_of_voxel_per_region(cerebra_id):
    """
    Get the number of voxels for a given cerebral region (Cerebra ID) and its region name.

    Parameters:
        cerebra_id (int): The ID of the cerebral region.

    Returns:
        tuple: (voxel_count (int), region_name (str))
    """
    voxel_count = np.sum(map_voxel == cerebra_id)  # Count occurrences of the Cerebra ID
    region_name = mapping_region_id_to_name.get(cerebra_id, "Unknown Region")
    return voxel_count, region_name


def plot_voxel_activation_for_region(time_location, cerebra_id, video_data):
    """
    Plot voxel activation histograms for the selected region and time location,
    for each video type in video_data.

    Parameters:
        time_location (int): The time index to analyze.
        cerebra_id (int): The Cerebra ID for the region of interest.
        video_data (dict): Dictionary of video datasets.
    """
    # Get indices for voxels in the desired region.
    region_indices = np.where(map_voxel == cerebra_id)[0]

    plt.figure(figsize=(12, 5))
    for video_label, data in video_data.items():
        # Restrict activation data to the region of interest and given time
        voxel_activation = data[region_indices, time_location]
        plt.hist(voxel_activation, bins=100, alpha=0.5, label=f"{video_label} - Time {time_location}")

    plt.title(f"Voxel Activation Distribution for Region {cerebra_id} at Time {time_location}")
    plt.xlabel("Voxel Activation Value")
    plt.ylabel("Frequency")
    plt.legend()
    plt.show()


def plot_number_of_voxel_per_region():
    """
    Plot a histogram showing the overall voxel counts per cortical region.
    """
    unique_regions = np.unique(map_voxel)
    plt.figure(figsize=(20, 5))
    plt.hist(map_voxel, bins=len(unique_regions), color="royalblue", alpha=0.7)
    plt.title("Number of Voxels per Cortical Region")
    plt.xlabel("Cortical Regions")
    plt.ylabel("Number of Voxels")
    plt.show()


# --------------------- Main Loop ---------------------

program_exit = False  # Flag to control overall program exit

while not program_exit:
    # ----- Outer Loop: Subject Selection -----
    subject_input = input("\nEnter Subject ID (or type 'exit' to quit): ").strip()
    if subject_input.lower() == "exit":
        program_exit = True
        break  # Exit the outer loop

    subject_id = subject_input  # Use the entered subject ID

    # Load video data for the subject
    try:
        video_data = {
            "Baseline": load_subject_video_data(subject_id, "baseline"),
            "Video1": load_subject_video_data(subject_id, "video1"),
            "Video2": load_subject_video_data(subject_id, "video2"),
            "Video3": load_subject_video_data(subject_id, "video3"),
        }
    except Exception as e:
        print(f"Error loading video data for subject {subject_id}: {e}")
        continue  # Ask for a new subject ID

    # Optionally, plot the overall voxel distribution per cortical region
    plot_number_of_voxel_per_region()

    # ----- Middle Loop: Cerebra ID Analysis -----
    while True:
        cerebra_input = input("\nEnter Cerebra ID (or type 'back' to choose another subject, or 'exit' to quit): ").strip()
        if cerebra_input.lower() == "back":
            print("Returning to subject selection.")
            break  # Return to the subject selection loop
        if cerebra_input.lower() == "exit":
            program_exit = True
            break  # Exit the middle loop, then outer loop

        try:
            cerebra_id = int(cerebra_input)
            voxel_count, region_name = get_number_of_voxel_per_region(cerebra_id)
            if voxel_count == 0:
                print(f"⚠️ Region '{region_name}' (ID: {cerebra_id}) has 0 voxels. Please try another Cerebra ID.")
                continue  # Remain in the Cerebra ID loop
            else:
                print(f"✅ Selected Region: {region_name} (ID: {cerebra_id}) with {voxel_count} voxels.")
        except ValueError:
            print("❌ Invalid input. Please enter a valid Cerebra ID (an integer).")
            continue  # Remain in the Cerebra ID loop

        # ----- Inner Loop: Time Location Analysis -----
        while True:
            time_input = input("\nEnter a Time Location (or type 'back' to choose another Cerebra ID, or 'exit' to quit): ").strip()
            if time_input.lower() == "back":
                print("Returning to Cerebra ID selection.")
                break  # Break inner loop to re-enter a new Cerebra ID
            if time_input.lower() == "exit":
                program_exit = True
                break  # Break inner loop, then break out to end program

            try:
                time_location = int(time_input)
                # Validate time location using the Baseline video shape
                num_timestamps = video_data["Baseline"].shape[1]
                if time_location < 0 or time_location >= num_timestamps:
                    print(f"⚠️ Invalid Time Location. Please enter a value between 0 and {num_timestamps - 1}.")
                    continue

                # Plot voxel activation histograms for the selected region and time location
                plot_voxel_activation_for_region(time_location, cerebra_id, video_data)
            except ValueError:
                print("❌ Invalid input. Please enter a valid time location (integer).")

        if program_exit:
            break  # Break out of the Cerebra ID loop if user requested to exit

    if program_exit:
        break  # Break out of the subject loop if user requested to exit

# --------------------- End of Program ---------------------
print("\nThank you for using the program. Run ended.")

"""Details of each video per subject"""

def load_subject_video_data(subject_id, video_type):
    """
    Load brain activation data for a given subject and video type.
    Parameters:
        subject_id (str): Subject ID (e.g., 'NDARZY502FAG')
        video_type (str): Video type (e.g., "baseline", "video1", "video2", "video3")

    Returns:
        np.ndarray: Brain voxel activation data.
    """
    file_path = f"/content/drive/MyDrive/Data TU PHD DUBLIN/subjects Data/{subject_id}/evaluation/{video_type}_eLORETA.npy"
    return np.load(file_path)

def print_video_details(video, video_label):
    """
    Print details for a video dataset.

    Parameters:
        video (np.ndarray): The video data array.
        video_label (str): Label indicating which video (e.g., 'baseline', 'video1', etc.)
    """
    print(f"--- {video_label} ---")
    print("Number of brain points:", len(video))          # e.g., 31553 points
    print("First data point:", video[0])
    print("Number of timestamps in first point:", len(video[0]))  # e.g., 11250 timestamps
    print("Data type of video:", type(video))
    print()


# enter a subject ID manually
subject_id = input("Enter Subject ID (e.g., NDARZY502FAG): ").strip()

# List of video types to load for the subject
video_types = ["baseline", "video1", "video2", "video3"]

print(f"\n==== Details for Subject: {subject_id} ====")

# Loop over each video type, load the video, and print its details
for video_type in video_types:
    try:
        video = load_subject_video_data(subject_id, video_type)
        print_video_details(video, video_type.capitalize())
    except Exception as e:
        print(f"Error loading {video_type} for subject {subject_id}: {e}")
        print()

"""# Visualisation of heatmap of brain activation over time"""

fig = plt.figure(figsize=(15,10))
ax = fig.gca()
ax.imshow(videodata)
plt.tight_layout()
plt.xlabel('X-axis', fontsize=10)
plt.ylabel('Y-axis', fontsize=10)
plt.title('Heat map of brain activation over time')
plt.xticks(rotation=90)
plt.show()

fig = plt.figure(figsize=(15,10))
ax = fig.gca()
ax.imshow(videodata1)
plt.tight_layout()
plt.xlabel('X-axis', fontsize=10)
plt.ylabel('Y-axis', fontsize=10)
plt.title('Heat map of brain activation over time')
plt.xticks(rotation=90)
plt.show()

fig = plt.figure(figsize=(15,10))
ax = fig.gca()
ax.imshow(videodata2)
plt.tight_layout()
plt.xlabel('X-axis', fontsize=10)
plt.ylabel('Y-axis', fontsize=10)
plt.title('Heat map of brain activation over time')
plt.xticks(rotation=90)
plt.show()

fig = plt.figure(figsize=(10,10))
ax = fig.gca()
ax.imshow(videodata3)
plt.tight_layout()
plt.xlabel('X-axis', fontsize=10)
plt.ylabel('Y-axis', fontsize=10)
plt.title('Heat map of brain activation over time')
plt.xticks(rotation=90)
plt.show()

"""Calculate the Mean and Standard Deviation"""

print(videodata.shape)



from re import S
meanOfVideoData= np.mean(videodata ,axis=0 )#mean for whole data
stdOfVideoData= np.std(videodata ,axis=0 )#mean for whole data
cv= stdOfVideoData/meanOfVideoData
SUM=np.sum(videodata,axis=0)
plt.plot(meanOfVideoData)
plt.xlabel('X Axis')
plt.ylabel('Y Axis')
plt.title('Mean of video data')
plt.show()
plt.plot(stdOfVideoData)
plt.xlabel('X Axis')
plt.ylabel('Y Axis')
plt.title('SD of video data')
plt.show()
plt.plot(cv)
plt.xlabel('X Axis')
plt.ylabel('Y Axis')
plt.title('CV of video data')
plt.show()
plt.plot(SUM)
plt.xlabel('X Axis')
plt.ylabel('Y Axis')
plt.title('SUM of video data')
plt.show()

meanOfVideoData1= np.mean(videodata1 ,axis=0 )#mean for whole data
stdOfVideoData1= np.std(videodata1 ,axis=0 )#mean for whole data#
cv1= stdOfVideoData1/meanOfVideoData1
SUM1=np.sum(videodata1,axis=0)
plt.plot(meanOfVideoData1)
plt.xlabel('X Axis')
plt.ylabel('Y Axis')
plt.title('Mean of video data1')
plt.show()
plt.plot(stdOfVideoData1)
plt.xlabel('X Axis')
plt.ylabel('Y Axis')
plt.title('SD of video data1')
plt.show()
plt.plot(cv1)
plt.xlabel('X Axis')
plt.ylabel('Y Axis')
plt.title('CV of video data1')
plt.show()
plt.plot(SUM1)
plt.xlabel('X Axis')
plt.ylabel('Y Axis')
plt.title('SUM of video data1')
plt.show()

meanOfVideoData2= np.mean(videodata2 ,axis=0 )#mean for whole data
stdOfVideoData2= np.std(videodata2 ,axis=0 )#mean for whole data
cv2= stdOfVideoData2/meanOfVideoData2
SUM2=np.sum(videodata2,axis=0)
plt.plot(meanOfVideoData2)
plt.xlabel('X Axis')
plt.ylabel('Y Axis')
plt.title('Mean of video data2')
plt.show()
plt.plot(stdOfVideoData2)
plt.xlabel('X Axis')
plt.ylabel('Y Axis')
plt.title('SD of video data2')
plt.show()
plt.plot(cv2)
plt.xlabel('X Axis')
plt.ylabel('Y Axis')
plt.title('CV of video data2')
plt.show()
plt.plot(SUM2)
plt.xlabel('X Axis')
plt.ylabel('Y Axis')
plt.title('SUM of video data2')
plt.show()

meanOfVideoData3= np.mean(videodata3 ,axis=0 )#mean for whole data
stdOfVideoData3= np.std(videodata3 ,axis=0 )#mean for whole data
cv3= stdOfVideoData3/meanOfVideoData3
SUM3=np.sum(videodata3,axis=0)
plt.plot(meanOfVideoData3)
plt.xlabel('X Axis')
plt.ylabel('Y Axis')
plt.title('Mean of video data3')
plt.show()
plt.plot(stdOfVideoData3)
plt.xlabel('X Axis')
plt.ylabel('Y Axis')
plt.title('SD of video data3')
plt.show()
plt.plot(cv3)
plt.xlabel('X Axis')
plt.ylabel('Y Axis')
plt.title('CV of video data3')
plt.show()
plt.plot(SUM3)
plt.xlabel('X Axis')
plt.ylabel('Y Axis')
plt.title('SUM of video data3')
plt.show()

"""Comparing between CV IN V1,V2,V3,RELAX"""

plt.plot(SUM)
plt.xlabel('X Axis')
plt.ylabel('Y Axis')
plt.title('SUMof video data')
plt.show()

plt.plot(SUM1)
plt.xlabel('X Axis')
plt.ylabel('Y Axis')
plt.title('SUM1 of video data1')
plt.show()

plt.plot(SUM2)
plt.xlabel('X Axis')
plt.ylabel('Y Axis')
plt.title('SUM2 of video data2')
plt.show()

plt.plot(SUM3)
plt.xlabel('X Axis')
plt.ylabel('Y Axis')
plt.title('SUM3 of video data3')
plt.show()
print(SUM)
print(SUM1)
print(SUM2)
print(SUM3)

import pandas as pd
from scipy.stats import f_oneway, ttest_ind
import matplotlib.pyplot as plt


datasets = [videodata, videodata1, videodata2, videodata3]
dataset_labels = ["videodata", "videodata1", "videodata2", "videodata3"]

# Step 2: Calculate Mean, SD, CV, and Average for each column
summary_stats = {}
for i, dataset in enumerate(datasets):
    means = np.mean(dataset, axis=0)
    sds = np.std(dataset, axis=0)
    cvs = np.std(dataset, axis=0) / np.mean(dataset, axis=0)
    sum = np.sum(dataset, axis=0)
    averages = np.mean(dataset, axis=1)

    summary_stats[dataset_labels[i]] = {
        "Mean": means,
        "SD": sds,
        "CV": cvs,
        'SUM':sum,
        "Average": averages
    }


# Step 3: Visualize the Mean and CV for each dataset (example for one column)
for label, stats in summary_stats.items():
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Mean"])), stats["Mean"], color='skyblue', label="Mean")
    plt.xlabel('Columns')
    plt.ylabel('Values')
    plt.title(f'Mean for {label}')
    plt.legend()
    plt.show()

# Step 4: Statistical Comparison between Datasets
print("ANOVA Results:")
anova_results = f_oneway(
    summary_stats["videodata"]["Mean"],
    summary_stats["videodata1"]["Mean"],
    summary_stats["videodata2"]["Mean"],
    summary_stats["videodata3"]["Mean"]
)
print(f"F-statistic: {anova_results.statistic:.4f}, P-value: {anova_results.pvalue:.4f}")

if anova_results.pvalue < 0.05:
    print("There is a statistically significant difference between the datasets.")
else:
    print("No statistically significant difference between the datasets.")

# Step 5: Pairwise Comparison (T-tests)
print("\nPairwise T-tests:")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Mean"],
            summary_stats[dataset_labels[j]]["Mean"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")

import numpy as np
import pandas as pd
from scipy.stats import f_oneway, ttest_ind
import matplotlib.pyplot as plt

datasets = [videodata, videodata1, videodata2, videodata3]
dataset_labels = ["videodata", "videodata1", "videodata2", "videodata3"]

# Step 2: Calculate Mean, SD, CV, SUM, and Average for each column
summary_stats = {}
for i, dataset in enumerate(datasets):
    means = np.mean(dataset, axis=0)
    sds = np.std(dataset, axis=0)
    cvs = (np.std(dataset, axis=0) / np.mean(dataset, axis=0))
    sums = np.sum(dataset, axis=0)
    averages = np.mean(dataset, axis=1)

    summary_stats[dataset_labels[i]] = {
        "Mean": means,
        "SD": sds,
        "CV": cvs,
        "SUM": sums,
        "Average": averages
    }

# Step 3: Visualize the Mean, CV, and SUM for each dataset
for label, stats in summary_stats.items():
    # Mean Visualization
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Mean"])), stats["Mean"], color='skyblue', label="Mean")
    plt.xlabel('Columns')
    plt.ylabel('Mean Values')
    plt.title(f'Mean for {label}')
    plt.legend()
    plt.show()

    # SUM Visualization
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["SUM"])), stats["SUM"], color='green', label="SUM")
    plt.xlabel('Columns')
    plt.ylabel('Sum Values')
    plt.title(f'SUM for {label}')
    plt.legend()
    plt.show()

# Step 4: Statistical Comparison (Mean and SUM) between Datasets
print("ANOVA Results (Mean Comparison):")
anova_results_mean = f_oneway(
    summary_stats["videodata"]["Mean"],
    summary_stats["videodata1"]["Mean"],
    summary_stats["videodata2"]["Mean"],
    summary_stats["videodata3"]["Mean"]
)
print(f"F-statistic: {anova_results_mean.statistic:.4f}, P-value: {anova_results_mean.pvalue:.4f}")

if anova_results_mean.pvalue < 0.05:
    print("There is a statistically significant difference between the means of the datasets.")
else:
    print("No statistically significant difference between the means of the datasets.")

print("\nANOVA Results (SUM Comparison):")
anova_results_sum = f_oneway(
    summary_stats["videodata"]["SUM"],
    summary_stats["videodata1"]["SUM"],
    summary_stats["videodata2"]["SUM"],
    summary_stats["videodata3"]["SUM"]
)
print(f"F-statistic: {anova_results_sum.statistic:.4f}, P-value: {anova_results_sum.pvalue:.4f}")

if anova_results_sum.pvalue < 0.05:
    print("There is a statistically significant difference between the sums of the datasets.")
else:
    print("No statistically significant difference between the sums of the datasets.")

# Step 5: Pairwise T-tests for Means and SUMs
print("\nPairwise T-tests (Mean Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Mean"],
            summary_stats[dataset_labels[j]]["Mean"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")

print("\nPairwise T-tests (SUM Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["SUM"],
            summary_stats[dataset_labels[j]]["SUM"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")

import numpy as np
import pandas as pd
from scipy.stats import skew, kurtosis
import matplotlib.pyplot as plt


datasets = [videodata, videodata1, videodata2, videodata3]
dataset_labels = ["videodata", "videodata1", "videodata2", "videodata3"]

# Step 1: Extract Features
features = {}
for i, dataset in enumerate(datasets):
    means = np.mean(dataset, axis=0)  # Mean of each column
    sds = np.std(dataset, axis=0)  # Standard deviation of each column
    cvs = sds / means  # Coefficient of variation
    sums = np.sum(dataset, axis=0)  # Sum of each column
    skews = skew(dataset, axis=0)  # Skewness of each column
    kurtoses = kurtosis(dataset, axis=0)  # Kurtosis of each column

    features[dataset_labels[i]] = {
        "Mean": means,
        "SD": sds,
        "CV": cvs,
        "Sum": sums,
        "Skew": skews,
        "Kurtosis": kurtoses
    }

# Step 2: Save Features to CSV
for label, stats in features.items():
    df = pd.DataFrame(stats)
    df.to_csv(f"{label}_features.csv", index=False)

# Step 3: Visualize Selected Features
for label, stats in features.items():
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Mean"])), stats["Mean"], color='skyblue', label="Mean")
    plt.xlabel('Columns')
    plt.ylabel('Mean Values')
    plt.title(f'Mean for {label}')
    plt.legend()
    plt.show()

    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Skew"])), stats["Skew"], color='orange', label="Skewness")
    plt.xlabel('Columns')
    plt.ylabel('Skewness Values')
    plt.title(f'Skewness for {label}')
    plt.legend()
    plt.show()

    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Kurtosis"])), stats["Kurtosis"], color='green', label="Kurtosis")
    plt.xlabel('Columns')
    plt.ylabel('Kurtosis Values')
    plt.title(f'Kurtosis for {label}')
    plt.legend()
    plt.show()

# Step 4: Combine All Features for Further Analysis
combined_features = []
for label, stats in features.items():
    df = pd.DataFrame(stats)
    df["Dataset"] = label
    combined_features.append(df)

combined_features_df = pd.concat(combined_features, ignore_index=True)
combined_features_df.to_csv("combined_features.csv", index=False)

print("Features extracted and saved to CSV files.")

import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.metrics import r2_score, mean_squared_error
import numpy as np

# Step 1: Load the combined features
data = pd.read_csv("combined_features.csv")

# Step 2: Preprocess the data
# Convert categorical labels to numerical (if present)
data["Dataset"] = data["Dataset"].astype("category").cat.codes

# Separate features and target variable
X = data.drop(columns=["Dataset"])  # Features
y = data["Dataset"]  # Target

# Normalize the features (optional for some models)
X = (X - X.mean()) / X.std()

# Step 3: Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 4: Train multiple models
models = {
    "Linear Regression": LinearRegression(),
    "Random Forest": RandomForestRegressor(random_state=42),
    "Support Vector Regressor": SVR()
}

performance = []
for name, model in models.items():
    # Cross-validation
    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')
    mean_cv_r2 = np.mean(cv_scores)

    # Fit and test
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    test_r2 = r2_score(y_test, y_pred)
    test_mse = mean_squared_error(y_test, y_pred)

    performance.append({
        "Model": name,
        "CV R²": mean_cv_r2,
        "Test R²": test_r2,
        "Test MSE": test_mse
    })

# Step 5: Summarize performance
performance_df = pd.DataFrame(performance)
performance_df.to_csv("model_performance.csv", index=False)
print(performance_df)

# Identify the best model
best_model = performance_df.loc[performance_df["Test R²"].idxmax()]
print("\nBest Model:")
print(best_model)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Load the combined features data
# Replace 'combined_features.csv' with the actual file name if different
data = pd.read_csv("combined_features.csv")

# Step 1: Check the structure of the data
print(data.head())

# Step 2: Pairwise Scatter Plots
# Visualize the relationship between features and the target variable
sns.pairplot(data, hue="Dataset", palette="Set2")
plt.suptitle("Pairwise Scatter Plots of Features", y=1.02)
plt.show()

# Step 3: Box Plots
# Visualize how features vary across datasets (target variable)
for column in data.columns[:-1]:  # Exclude the target variable (assumed to be the last column)
    plt.figure(figsize=(10, 5))
    sns.boxplot(x="Dataset", y=column, data=data, palette="Set3")
    plt.title(f"Box Plot of {column} by Dataset")
    plt.xlabel("Dataset")
    plt.ylabel(column)
    plt.show()

# Step 4: Correlation Heatmap
# Compute the correlation matrix
numeric_data = data.select_dtypes(include=[np.number])
# Compute the correlation matrix using only numeric data
correlation_matrix = numeric_data.corr()

# Plot the heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap="coolwarm", cbar=True)
plt.title("Correlation Heatmap of Features")
plt.show()

# Step 5: Distribution Plots
# Visualize the distribution of features for each dataset
for column in data.columns[:-1]:  # Exclude the target variable
    plt.figure(figsize=(10, 5))
    for dataset in data["Dataset"].unique():
        sns.kdeplot(data[data["Dataset"] == dataset][column], label=f"Dataset {dataset}")
    plt.title(f"Distribution of {column} by Dataset")
    plt.xlabel(column)
    plt.ylabel("Density")
    plt.legend()
    plt.show()

"""to here old data

Compare the average sum and mean
"""

import numpy as np
import pandas as pd
from scipy.stats import f_oneway, ttest_ind
import matplotlib.pyplot as plt


videodata1= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/First data/video1_eLORETA.npy") #marowa's drive video1
videodata= np.load("/content/drive/MyDrive/Data TU PHD DUBLIN/First data/baseline_eLORETA.npy")  #base relax
videodata2= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/First data/video2_eLORETA.npy") #video2
videodata3= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/First data/video3_eLORETA.npy") #video3
datasets = [videodata, videodata1, videodata2, videodata3]
dataset_labels = ["videodata", "videodata1", "videodata2", "videodata3"]

# Step 2: Calculate Mean, SD, Sum, and Average for each column
summary_stats = {}
for i, dataset in enumerate(datasets):
    means = np.mean(dataset, axis=0)
    sds = np.std(dataset, axis=0)
    sums = np.sum(dataset, axis=0)
    averages = np.mean(dataset, axis=1)

    summary_stats[dataset_labels[i]] = {
        "Mean": means,
        "SD": sds,
        "Sum": sums,
        "Average": averages
    }

# Step 3: Visualize the Mean and Sum for each dataset
for label, stats in summary_stats.items():
    # Mean Visualization
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Mean"])), stats["Mean"], color='skyblue', label="Mean")
    plt.xlabel('Columns')
    plt.ylabel('Mean Values')
    plt.title(f'Mean for {label}')
    plt.legend()
    plt.show()

    # Sum Visualization
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Sum"])), stats["Sum"], color='orange', label="Sum")
    plt.xlabel('Columns')
    plt.ylabel('Sum Values')
    plt.title(f'Sum for {label}')
    plt.legend()
    plt.show()

    # Average Visualization (per subject)
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Average"])), stats["Average"], color='green', label="Average per Subject")
    plt.xlabel('Subjects')
    plt.ylabel('Average Values')
    plt.title(f'Average per Subject in {label}')
    plt.legend()
    plt.show()
# Step 4: Compare Mean and Sum between subjects in each dataset
for label, stats in summary_stats.items():
    # Mean comparison
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Average"])), stats["Average"], color='green', label="Average per Subject")
    plt.xlabel('Subjects')
    plt.ylabel('Average Values')
    plt.title(f'Average in {label}')
    plt.legend()
    plt.show()

# Step 5: Statistical Comparison between Datasets
print("ANOVA Results (Mean Comparison):")
mean_anova_results = f_oneway(
    summary_stats["videodata"]["Mean"],
    summary_stats["videodata1"]["Mean"],
    summary_stats["videodata2"]["Mean"],
    summary_stats["videodata3"]["Mean"]
)
print(f"F-statistic: {mean_anova_results.statistic:.4f}, P-value: {mean_anova_results.pvalue:.4f}")

print("\nANOVA Results (Sum Comparison):")
sum_anova_results = f_oneway(
    summary_stats["videodata"]["Sum"],
    summary_stats["videodata1"]["Sum"],
    summary_stats["videodata2"]["Sum"],
    summary_stats["videodata3"]["Sum"]
)
print(f"F-statistic: {sum_anova_results.statistic:.4f}, P-value: {sum_anova_results.pvalue:.4f}")

# Pairwise Comparison for Means and Sums
print("\nPairwise T-tests (Mean Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Mean"],
            summary_stats[dataset_labels[j]]["Mean"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")

print("\nANOVA Results (Average Comparison):")
average_anova_results = f_oneway(
    summary_stats["videodata"]["Average"],
    summary_stats["videodata1"]["Average"],
    summary_stats["videodata2"]["Average"],
    summary_stats["videodata3"]["Average"]
)
print(f"F-statistic: {average_anova_results.statistic:.4f}, P-value: {average_anova_results.pvalue:.4f}")

print("\nPairwise T-tests (Sum Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Sum"],
            summary_stats[dataset_labels[j]]["Sum"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")
# Pairwise Comparison for Averages
print("\nPairwise T-tests (Average Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Average"],
            summary_stats[dataset_labels[j]]["Average"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from statsmodels.formula.api import mixedlm
import scipy.stats as stats # Import the scipy.stats module
# Step 1: Simulate or load data
# Replace this with your actual data
data = {
    "Subject": np.tile(range(1, 11), 4),  # 10 subjects, repeated for 4 datasets
    "Dataset": np.repeat(["videodata", "videodata1", "videodata2", "videodata3"], 10),
    "CV": np.random.rand(40),  # Replace with your CV values
    "Mean": np.random.rand(40),  # Replace with your Mean values
}

df = pd.DataFrame(data)

# Step 2: Visualize the data
sns.boxplot(x="Dataset", y="CV", data=df, palette="Set2",legend=False)
plt.title("Coefficient of Variation (CV) Across Datasets")
plt.show()

sns.boxplot(x="Dataset", y="Mean", data=df, palette="Set3",legend=False)
plt.title("Mean Across Datasets")
plt.show()

# Step 3: Fit a Linear Mixed Effects Model
# Random effect: Subject, Fixed effect: Dataset
model = mixedlm("CV ~ Dataset", df, groups=df["Subject"])
result = model.fit()

# Step 4: Summarize Results
print(result.summary())

# Step 5: Statistical Significance of Fixed Effects using LRT
null_model = mixedlm("CV ~ 1", df, groups=df["Subject"]).fit()  # Null model (no fixed effects)
alt_model = mixedlm("CV ~ Dataset", df, groups=df["Subject"]).fit()  # Alternative model (with fixed effects)

# Likelihood Ratio Test (LRT)
lrt_stat = 2 * (alt_model.llf - null_model.llf)  # LRT statistic
df_diff = alt_model.df_modelwc - null_model.df_modelwc  # Difference in degrees of freedom
p_value = stats.chi2.sf(lrt_stat, df_diff)  # P-value from chi-square distribution

print("Likelihood Ratio Test Results:")
print(f"LRT Statistic: {lrt_stat:.4f}")
print(f"Degrees of Freedom: {df_diff}")
print(f"P-value: {p_value:.4f}")

if p_value < 0.05:
    print("The fixed effects (Dataset) significantly improve the model.")
else:
    print("No significant effect of the fixed effects (Dataset).")


# Step 6: Pairwise Comparisons (Optional)
from statsmodels.stats.multicomp import pairwise_tukeyhsd

tukey = pairwise_tukeyhsd(endog=df["CV"], groups=df["Dataset"], alpha=0.05)
print(tukey)

""" 4 data for four video each video record the resource for each subject"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns


videodata1= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/First data/video1_eLORETA.npy") #marowa's drive video1
videodata= np.load("/content/drive/MyDrive/Data TU PHD DUBLIN/First data/baseline_eLORETA.npy")  #base relax
videodata2= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/First data/video2_eLORETA.npy") #video2
videodata3= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/First data/video3_eLORETA.npy") #video3

datasets = [videodata, videodata1, videodata2, videodata3]
dataset_labels = ["Video1_Average", "Video2_Average", "Video3_Average", "Video4_Average"]

# Calculate the average for each subject in each video
averages = []
for dataset in datasets:
    averages.append(np.mean(dataset, axis=1))

# Determine the maximum number of subjects across all datasets
max_subjects = max([len(avg) for avg in averages])

# Pad shorter datasets with NaN values to align their lengths
aligned_averages = [
    np.pad(avg, (0, max_subjects - len(avg)), constant_values=np.nan) for avg in averages
]

# Create a DataFrame to store the results
subjects = [f"Subject_{i+1}" for i in range(max_subjects)]  # Generate subject labels

average_data = pd.DataFrame({
    "Subject": subjects,
    dataset_labels[0]: aligned_averages[0],
    dataset_labels[1]: aligned_averages[1],
    dataset_labels[2]: aligned_averages[2],
    dataset_labels[3]: aligned_averages[3],
})

# Save the data to a CSV file
output_file = "subject_averages.csv"
average_data.to_csv(output_file, index=False)

# Calculate correlation matrix (ignoring NaN values)
correlation_matrix = average_data.drop("Subject", axis=1).corr()
print("Correlation Matrix:")
print(correlation_matrix)

# Visualize the correlation matrix
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Correlation Matrix Between Videos")
plt.show()

# Determine the best model (pair with the highest correlation)
best_pair = None
max_correlation = -1

for i, col1 in enumerate(correlation_matrix.columns):
    for j, col2 in enumerate(correlation_matrix.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix.loc[col1, col2]
            if correlation > max_correlation:
                max_correlation = correlation
                best_pair = (col1, col2)

print(f"Best Correlated Pair: {best_pair} with Correlation: {max_correlation:.2f}")
print(f"Averages for each subject saved to {output_file}")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

videodata1= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/First data/video1_eLORETA.npy") #marowa's drive video1
videodata= np.load("/content/drive/MyDrive/Data TU PHD DUBLIN/First data/baseline_eLORETA.npy")  #base relax
videodata2= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/First data/video2_eLORETA.npy") #video2
videodata3= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/First data/video3_eLORETA.npy") #video3
# Ensure all datasets have the same number of rows
num_subjects = videodata.shape[0]
if any(dataset.shape[0] != num_subjects for dataset in [videodata1, videodata2, videodata3]):
    raise ValueError("All video datasets must have the same number of subjects (rows).")

# Calculate the sum for each subject in each video
sums_video1 = np.sum(videodata, axis=1)
sums_video2 = np.sum(videodata1, axis=1)
sums_video3 = np.sum(videodata2, axis=1)
sums_video4 = np.sum(videodata3, axis=1)

# Create a DataFrame to store the results
subjects = [f"Subject_{i+1}" for i in range(num_subjects)]  # Generate subject labels

sum_data = pd.DataFrame({
    "Subject": subjects,
    "Video1_Sum": sums_video1,
    "Video2_Sum": sums_video2,
    "Video3_Sum": sums_video3,
    "Video4_Sum": sums_video4
})

# Save the data to a CSV file
output_file = "subject_sums.csv"
sum_data.to_csv(output_file, index=False)

# Calculate correlation matrices using different methods
correlation_matrix_pearson = sum_data.drop("Subject", axis=1).corr(method="pearson")
correlation_matrix_spearman = sum_data.drop("Subject", axis=1).corr(method="spearman")
correlation_matrix_kendall = sum_data.drop("Subject", axis=1).corr(method="kendall")

# Print correlation matrices
print("Pearson Correlation Matrix:")
print(correlation_matrix_pearson)
print("\nSpearman Correlation Matrix:")
print(correlation_matrix_spearman)
print("\nKendall Correlation Matrix:")
print(correlation_matrix_kendall)

# Visualize the correlation matrices
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_pearson, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Pearson Correlation Matrix Between Videos (Based on Sums)")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_spearman, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Spearman Correlation Matrix Between Videos (Based on Sums)")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_kendall, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Kendall Correlation Matrix Between Videos (Based on Sums)")
plt.show()

# Determine the best model (pair with the highest correlation in Pearson)
best_pair_pearson = None
max_correlation_pearson = -1

for i, col1 in enumerate(correlation_matrix_pearson.columns):
    for j, col2 in enumerate(correlation_matrix_pearson.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_pearson.loc[col1, col2]
            if correlation > max_correlation_pearson:
                max_correlation_pearson = correlation
                best_pair_pearson = (col1, col2)

print(f"Best Correlated Pair (Pearson): {best_pair_pearson} with Correlation: {max_correlation_pearson:.2f}")

# Determine the best model (pair with the highest correlation in Spearman)
best_pair_spearman = None
max_correlation_spearman = -1

for i, col1 in enumerate(correlation_matrix_spearman.columns):
    for j, col2 in enumerate(correlation_matrix_spearman.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_spearman.loc[col1, col2]
            if correlation > max_correlation_spearman:
                max_correlation_spearman = correlation
                best_pair_spearman = (col1, col2)

print(f"Best Correlated Pair (Spearman): {best_pair_spearman} with Correlation: {max_correlation_spearman:.2f}")

# Determine the best model (pair with the highest correlation in Kendall)
best_pair_kendall = None
max_correlation_kendall = -1

for i, col1 in enumerate(correlation_matrix_kendall.columns):
    for j, col2 in enumerate(correlation_matrix_kendall.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_kendall.loc[col1, col2]
            if correlation > max_correlation_kendall:
                max_correlation_kendall = correlation
                best_pair_kendall = (col1, col2)

print(f"Best Correlated Pair (Kendall): {best_pair_kendall} with Correlation: {max_correlation_kendall:.2f}")

print(f"Sum for each subject saved to {output_file}")

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, r2_score

datasets = [videodata, videodata1, videodata2, videodata3]
dataset_labels = ["videodata", "videodata1", "videodata2", "videodata3"]

# Combine data into a DataFrame
subjects = [f"Subject_{i+1}" for i in range(len(video1))]
average_data = pd.DataFrame({
    "Subject": subjects,
    "Video1_Average": video1,
    "Video2_Average": video2,
    "Video3_Average": video3,
    "Video4_Average": video4
})

# Define features (X) and target (y)
X = average_data.drop(["Subject", "Video4_Average"], axis=1)  # Use Video1, Video2, Video3 as features
y = average_data["Video4_Average"]  # Predict Video4_Average

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize models
models = {
    "Linear Regression": LinearRegression(),
    "Random Forest": RandomForestRegressor(random_state=42),
    "Support Vector Regressor": SVR()
}

# Train and evaluate models
best_model = None
best_score = float('-inf')
results = []

for name, model in models.items():
    # Cross-validation
    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')
    mean_cv_score = np.mean(cv_scores)

    # Train the model on the full training set
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    # Evaluate on the test set
    test_r2 = r2_score(y_test, y_pred)
    test_mse = mean_squared_error(y_test, y_pred)

    results.append({
        "Model": name,
        "CV R²": mean_cv_score,
        "Test R²": test_r2,
        "Test MSE": test_mse
    })

    # Track the best model
    if test_r2 > best_score:
        best_score = test_r2
        best_model = model

# Display results
results_df = pd.DataFrame(results)
print("Model Performance:")
print(results_df)

# Save results to CSV
results_df.to_csv("model_performance.csv", index=False)
print("Model performance saved to 'model_performance.csv'")

# Best model
print(f"Best Model: {type(best_model).__name__} with Test R²: {best_score:.2f}")



"""# VISUALIZE 35 SUBJECTS

NDARZY502FAG
"""

videodata= np.load("/content/drive/MyDrive/Data TU PHD DUBLIN/subjects Data/NDARZY502FAG/evaluation/baseline_eLORETA.npy")  #base relax
videodata1= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/subjects Data/NDARZY502FAG/evaluation/video1_eLORETA.npy") #marowa's drive video1
videodata2= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/subjects Data/NDARZY502FAG/evaluation/video2_eLORETA.npy") #video2
videodata3= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/subjects Data/NDARZY502FAG/evaluation/video3_eLORETA.npy") #video3
#luca's drive
#pathLuca = "/content/drive/MyDrive/TU Dublin-NN/research/PHD CANDIDATES - AICL research LAB/CURRENT/Carlos Gomez Tapia - PB04482/dataset - full source activation space (31000x11250) for 35 subjects/"
#fileName = "video1_eLORETA.npy"
#videodata= np.load (pathLuca + fileName)
print(videodata1.shape)
print(len(videodata)) #len of points in brain (31553)
print(videodata[0])
print(len(videodata[0]))  #timestamps  (11250=125hz x 90 seconds)
print(type(videodata))
#video2data is a list of list (first list 31553 lenghth,each of this cell contains 11250 point,31553 is represented the number of the brain and the 11250 is the point in time  )
import numpy as np
import pandas as pd
from scipy.stats import f_oneway, ttest_ind
import matplotlib.pyplot as plt


datasets = [videodata, videodata1, videodata2, videodata3]
dataset_labels = ["videodata", "videodata1", "videodata2", "videodata3"]

# Step 2: Calculate Mean, SD, Sum, and Average for each column
summary_stats = {}
for i, dataset in enumerate(datasets):
    means = np.mean(dataset, axis=0)
    sds = np.std(dataset, axis=0)
    sums = np.sum(dataset, axis=0)
    averages = np.mean(dataset, axis=1)

    summary_stats[dataset_labels[i]] = {
        "Mean": means,
        "SD": sds,
        "Sum": sums,
        "Average": averages
    }

# Step 3: Visualize the Mean and Sum for each dataset
for label, stats in summary_stats.items():
    # Mean Visualization
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Mean"])), stats["Mean"], color='skyblue', label="Mean")
    plt.xlabel('Columns')
    plt.ylabel('Mean Values')
    plt.title(f'Mean for {label}')
    plt.legend()
    plt.show()

    # Sum Visualization
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Sum"])), stats["Sum"], color='orange', label="Sum")
    plt.xlabel('Columns')
    plt.ylabel('Sum Values')
    plt.title(f'Sum for {label}')
    plt.legend()
    plt.show()

    # Average Visualization (per subject)
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Average"])), stats["Average"], color='green', label="Average per Subject")
    plt.xlabel('Subjects')
    plt.ylabel('Average Values')
    plt.title(f'Average per Subject in {label}')
    plt.legend()
    plt.show()
# Step 4: Compare Mean and Sum between subjects in each dataset
for label, stats in summary_stats.items():
    # Mean comparison
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Average"])), stats["Average"], color='green', label="Average per Subject")
    plt.xlabel('Subjects')
    plt.ylabel('Average Values')
    plt.title(f'Average in {label}')
    plt.legend()
    plt.show()

# Step 5: Statistical Comparison between Datasets
print("ANOVA Results (Mean Comparison):")
mean_anova_results = f_oneway(
    summary_stats["videodata"]["Mean"],
    summary_stats["videodata1"]["Mean"],
    summary_stats["videodata2"]["Mean"],
    summary_stats["videodata3"]["Mean"]
)
print(f"F-statistic: {mean_anova_results.statistic:.4f}, P-value: {mean_anova_results.pvalue:.4f}")

print("\nANOVA Results (Sum Comparison):")
sum_anova_results = f_oneway(
    summary_stats["videodata"]["Sum"],
    summary_stats["videodata1"]["Sum"],
    summary_stats["videodata2"]["Sum"],
    summary_stats["videodata3"]["Sum"]
)
print(f"F-statistic: {sum_anova_results.statistic:.4f}, P-value: {sum_anova_results.pvalue:.4f}")

# Pairwise Comparison for Means and Sums
print("\nPairwise T-tests (Mean Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Mean"],
            summary_stats[dataset_labels[j]]["Mean"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")

print("\nANOVA Results (Average Comparison):")
average_anova_results = f_oneway(
    summary_stats["videodata"]["Average"],
    summary_stats["videodata1"]["Average"],
    summary_stats["videodata2"]["Average"],
    summary_stats["videodata3"]["Average"]
)
print(f"F-statistic: {average_anova_results.statistic:.4f}, P-value: {average_anova_results.pvalue:.4f}")

print("\nPairwise T-tests (Sum Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Sum"],
            summary_stats[dataset_labels[j]]["Sum"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")
# Pairwise Comparison for Averages
print("\nPairwise T-tests (Average Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Average"],
            summary_stats[dataset_labels[j]]["Average"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")

# Ensure all datasets have the same number of rows
num_subjects = videodata.shape[0]
if any(dataset.shape[0] != num_subjects for dataset in [videodata1, videodata2, videodata3]):
    raise ValueError("All video datasets must have the same number of subjects (rows).")

# Calculate the sum for each subject in each video
sums_video1 = np.sum(videodata, axis=1)
sums_video2 = np.sum(videodata1, axis=1)
sums_video3 = np.sum(videodata2, axis=1)
sums_video4 = np.sum(videodata3, axis=1)

# Create a DataFrame to store the results
subjects = [f"Subject_{i+1}" for i in range(num_subjects)]  # Generate subject labels

sum_data = pd.DataFrame({
    "Subject": subjects,
    "Video1_Sum": sums_video1,
    "Video2_Sum": sums_video2,
    "Video3_Sum": sums_video3,
    "Video4_Sum": sums_video4
})

# Save the data to a CSV file
output_file = "subject_sums.csv"
sum_data.to_csv(output_file, index=False)

# Calculate correlation matrices using different methods
correlation_matrix_pearson = sum_data.drop("Subject", axis=1).corr(method="pearson")
correlation_matrix_spearman = sum_data.drop("Subject", axis=1).corr(method="spearman")
correlation_matrix_kendall = sum_data.drop("Subject", axis=1).corr(method="kendall")

# Print correlation matrices
print("Pearson Correlation Matrix:")
print(correlation_matrix_pearson)
print("\nSpearman Correlation Matrix:")
print(correlation_matrix_spearman)
print("\nKendall Correlation Matrix:")
print(correlation_matrix_kendall)

# Visualize the correlation matrices
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_pearson, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Pearson Correlation Matrix Between Videos (Based on Sums)")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_spearman, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Spearman Correlation Matrix Between Videos (Based on Sums)")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_kendall, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Kendall Correlation Matrix Between Videos (Based on Sums)")
plt.show()

# Determine the best model (pair with the highest correlation in Pearson)
best_pair_pearson = None
max_correlation_pearson = -1

for i, col1 in enumerate(correlation_matrix_pearson.columns):
    for j, col2 in enumerate(correlation_matrix_pearson.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_pearson.loc[col1, col2]
            if correlation > max_correlation_pearson:
                max_correlation_pearson = correlation
                best_pair_pearson = (col1, col2)

print(f"Best Correlated Pair (Pearson): {best_pair_pearson} with Correlation: {max_correlation_pearson:.2f}")

# Determine the best model (pair with the highest correlation in Spearman)
best_pair_spearman = None
max_correlation_spearman = -1

for i, col1 in enumerate(correlation_matrix_spearman.columns):
    for j, col2 in enumerate(correlation_matrix_spearman.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_spearman.loc[col1, col2]
            if correlation > max_correlation_spearman:
                max_correlation_spearman = correlation
                best_pair_spearman = (col1, col2)

print(f"Best Correlated Pair (Spearman): {best_pair_spearman} with Correlation: {max_correlation_spearman:.2f}")

# Determine the best model (pair with the highest correlation in Kendall)
best_pair_kendall = None
max_correlation_kendall = -1

for i, col1 in enumerate(correlation_matrix_kendall.columns):
    for j, col2 in enumerate(correlation_matrix_kendall.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_kendall.loc[col1, col2]
            if correlation > max_correlation_kendall:
                max_correlation_kendall = correlation
                best_pair_kendall = (col1, col2)

print(f"Best Correlated Pair (Kendall): {best_pair_kendall} with Correlation: {max_correlation_kendall:.2f}")

print(f"Sum for each subject saved to {output_file}")

"""NDARZP564MHU

"""

videodata= np.load("/content/drive/MyDrive/Data TU PHD DUBLIN/subjects Data/NDARZP564MHU/evaluation/baseline_eLORETA.npy")  #base relax
videodata1= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/subjects Data/NDARZP564MHU/evaluation/video1_eLORETA.npy") #marowa's drive video1
videodata2= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/subjects Data/NDARZP564MHU/evaluation/video2_eLORETA.npy") #video2
videodata3= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/subjects Data/NDARZP564MHU/evaluation/video3_eLORETA.npy") #video3
#luca's drive
#pathLuca = "/content/drive/MyDrive/TU Dublin-NN/research/PHD CANDIDATES - AICL research LAB/CURRENT/Carlos Gomez Tapia - PB04482/dataset - full source activation space (31000x11250) for 35 subjects/"
#fileName = "video1_eLORETA.npy"
#videodata= np.load (pathLuca + fileName)
print(videodata1.shape)
print(len(videodata)) #len of points in brain (31553)
print(videodata[0])
print(len(videodata[0]))  #timestamps  (11250=125hz x 90 seconds)
print(type(videodata))
#video2data is a list of list (first list 31553 lenghth,each of this cell contains 11250 point,31553 is represented the number of the brain and the 11250 is the point in time  )
import numpy as np
import pandas as pd
from scipy.stats import f_oneway, ttest_ind
import matplotlib.pyplot as plt


datasets = [videodata, videodata1, videodata2, videodata3]
dataset_labels = ["videodata", "videodata1", "videodata2", "videodata3"]

# Step 2: Calculate Mean, SD, Sum, and Average for each column
summary_stats = {}
for i, dataset in enumerate(datasets):
    means = np.mean(dataset, axis=0)
    sds = np.std(dataset, axis=0)
    sums = np.sum(dataset, axis=0)
    averages = np.mean(dataset, axis=1)

    summary_stats[dataset_labels[i]] = {
        "Mean": means,
        "SD": sds,
        "Sum": sums,
        "Average": averages
    }

# Step 3: Visualize the Mean and Sum for each dataset
for label, stats in summary_stats.items():
    # Mean Visualization
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Mean"])), stats["Mean"], color='skyblue', label="Mean")
    plt.xlabel('Columns')
    plt.ylabel('Mean Values')
    plt.title(f'Mean for {label}')
    plt.legend()
    plt.show()

    # Sum Visualization
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Sum"])), stats["Sum"], color='orange', label="Sum")
    plt.xlabel('Columns')
    plt.ylabel('Sum Values')
    plt.title(f'Sum for {label}')
    plt.legend()
    plt.show()

    # Average Visualization (per subject)
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Average"])), stats["Average"], color='green', label="Average per Subject")
    plt.xlabel('Subjects')
    plt.ylabel('Average Values')
    plt.title(f'Average per Subject in {label}')
    plt.legend()
    plt.show()
# Step 4: Compare Mean and Sum between subjects in each dataset
for label, stats in summary_stats.items():
    # Mean comparison
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Average"])), stats["Average"], color='green', label="Average per Subject")
    plt.xlabel('Subjects')
    plt.ylabel('Average Values')
    plt.title(f'Average in {label}')
    plt.legend()
    plt.show()

# Step 5: Statistical Comparison between Datasets
print("ANOVA Results (Mean Comparison):")
mean_anova_results = f_oneway(
    summary_stats["videodata"]["Mean"],
    summary_stats["videodata1"]["Mean"],
    summary_stats["videodata2"]["Mean"],
    summary_stats["videodata3"]["Mean"]
)
print(f"F-statistic: {mean_anova_results.statistic:.4f}, P-value: {mean_anova_results.pvalue:.4f}")

print("\nANOVA Results (Sum Comparison):")
sum_anova_results = f_oneway(
    summary_stats["videodata"]["Sum"],
    summary_stats["videodata1"]["Sum"],
    summary_stats["videodata2"]["Sum"],
    summary_stats["videodata3"]["Sum"]
)
print(f"F-statistic: {sum_anova_results.statistic:.4f}, P-value: {sum_anova_results.pvalue:.4f}")

# Pairwise Comparison for Means and Sums
print("\nPairwise T-tests (Mean Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Mean"],
            summary_stats[dataset_labels[j]]["Mean"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")

print("\nANOVA Results (Average Comparison):")
average_anova_results = f_oneway(
    summary_stats["videodata"]["Average"],
    summary_stats["videodata1"]["Average"],
    summary_stats["videodata2"]["Average"],
    summary_stats["videodata3"]["Average"]
)
print(f"F-statistic: {average_anova_results.statistic:.4f}, P-value: {average_anova_results.pvalue:.4f}")

print("\nPairwise T-tests (Sum Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Sum"],
            summary_stats[dataset_labels[j]]["Sum"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")
# Pairwise Comparison for Averages
print("\nPairwise T-tests (Average Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Average"],
            summary_stats[dataset_labels[j]]["Average"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")

# Ensure all datasets have the same number of rows
num_subjects = videodata.shape[0]
if any(dataset.shape[0] != num_subjects for dataset in [videodata1, videodata2, videodata3]):
    raise ValueError("All video datasets must have the same number of subjects (rows).")

# Calculate the sum for each subject in each video
sums_video1 = np.sum(videodata, axis=1)
sums_video2 = np.sum(videodata1, axis=1)
sums_video3 = np.sum(videodata2, axis=1)
sums_video4 = np.sum(videodata3, axis=1)

# Create a DataFrame to store the results
subjects = [f"Subject_{i+1}" for i in range(num_subjects)]  # Generate subject labels

sum_data = pd.DataFrame({
    "Subject": subjects,
    "Video1_Sum": sums_video1,
    "Video2_Sum": sums_video2,
    "Video3_Sum": sums_video3,
    "Video4_Sum": sums_video4
})

# Save the data to a CSV file
output_file = "subject_sums.csv"
sum_data.to_csv(output_file, index=False)

# Calculate correlation matrices using different methods
correlation_matrix_pearson = sum_data.drop("Subject", axis=1).corr(method="pearson")
correlation_matrix_spearman = sum_data.drop("Subject", axis=1).corr(method="spearman")
correlation_matrix_kendall = sum_data.drop("Subject", axis=1).corr(method="kendall")

# Print correlation matrices
print("Pearson Correlation Matrix:")
print(correlation_matrix_pearson)
print("\nSpearman Correlation Matrix:")
print(correlation_matrix_spearman)
print("\nKendall Correlation Matrix:")
print(correlation_matrix_kendall)

# Visualize the correlation matrices
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_pearson, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Pearson Correlation Matrix Between Videos (Based on Sums)")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_spearman, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Spearman Correlation Matrix Between Videos (Based on Sums)")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_kendall, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Kendall Correlation Matrix Between Videos (Based on Sums)")
plt.show()

# Determine the best model (pair with the highest correlation in Pearson)
best_pair_pearson = None
max_correlation_pearson = -1

for i, col1 in enumerate(correlation_matrix_pearson.columns):
    for j, col2 in enumerate(correlation_matrix_pearson.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_pearson.loc[col1, col2]
            if correlation > max_correlation_pearson:
                max_correlation_pearson = correlation
                best_pair_pearson = (col1, col2)

print(f"Best Correlated Pair (Pearson): {best_pair_pearson} with Correlation: {max_correlation_pearson:.2f}")

# Determine the best model (pair with the highest correlation in Spearman)
best_pair_spearman = None
max_correlation_spearman = -1

for i, col1 in enumerate(correlation_matrix_spearman.columns):
    for j, col2 in enumerate(correlation_matrix_spearman.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_spearman.loc[col1, col2]
            if correlation > max_correlation_spearman:
                max_correlation_spearman = correlation
                best_pair_spearman = (col1, col2)

print(f"Best Correlated Pair (Spearman): {best_pair_spearman} with Correlation: {max_correlation_spearman:.2f}")

# Determine the best model (pair with the highest correlation in Kendall)
best_pair_kendall = None
max_correlation_kendall = -1

for i, col1 in enumerate(correlation_matrix_kendall.columns):
    for j, col2 in enumerate(correlation_matrix_kendall.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_kendall.loc[col1, col2]
            if correlation > max_correlation_kendall:
                max_correlation_kendall = correlation
                best_pair_kendall = (col1, col2)

print(f"Best Correlated Pair (Kendall): {best_pair_kendall} with Correlation: {max_correlation_kendall:.2f}")

print(f"Sum for each subject saved to {output_file}")

"""NDARYY218AGA"""

videodata= np.load("/content/drive/MyDrive/Data TU PHD DUBLIN/subjects Data/NDARZP564MHU/evaluation/baseline_eLORETA.npy")  #base relax
videodata1= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/subjects Data/NDARZP564MHU/evaluation/video1_eLORETA.npy") #marowa's drive video1
videodata2= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/subjects Data/NDARZP564MHU/evaluation/video2_eLORETA.npy") #video2
videodata3= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/subjects Data/NDARZP564MHU/evaluation/video3_eLORETA.npy") #video3
#luca's drive
#pathLuca = "/content/drive/MyDrive/TU Dublin-NN/research/PHD CANDIDATES - AICL research LAB/CURRENT/Carlos Gomez Tapia - PB04482/dataset - full source activation space (31000x11250) for 35 subjects/"
#fileName = "video1_eLORETA.npy"
#videodata= np.load (pathLuca + fileName)
print(videodata1.shape)
print(len(videodata)) #len of points in brain (31553)
print(videodata[0])
print(len(videodata[0]))  #timestamps  (11250=125hz x 90 seconds)
print(type(videodata))
#video2data is a list of list (first list 31553 lenghth,each of this cell contains 11250 point,31553 is represented the number of the brain and the 11250 is the point in time  )

datasets = [videodata, videodata1, videodata2, videodata3]
dataset_labels = ["videodata", "videodata1", "videodata2", "videodata3"]

# Step 2: Calculate Mean, SD, Sum, and Average for each column
summary_stats = {}
for i, dataset in enumerate(datasets):
    means = np.mean(dataset, axis=0)
    sds = np.std(dataset, axis=0)
    sums = np.sum(dataset, axis=0)
    averages = np.mean(dataset, axis=1)

    summary_stats[dataset_labels[i]] = {
        "Mean": means,
        "SD": sds,
        "Sum": sums,
        "Average": averages
    }

# Step 3: Visualize the Mean and Sum for each dataset
for label, stats in summary_stats.items():
    # Mean Visualization
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Mean"])), stats["Mean"], color='skyblue', label="Mean")
    plt.xlabel('Columns')
    plt.ylabel('Mean Values')
    plt.title(f'Mean for {label}')
    plt.legend()
    plt.show()

    # Sum Visualization
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Sum"])), stats["Sum"], color='orange', label="Sum")
    plt.xlabel('Columns')
    plt.ylabel('Sum Values')
    plt.title(f'Sum for {label}')
    plt.legend()
    plt.show()

    # Average Visualization (per subject)
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Average"])), stats["Average"], color='green', label="Average per Subject")
    plt.xlabel('Subjects')
    plt.ylabel('Average Values')
    plt.title(f'Average per Subject in {label}')
    plt.legend()
    plt.show()
# Step 4: Compare Mean and Sum between subjects in each dataset
for label, stats in summary_stats.items():
    # Mean comparison
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Average"])), stats["Average"], color='green', label="Average per Subject")
    plt.xlabel('Subjects')
    plt.ylabel('Average Values')
    plt.title(f'Average in {label}')
    plt.legend()
    plt.show()

# Step 5: Statistical Comparison between Datasets
print("ANOVA Results (Mean Comparison):")
mean_anova_results = f_oneway(
    summary_stats["videodata"]["Mean"],
    summary_stats["videodata1"]["Mean"],
    summary_stats["videodata2"]["Mean"],
    summary_stats["videodata3"]["Mean"]
)
print(f"F-statistic: {mean_anova_results.statistic:.4f}, P-value: {mean_anova_results.pvalue:.4f}")

print("\nANOVA Results (Sum Comparison):")
sum_anova_results = f_oneway(
    summary_stats["videodata"]["Sum"],
    summary_stats["videodata1"]["Sum"],
    summary_stats["videodata2"]["Sum"],
    summary_stats["videodata3"]["Sum"]
)
print(f"F-statistic: {sum_anova_results.statistic:.4f}, P-value: {sum_anova_results.pvalue:.4f}")

# Pairwise Comparison for Means and Sums
print("\nPairwise T-tests (Mean Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Mean"],
            summary_stats[dataset_labels[j]]["Mean"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")

print("\nANOVA Results (Average Comparison):")
average_anova_results = f_oneway(
    summary_stats["videodata"]["Average"],
    summary_stats["videodata1"]["Average"],
    summary_stats["videodata2"]["Average"],
    summary_stats["videodata3"]["Average"]
)
print(f"F-statistic: {average_anova_results.statistic:.4f}, P-value: {average_anova_results.pvalue:.4f}")

print("\nPairwise T-tests (Sum Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Sum"],
            summary_stats[dataset_labels[j]]["Sum"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")
# Pairwise Comparison for Averages
print("\nPairwise T-tests (Average Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Average"],
            summary_stats[dataset_labels[j]]["Average"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")

videodata= np.load("/content/drive/MyDrive/Data TU PHD DUBLIN/subjects Data/NDARZP564MHU/evaluation/baseline_eLORETA.npy")  #base relax
videodata1= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/subjects Data/NDARZP564MHU/evaluation/video1_eLORETA.npy") #marowa's drive video1
videodata2= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/subjects Data/NDARZP564MHU/evaluation/video2_eLORETA.npy") #video2
videodata3= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/subjects Data/NDARZP564MHU/evaluation/video3_eLORETA.npy") #video3

# Ensure all datasets have the same number of rows
num_subjects = videodata.shape[0]
if any(dataset.shape[0] != num_subjects for dataset in [videodata1, videodata2, videodata3]):
    raise ValueError("All video datasets must have the same number of subjects (rows).")

# Calculate the sum for each subject in each video
sums_video1 = np.sum(videodata, axis=1)
sums_video2 = np.sum(videodata1, axis=1)
sums_video3 = np.sum(videodata2, axis=1)
sums_video4 = np.sum(videodata3, axis=1)

# Create a DataFrame to store the results
subjects = [f"Subject_{i+1}" for i in range(num_subjects)]  # Generate subject labels

sum_data = pd.DataFrame({
    "Subject": subjects,
    "Video1_Sum": sums_video1,
    "Video2_Sum": sums_video2,
    "Video3_Sum": sums_video3,
    "Video4_Sum": sums_video4
})

# Save the data to a CSV file
output_file = "subject_sums.csv"
sum_data.to_csv(output_file, index=False)

# Calculate correlation matrices using different methods
correlation_matrix_pearson = sum_data.drop("Subject", axis=1).corr(method="pearson")
correlation_matrix_spearman = sum_data.drop("Subject", axis=1).corr(method="spearman")
correlation_matrix_kendall = sum_data.drop("Subject", axis=1).corr(method="kendall")

# Print correlation matrices
print("Pearson Correlation Matrix:")
print(correlation_matrix_pearson)
print("\nSpearman Correlation Matrix:")
print(correlation_matrix_spearman)
print("\nKendall Correlation Matrix:")
print(correlation_matrix_kendall)

# Visualize the correlation matrices
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_pearson, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Pearson Correlation Matrix Between Videos (Based on Sums)")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_spearman, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Spearman Correlation Matrix Between Videos (Based on Sums)")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_kendall, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Kendall Correlation Matrix Between Videos (Based on Sums)")
plt.show()

# Determine the best model (pair with the highest correlation in Pearson)
best_pair_pearson = None
max_correlation_pearson = -1

for i, col1 in enumerate(correlation_matrix_pearson.columns):
    for j, col2 in enumerate(correlation_matrix_pearson.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_pearson.loc[col1, col2]
            if correlation > max_correlation_pearson:
                max_correlation_pearson = correlation
                best_pair_pearson = (col1, col2)

print(f"Best Correlated Pair (Pearson): {best_pair_pearson} with Correlation: {max_correlation_pearson:.2f}")

# Determine the best model (pair with the highest correlation in Spearman)
best_pair_spearman = None
max_correlation_spearman = -1

for i, col1 in enumerate(correlation_matrix_spearman.columns):
    for j, col2 in enumerate(correlation_matrix_spearman.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_spearman.loc[col1, col2]
            if correlation > max_correlation_spearman:
                max_correlation_spearman = correlation
                best_pair_spearman = (col1, col2)

print(f"Best Correlated Pair (Spearman): {best_pair_spearman} with Correlation: {max_correlation_spearman:.2f}")

# Determine the best model (pair with the highest correlation in Kendall)
best_pair_kendall = None
max_correlation_kendall = -1

for i, col1 in enumerate(correlation_matrix_kendall.columns):
    for j, col2 in enumerate(correlation_matrix_kendall.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_kendall.loc[col1, col2]
            if correlation > max_correlation_kendall:
                max_correlation_kendall = correlation
                best_pair_kendall = (col1, col2)

print(f"Best Correlated Pair (Kendall): {best_pair_kendall} with Correlation: {max_correlation_kendall:.2f}")

print(f"Sum for each subject saved to {output_file}")

"""NDARGB441VVD"""

videodata= np.load("/content/drive/MyDrive/Data TU PHD DUBLIN/subjects Data/NDARGB441VVD/evaluation/baseline_eLORETA.npy")  #base relax
videodata1= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/subjects Data/NDARGB441VVD/evaluation/video1_eLORETA.npy") #marowa's drive video1
videodata2= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/subjects Data/NDARGB441VVD/evaluation/video2_eLORETA.npy") #video2
videodata3= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/subjects Data/NDARGB441VVD/evaluation/video3_eLORETA.npy") #video3
#luca's drive
#pathLuca = "/content/drive/MyDrive/TU Dublin-NN/research/PHD CANDIDATES - AICL research LAB/CURRENT/Carlos Gomez Tapia - PB04482/dataset - full source activation space (31000x11250) for 35 subjects/"
#fileName = "video1_eLORETA.npy"
#videodata= np.load (pathLuca + fileName)
print(videodata1.shape)
print(len(videodata)) #len of points in brain (31553)
print(videodata[0])
print(len(videodata[0]))  #timestamps  (11250=125hz x 90 seconds)
print(type(videodata))
#video2data is a list of list (first list 31553 lenghth,each of this cell contains 11250 point,31553 is represented the number of the brain and the 11250 is the point in time  )
import numpy as np
import pandas as pd
from scipy.stats import f_oneway, ttest_ind
import matplotlib.pyplot as plt


datasets = [videodata, videodata1, videodata2, videodata3]
dataset_labels = ["videodata", "videodata1", "videodata2", "videodata3"]

# Step 2: Calculate Mean, SD, Sum, and Average for each column
summary_stats = {}
for i, dataset in enumerate(datasets):
    means = np.mean(dataset, axis=0)
    sds = np.std(dataset, axis=0)
    sums = np.sum(dataset, axis=0)
    averages = np.mean(dataset, axis=1)

    summary_stats[dataset_labels[i]] = {
        "Mean": means,
        "SD": sds,
        "Sum": sums,
        "Average": averages
    }

# Step 3: Visualize the Mean and Sum for each dataset
for label, stats in summary_stats.items():
    # Mean Visualization
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Mean"])), stats["Mean"], color='skyblue', label="Mean")
    plt.xlabel('Columns')
    plt.ylabel('Mean Values')
    plt.title(f'Mean for {label}')
    plt.legend()
    plt.show()

    # Sum Visualization
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Sum"])), stats["Sum"], color='orange', label="Sum")
    plt.xlabel('Columns')
    plt.ylabel('Sum Values')
    plt.title(f'Sum for {label}')
    plt.legend()
    plt.show()

    # Average Visualization (per subject)
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Average"])), stats["Average"], color='green', label="Average per Subject")
    plt.xlabel('Subjects')
    plt.ylabel('Average Values')
    plt.title(f'Average per Subject in {label}')
    plt.legend()
    plt.show()
# Step 4: Compare Mean and Sum between subjects in each dataset
for label, stats in summary_stats.items():
    # Mean comparison
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Average"])), stats["Average"], color='green', label="Average per Subject")
    plt.xlabel('Subjects')
    plt.ylabel('Average Values')
    plt.title(f'Average in {label}')
    plt.legend()
    plt.show()

# Step 5: Statistical Comparison between Datasets
print("ANOVA Results (Mean Comparison):")
mean_anova_results = f_oneway(
    summary_stats["videodata"]["Mean"],
    summary_stats["videodata1"]["Mean"],
    summary_stats["videodata2"]["Mean"],
    summary_stats["videodata3"]["Mean"]
)
print(f"F-statistic: {mean_anova_results.statistic:.4f}, P-value: {mean_anova_results.pvalue:.4f}")

print("\nANOVA Results (Sum Comparison):")
sum_anova_results = f_oneway(
    summary_stats["videodata"]["Sum"],
    summary_stats["videodata1"]["Sum"],
    summary_stats["videodata2"]["Sum"],
    summary_stats["videodata3"]["Sum"]
)
print(f"F-statistic: {sum_anova_results.statistic:.4f}, P-value: {sum_anova_results.pvalue:.4f}")

# Pairwise Comparison for Means and Sums
print("\nPairwise T-tests (Mean Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Mean"],
            summary_stats[dataset_labels[j]]["Mean"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")

print("\nANOVA Results (Average Comparison):")
average_anova_results = f_oneway(
    summary_stats["videodata"]["Average"],
    summary_stats["videodata1"]["Average"],
    summary_stats["videodata2"]["Average"],
    summary_stats["videodata3"]["Average"]
)
print(f"F-statistic: {average_anova_results.statistic:.4f}, P-value: {average_anova_results.pvalue:.4f}")

print("\nPairwise T-tests (Sum Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Sum"],
            summary_stats[dataset_labels[j]]["Sum"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")
# Pairwise Comparison for Averages
print("\nPairwise T-tests (Average Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Average"],
            summary_stats[dataset_labels[j]]["Average"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")

# Ensure all datasets have the same number of rows
num_subjects = videodata.shape[0]
if any(dataset.shape[0] != num_subjects for dataset in [videodata1, videodata2, videodata3]):
    raise ValueError("All video datasets must have the same number of subjects (rows).")

# Calculate the sum for each subject in each video
sums_video1 = np.sum(videodata, axis=1)
sums_video2 = np.sum(videodata1, axis=1)
sums_video3 = np.sum(videodata2, axis=1)
sums_video4 = np.sum(videodata3, axis=1)

# Create a DataFrame to store the results
subjects = [f"Subject_{i+1}" for i in range(num_subjects)]  # Generate subject labels

sum_data = pd.DataFrame({
    "Subject": subjects,
    "Video1_Sum": sums_video1,
    "Video2_Sum": sums_video2,
    "Video3_Sum": sums_video3,
    "Video4_Sum": sums_video4
})

# Save the data to a CSV file
output_file = "subject_sums.csv"
sum_data.to_csv(output_file, index=False)

# Calculate correlation matrices using different methods
correlation_matrix_pearson = sum_data.drop("Subject", axis=1).corr(method="pearson")
correlation_matrix_spearman = sum_data.drop("Subject", axis=1).corr(method="spearman")
correlation_matrix_kendall = sum_data.drop("Subject", axis=1).corr(method="kendall")

# Print correlation matrices
print("Pearson Correlation Matrix:")
print(correlation_matrix_pearson)
print("\nSpearman Correlation Matrix:")
print(correlation_matrix_spearman)
print("\nKendall Correlation Matrix:")
print(correlation_matrix_kendall)

# Visualize the correlation matrices
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_pearson, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Pearson Correlation Matrix Between Videos (Based on Sums)")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_spearman, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Spearman Correlation Matrix Between Videos (Based on Sums)")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_kendall, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Kendall Correlation Matrix Between Videos (Based on Sums)")
plt.show()

# Determine the best model (pair with the highest correlation in Pearson)
best_pair_pearson = None
max_correlation_pearson = -1

for i, col1 in enumerate(correlation_matrix_pearson.columns):
    for j, col2 in enumerate(correlation_matrix_pearson.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_pearson.loc[col1, col2]
            if correlation > max_correlation_pearson:
                max_correlation_pearson = correlation
                best_pair_pearson = (col1, col2)

print(f"Best Correlated Pair (Pearson): {best_pair_pearson} with Correlation: {max_correlation_pearson:.2f}")

# Determine the best model (pair with the highest correlation in Spearman)
best_pair_spearman = None
max_correlation_spearman = -1

for i, col1 in enumerate(correlation_matrix_spearman.columns):
    for j, col2 in enumerate(correlation_matrix_spearman.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_spearman.loc[col1, col2]
            if correlation > max_correlation_spearman:
                max_correlation_spearman = correlation
                best_pair_spearman = (col1, col2)

print(f"Best Correlated Pair (Spearman): {best_pair_spearman} with Correlation: {max_correlation_spearman:.2f}")

# Determine the best model (pair with the highest correlation in Kendall)
best_pair_kendall = None
max_correlation_kendall = -1

for i, col1 in enumerate(correlation_matrix_kendall.columns):
    for j, col2 in enumerate(correlation_matrix_kendall.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_kendall.loc[col1, col2]
            if correlation > max_correlation_kendall:
                max_correlation_kendall = correlation
                best_pair_kendall = (col1, col2)

print(f"Best Correlated Pair (Kendall): {best_pair_kendall} with Correlation: {max_correlation_kendall:.2f}")

print(f"Sum for each subject saved to {output_file}")

"""NDARFV061AR5"""

videodata= np.load("/content/drive/MyDrive/Data TU PHD DUBLIN/subjects Data/NDARFV061AR5/evaluation/baseline_eLORETA.npy")  #base relax
videodata1= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/subjects Data/NDARFV061AR5/evaluation/video1_eLORETA.npy") #marowa's drive video1
videodata2= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/subjects Data/NDARFV061AR5/evaluation/video2_eLORETA.npy") #video2
videodata3= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/subjects Data/NDARFV061AR5/evaluation/video3_eLORETA.npy") #video3
#luca's drive
#pathLuca = "/content/drive/MyDrive/TU Dublin-NN/research/PHD CANDIDATES - AICL research LAB/CURRENT/Carlos Gomez Tapia - PB04482/dataset - full source activation space (31000x11250) for 35 subjects/"
#fileName = "video1_eLORETA.npy"
#videodata= np.load (pathLuca + fileName)
print(videodata1.shape)
print(len(videodata)) #len of points in brain (31553)
print(videodata[0])
print(len(videodata[0]))  #timestamps  (11250=125hz x 90 seconds)
print(type(videodata))
#video2data is a list of list (first list 31553 lenghth,each of this cell contains 11250 point,31553 is represented the number of the brain and the 11250 is the point in time  )
import numpy as np
import pandas as pd
from scipy.stats import f_oneway, ttest_ind
import matplotlib.pyplot as plt


datasets = [videodata, videodata1, videodata2, videodata3]
dataset_labels = ["videodata", "videodata1", "videodata2", "videodata3"]

# Step 2: Calculate Mean, SD, Sum, and Average for each column
summary_stats = {}
for i, dataset in enumerate(datasets):
    means = np.mean(dataset, axis=0)
    sds = np.std(dataset, axis=0)
    sums = np.sum(dataset, axis=0)
    averages = np.mean(dataset, axis=1)

    summary_stats[dataset_labels[i]] = {
        "Mean": means,
        "SD": sds,
        "Sum": sums,
        "Average": averages
    }

# Step 3: Visualize the Mean and Sum for each dataset
for label, stats in summary_stats.items():
    # Mean Visualization
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Mean"])), stats["Mean"], color='skyblue', label="Mean")
    plt.xlabel('Columns')
    plt.ylabel('Mean Values')
    plt.title(f'Mean for {label}')
    plt.legend()
    plt.show()

    # Sum Visualization
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Sum"])), stats["Sum"], color='orange', label="Sum")
    plt.xlabel('Columns')
    plt.ylabel('Sum Values')
    plt.title(f'Sum for {label}')
    plt.legend()
    plt.show()

    # Average Visualization (per subject)
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Average"])), stats["Average"], color='green', label="Average per Subject")
    plt.xlabel('Subjects')
    plt.ylabel('Average Values')
    plt.title(f'Average per Subject in {label}')
    plt.legend()
    plt.show()
# Step 4: Compare Mean and Sum between subjects in each dataset
for label, stats in summary_stats.items():
    # Mean comparison
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Average"])), stats["Average"], color='green', label="Average per Subject")
    plt.xlabel('Subjects')
    plt.ylabel('Average Values')
    plt.title(f'Average in {label}')
    plt.legend()
    plt.show()

# Step 5: Statistical Comparison between Datasets
print("ANOVA Results (Mean Comparison):")
mean_anova_results = f_oneway(
    summary_stats["videodata"]["Mean"],
    summary_stats["videodata1"]["Mean"],
    summary_stats["videodata2"]["Mean"],
    summary_stats["videodata3"]["Mean"]
)
print(f"F-statistic: {mean_anova_results.statistic:.4f}, P-value: {mean_anova_results.pvalue:.4f}")

print("\nANOVA Results (Sum Comparison):")
sum_anova_results = f_oneway(
    summary_stats["videodata"]["Sum"],
    summary_stats["videodata1"]["Sum"],
    summary_stats["videodata2"]["Sum"],
    summary_stats["videodata3"]["Sum"]
)
print(f"F-statistic: {sum_anova_results.statistic:.4f}, P-value: {sum_anova_results.pvalue:.4f}")

# Pairwise Comparison for Means and Sums
print("\nPairwise T-tests (Mean Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Mean"],
            summary_stats[dataset_labels[j]]["Mean"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")

print("\nANOVA Results (Average Comparison):")
average_anova_results = f_oneway(
    summary_stats["videodata"]["Average"],
    summary_stats["videodata1"]["Average"],
    summary_stats["videodata2"]["Average"],
    summary_stats["videodata3"]["Average"]
)
print(f"F-statistic: {average_anova_results.statistic:.4f}, P-value: {average_anova_results.pvalue:.4f}")

print("\nPairwise T-tests (Sum Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Sum"],
            summary_stats[dataset_labels[j]]["Sum"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")
# Pairwise Comparison for Averages
print("\nPairwise T-tests (Average Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Average"],
            summary_stats[dataset_labels[j]]["Average"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")

# Ensure all datasets have the same number of rows
num_subjects = videodata.shape[0]
if any(dataset.shape[0] != num_subjects for dataset in [videodata1, videodata2, videodata3]):
    raise ValueError("All video datasets must have the same number of subjects (rows).")

# Calculate the sum for each subject in each video
sums_video1 = np.sum(videodata, axis=1)
sums_video2 = np.sum(videodata1, axis=1)
sums_video3 = np.sum(videodata2, axis=1)
sums_video4 = np.sum(videodata3, axis=1)

# Create a DataFrame to store the results
subjects = [f"Subject_{i+1}" for i in range(num_subjects)]  # Generate subject labels

sum_data = pd.DataFrame({
    "Subject": subjects,
    "Video1_Sum": sums_video1,
    "Video2_Sum": sums_video2,
    "Video3_Sum": sums_video3,
    "Video4_Sum": sums_video4
})

# Save the data to a CSV file
output_file = "subject_sums.csv"
sum_data.to_csv(output_file, index=False)

# Calculate correlation matrices using different methods
correlation_matrix_pearson = sum_data.drop("Subject", axis=1).corr(method="pearson")
correlation_matrix_spearman = sum_data.drop("Subject", axis=1).corr(method="spearman")
correlation_matrix_kendall = sum_data.drop("Subject", axis=1).corr(method="kendall")

# Print correlation matrices
print("Pearson Correlation Matrix:")
print(correlation_matrix_pearson)
print("\nSpearman Correlation Matrix:")
print(correlation_matrix_spearman)
print("\nKendall Correlation Matrix:")
print(correlation_matrix_kendall)

# Visualize the correlation matrices
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_pearson, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Pearson Correlation Matrix Between Videos (Based on Sums)")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_spearman, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Spearman Correlation Matrix Between Videos (Based on Sums)")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_kendall, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Kendall Correlation Matrix Between Videos (Based on Sums)")
plt.show()

# Determine the best model (pair with the highest correlation in Pearson)
best_pair_pearson = None
max_correlation_pearson = -1

for i, col1 in enumerate(correlation_matrix_pearson.columns):
    for j, col2 in enumerate(correlation_matrix_pearson.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_pearson.loc[col1, col2]
            if correlation > max_correlation_pearson:
                max_correlation_pearson = correlation
                best_pair_pearson = (col1, col2)

print(f"Best Correlated Pair (Pearson): {best_pair_pearson} with Correlation: {max_correlation_pearson:.2f}")

# Determine the best model (pair with the highest correlation in Spearman)
best_pair_spearman = None
max_correlation_spearman = -1

for i, col1 in enumerate(correlation_matrix_spearman.columns):
    for j, col2 in enumerate(correlation_matrix_spearman.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_spearman.loc[col1, col2]
            if correlation > max_correlation_spearman:
                max_correlation_spearman = correlation
                best_pair_spearman = (col1, col2)

print(f"Best Correlated Pair (Spearman): {best_pair_spearman} with Correlation: {max_correlation_spearman:.2f}")

# Determine the best model (pair with the highest correlation in Kendall)
best_pair_kendall = None
max_correlation_kendall = -1

for i, col1 in enumerate(correlation_matrix_kendall.columns):
    for j, col2 in enumerate(correlation_matrix_kendall.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_kendall.loc[col1, col2]
            if correlation > max_correlation_kendall:
                max_correlation_kendall = correlation
                best_pair_kendall = (col1, col2)

print(f"Best Correlated Pair (Kendall): {best_pair_kendall} with Correlation: {max_correlation_kendall:.2f}")

print(f"Sum for each subject saved to {output_file}")

"""NDARFA737TG6"""

videodata= np.load("/content/drive/MyDrive/Data TU PHD DUBLIN/subjects Data/NDARFA737TG6/evaluation/baseline_eLORETA.npy")  #base relax
videodata1= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/subjects Data/NDARFA737TG6/evaluation/video1_eLORETA.npy") #marowa's drive video1
videodata2= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/subjects Data/NDARFA737TG6/evaluation/video2_eLORETA.npy") #video2
videodata3= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/subjects Data/NDARFA737TG6/evaluation/video3_eLORETA.npy") #video3
#luca's drive
#pathLuca = "/content/drive/MyDrive/TU Dublin-NN/research/PHD CANDIDATES - AICL research LAB/CURRENT/Carlos Gomez Tapia - PB04482/dataset - full source activation space (31000x11250) for 35 subjects/"
#fileName = "video1_eLORETA.npy"
#videodata= np.load (pathLuca + fileName)
print(videodata1.shape)
print(len(videodata)) #len of points in brain (31553)
print(videodata[0])
print(len(videodata[0]))  #timestamps  (11250=125hz x 90 seconds)
print(type(videodata))
#video2data is a list of list (first list 31553 lenghth,each of this cell contains 11250 point,31553 is represented the number of the brain and the 11250 is the point in time  )
import numpy as np
import pandas as pd
from scipy.stats import f_oneway, ttest_ind
import matplotlib.pyplot as plt


datasets = [videodata, videodata1, videodata2, videodata3]
dataset_labels = ["videodata", "videodata1", "videodata2", "videodata3"]

# Step 2: Calculate Mean, SD, Sum, and Average for each column
summary_stats = {}
for i, dataset in enumerate(datasets):
    means = np.mean(dataset, axis=0)
    sds = np.std(dataset, axis=0)
    sums = np.sum(dataset, axis=0)
    averages = np.mean(dataset, axis=1)

    summary_stats[dataset_labels[i]] = {
        "Mean": means,
        "SD": sds,
        "Sum": sums,
        "Average": averages
    }

# Step 3: Visualize the Mean and Sum for each dataset
for label, stats in summary_stats.items():
    # Mean Visualization
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Mean"])), stats["Mean"], color='skyblue', label="Mean")
    plt.xlabel('Columns')
    plt.ylabel('Mean Values')
    plt.title(f'Mean for {label}')
    plt.legend()
    plt.show()

    # Sum Visualization
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Sum"])), stats["Sum"], color='orange', label="Sum")
    plt.xlabel('Columns')
    plt.ylabel('Sum Values')
    plt.title(f'Sum for {label}')
    plt.legend()
    plt.show()

    # Average Visualization (per subject)
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Average"])), stats["Average"], color='green', label="Average per Subject")
    plt.xlabel('Subjects')
    plt.ylabel('Average Values')
    plt.title(f'Average per Subject in {label}')
    plt.legend()
    plt.show()
# Step 4: Compare Mean and Sum between subjects in each dataset
for label, stats in summary_stats.items():
    # Mean comparison
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Average"])), stats["Average"], color='green', label="Average per Subject")
    plt.xlabel('Subjects')
    plt.ylabel('Average Values')
    plt.title(f'Average in {label}')
    plt.legend()
    plt.show()

# Step 5: Statistical Comparison between Datasets
print("ANOVA Results (Mean Comparison):")
mean_anova_results = f_oneway(
    summary_stats["videodata"]["Mean"],
    summary_stats["videodata1"]["Mean"],
    summary_stats["videodata2"]["Mean"],
    summary_stats["videodata3"]["Mean"]
)
print(f"F-statistic: {mean_anova_results.statistic:.4f}, P-value: {mean_anova_results.pvalue:.4f}")

print("\nANOVA Results (Sum Comparison):")
sum_anova_results = f_oneway(
    summary_stats["videodata"]["Sum"],
    summary_stats["videodata1"]["Sum"],
    summary_stats["videodata2"]["Sum"],
    summary_stats["videodata3"]["Sum"]
)
print(f"F-statistic: {sum_anova_results.statistic:.4f}, P-value: {sum_anova_results.pvalue:.4f}")

# Pairwise Comparison for Means and Sums
print("\nPairwise T-tests (Mean Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Mean"],
            summary_stats[dataset_labels[j]]["Mean"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")

print("\nANOVA Results (Average Comparison):")
average_anova_results = f_oneway(
    summary_stats["videodata"]["Average"],
    summary_stats["videodata1"]["Average"],
    summary_stats["videodata2"]["Average"],
    summary_stats["videodata3"]["Average"]
)
print(f"F-statistic: {average_anova_results.statistic:.4f}, P-value: {average_anova_results.pvalue:.4f}")

print("\nPairwise T-tests (Sum Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Sum"],
            summary_stats[dataset_labels[j]]["Sum"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")
# Pairwise Comparison for Averages
print("\nPairwise T-tests (Average Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Average"],
            summary_stats[dataset_labels[j]]["Average"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")

# Ensure all datasets have the same number of rows
num_subjects = videodata.shape[0]
if any(dataset.shape[0] != num_subjects for dataset in [videodata1, videodata2, videodata3]):
    raise ValueError("All video datasets must have the same number of subjects (rows).")

# Calculate the sum for each subject in each video
sums_video1 = np.sum(videodata, axis=1)
sums_video2 = np.sum(videodata1, axis=1)
sums_video3 = np.sum(videodata2, axis=1)
sums_video4 = np.sum(videodata3, axis=1)

# Create a DataFrame to store the results
subjects = [f"Subject_{i+1}" for i in range(num_subjects)]  # Generate subject labels

sum_data = pd.DataFrame({
    "Subject": subjects,
    "Video1_Sum": sums_video1,
    "Video2_Sum": sums_video2,
    "Video3_Sum": sums_video3,
    "Video4_Sum": sums_video4
})

# Save the data to a CSV file
output_file = "subject_sums.csv"
sum_data.to_csv(output_file, index=False)

# Calculate correlation matrices using different methods
correlation_matrix_pearson = sum_data.drop("Subject", axis=1).corr(method="pearson")
correlation_matrix_spearman = sum_data.drop("Subject", axis=1).corr(method="spearman")
correlation_matrix_kendall = sum_data.drop("Subject", axis=1).corr(method="kendall")

# Print correlation matrices
print("Pearson Correlation Matrix:")
print(correlation_matrix_pearson)
print("\nSpearman Correlation Matrix:")
print(correlation_matrix_spearman)
print("\nKendall Correlation Matrix:")
print(correlation_matrix_kendall)

# Visualize the correlation matrices
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_pearson, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Pearson Correlation Matrix Between Videos (Based on Sums)")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_spearman, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Spearman Correlation Matrix Between Videos (Based on Sums)")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_kendall, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Kendall Correlation Matrix Between Videos (Based on Sums)")
plt.show()

# Determine the best model (pair with the highest correlation in Pearson)
best_pair_pearson = None
max_correlation_pearson = -1

for i, col1 in enumerate(correlation_matrix_pearson.columns):
    for j, col2 in enumerate(correlation_matrix_pearson.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_pearson.loc[col1, col2]
            if correlation > max_correlation_pearson:
                max_correlation_pearson = correlation
                best_pair_pearson = (col1, col2)

print(f"Best Correlated Pair (Pearson): {best_pair_pearson} with Correlation: {max_correlation_pearson:.2f}")

# Determine the best model (pair with the highest correlation in Spearman)
best_pair_spearman = None
max_correlation_spearman = -1

for i, col1 in enumerate(correlation_matrix_spearman.columns):
    for j, col2 in enumerate(correlation_matrix_spearman.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_spearman.loc[col1, col2]
            if correlation > max_correlation_spearman:
                max_correlation_spearman = correlation
                best_pair_spearman = (col1, col2)

print(f"Best Correlated Pair (Spearman): {best_pair_spearman} with Correlation: {max_correlation_spearman:.2f}")

# Determine the best model (pair with the highest correlation in Kendall)
best_pair_kendall = None
max_correlation_kendall = -1

for i, col1 in enumerate(correlation_matrix_kendall.columns):
    for j, col2 in enumerate(correlation_matrix_kendall.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_kendall.loc[col1, col2]
            if correlation > max_correlation_kendall:
                max_correlation_kendall = correlation
                best_pair_kendall = (col1, col2)

print(f"Best Correlated Pair (Kendall): {best_pair_kendall} with Correlation: {max_correlation_kendall:.2f}")

print(f"Sum for each subject saved to {output_file}")

"""NDAREV342ABE"""

videodata= np.load("/content/drive/MyDrive/Data TU PHD DUBLIN/subjects Data/NDAREV342ABE/evaluation/baseline_eLORETA.npy")  #base relax
videodata1= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/subjects Data/NDAREV342ABE/evaluation/video1_eLORETA.npy") #marowa's drive video1
videodata2= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/subjects Data/NDAREV342ABE/evaluation/video2_eLORETA.npy") #video2
videodata3= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/subjects Data/NDAREV342ABE/evaluation/video3_eLORETA.npy") #video3
#luca's drive
#pathLuca = "/content/drive/MyDrive/TU Dublin-NN/research/PHD CANDIDATES - AICL research LAB/CURRENT/Carlos Gomez Tapia - PB04482/dataset - full source activation space (31000x11250) for 35 subjects/"
#fileName = "video1_eLORETA.npy"
#videodata= np.load (pathLuca + fileName)
print(videodata1.shape)
print(len(videodata)) #len of points in brain (31553)
print(videodata[0])
print(len(videodata[0]))  #timestamps  (11250=125hz x 90 seconds)
print(type(videodata))
#video2data is a list of list (first list 31553 lenghth,each of this cell contains 11250 point,31553 is represented the number of the brain and the 11250 is the point in time  )
import numpy as np
import pandas as pd
from scipy.stats import f_oneway, ttest_ind
import matplotlib.pyplot as plt


datasets = [videodata, videodata1, videodata2, videodata3]
dataset_labels = ["videodata", "videodata1", "videodata2", "videodata3"]

# Step 2: Calculate Mean, SD, Sum, and Average for each column
summary_stats = {}
for i, dataset in enumerate(datasets):
    means = np.mean(dataset, axis=0)
    sds = np.std(dataset, axis=0)
    sums = np.sum(dataset, axis=0)
    averages = np.mean(dataset, axis=1)

    summary_stats[dataset_labels[i]] = {
        "Mean": means,
        "SD": sds,
        "Sum": sums,
        "Average": averages
    }

# Step 3: Visualize the Mean and Sum for each dataset
for label, stats in summary_stats.items():
    # Mean Visualization
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Mean"])), stats["Mean"], color='skyblue', label="Mean")
    plt.xlabel('Columns')
    plt.ylabel('Mean Values')
    plt.title(f'Mean for {label}')
    plt.legend()
    plt.show()

    # Sum Visualization
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Sum"])), stats["Sum"], color='orange', label="Sum")
    plt.xlabel('Columns')
    plt.ylabel('Sum Values')
    plt.title(f'Sum for {label}')
    plt.legend()
    plt.show()

    # Average Visualization (per subject)
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Average"])), stats["Average"], color='green', label="Average per Subject")
    plt.xlabel('Subjects')
    plt.ylabel('Average Values')
    plt.title(f'Average per Subject in {label}')
    plt.legend()
    plt.show()
# Step 4: Compare Mean and Sum between subjects in each dataset
for label, stats in summary_stats.items():
    # Mean comparison
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Average"])), stats["Average"], color='green', label="Average per Subject")
    plt.xlabel('Subjects')
    plt.ylabel('Average Values')
    plt.title(f'Average in {label}')
    plt.legend()
    plt.show()

# Step 5: Statistical Comparison between Datasets
print("ANOVA Results (Mean Comparison):")
mean_anova_results = f_oneway(
    summary_stats["videodata"]["Mean"],
    summary_stats["videodata1"]["Mean"],
    summary_stats["videodata2"]["Mean"],
    summary_stats["videodata3"]["Mean"]
)
print(f"F-statistic: {mean_anova_results.statistic:.4f}, P-value: {mean_anova_results.pvalue:.4f}")

print("\nANOVA Results (Sum Comparison):")
sum_anova_results = f_oneway(
    summary_stats["videodata"]["Sum"],
    summary_stats["videodata1"]["Sum"],
    summary_stats["videodata2"]["Sum"],
    summary_stats["videodata3"]["Sum"]
)
print(f"F-statistic: {sum_anova_results.statistic:.4f}, P-value: {sum_anova_results.pvalue:.4f}")

# Pairwise Comparison for Means and Sums
print("\nPairwise T-tests (Mean Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Mean"],
            summary_stats[dataset_labels[j]]["Mean"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")

print("\nANOVA Results (Average Comparison):")
average_anova_results = f_oneway(
    summary_stats["videodata"]["Average"],
    summary_stats["videodata1"]["Average"],
    summary_stats["videodata2"]["Average"],
    summary_stats["videodata3"]["Average"]
)
print(f"F-statistic: {average_anova_results.statistic:.4f}, P-value: {average_anova_results.pvalue:.4f}")

print("\nPairwise T-tests (Sum Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Sum"],
            summary_stats[dataset_labels[j]]["Sum"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")
# Pairwise Comparison for Averages
print("\nPairwise T-tests (Average Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Average"],
            summary_stats[dataset_labels[j]]["Average"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")

# Ensure all datasets have the same number of rows
num_subjects = videodata.shape[0]
if any(dataset.shape[0] != num_subjects for dataset in [videodata1, videodata2, videodata3]):
    raise ValueError("All video datasets must have the same number of subjects (rows).")

# Calculate the sum for each subject in each video
sums_video1 = np.sum(videodata, axis=1)
sums_video2 = np.sum(videodata1, axis=1)
sums_video3 = np.sum(videodata2, axis=1)
sums_video4 = np.sum(videodata3, axis=1)

# Create a DataFrame to store the results
subjects = [f"Subject_{i+1}" for i in range(num_subjects)]  # Generate subject labels

sum_data = pd.DataFrame({
    "Subject": subjects,
    "Video1_Sum": sums_video1,
    "Video2_Sum": sums_video2,
    "Video3_Sum": sums_video3,
    "Video4_Sum": sums_video4
})

# Save the data to a CSV file
output_file = "subject_sums.csv"
sum_data.to_csv(output_file, index=False)

# Calculate correlation matrices using different methods
correlation_matrix_pearson = sum_data.drop("Subject", axis=1).corr(method="pearson")
correlation_matrix_spearman = sum_data.drop("Subject", axis=1).corr(method="spearman")
correlation_matrix_kendall = sum_data.drop("Subject", axis=1).corr(method="kendall")

# Print correlation matrices
print("Pearson Correlation Matrix:")
print(correlation_matrix_pearson)
print("\nSpearman Correlation Matrix:")
print(correlation_matrix_spearman)
print("\nKendall Correlation Matrix:")
print(correlation_matrix_kendall)

# Visualize the correlation matrices
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_pearson, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Pearson Correlation Matrix Between Videos (Based on Sums)")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_spearman, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Spearman Correlation Matrix Between Videos (Based on Sums)")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_kendall, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Kendall Correlation Matrix Between Videos (Based on Sums)")
plt.show()

# Determine the best model (pair with the highest correlation in Pearson)
best_pair_pearson = None
max_correlation_pearson = -1

for i, col1 in enumerate(correlation_matrix_pearson.columns):
    for j, col2 in enumerate(correlation_matrix_pearson.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_pearson.loc[col1, col2]
            if correlation > max_correlation_pearson:
                max_correlation_pearson = correlation
                best_pair_pearson = (col1, col2)

print(f"Best Correlated Pair (Pearson): {best_pair_pearson} with Correlation: {max_correlation_pearson:.2f}")

# Determine the best model (pair with the highest correlation in Spearman)
best_pair_spearman = None
max_correlation_spearman = -1

for i, col1 in enumerate(correlation_matrix_spearman.columns):
    for j, col2 in enumerate(correlation_matrix_spearman.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_spearman.loc[col1, col2]
            if correlation > max_correlation_spearman:
                max_correlation_spearman = correlation
                best_pair_spearman = (col1, col2)

print(f"Best Correlated Pair (Spearman): {best_pair_spearman} with Correlation: {max_correlation_spearman:.2f}")

# Determine the best model (pair with the highest correlation in Kendall)
best_pair_kendall = None
max_correlation_kendall = -1

for i, col1 in enumerate(correlation_matrix_kendall.columns):
    for j, col2 in enumerate(correlation_matrix_kendall.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_kendall.loc[col1, col2]
            if correlation > max_correlation_kendall:
                max_correlation_kendall = correlation
                best_pair_kendall = (col1, col2)

print(f"Best Correlated Pair (Kendall): {best_pair_kendall} with Correlation: {max_correlation_kendall:.2f}")

print(f"Sum for each subject saved to {output_file}")

"""NDARCT889DMB"""

videodata= np.load("/content/drive/MyDrive/Data TU PHD DUBLIN/subjects Data/NDARCT889DMB/evaluation/baseline_eLORETA.npy")  #base relax
videodata1= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/subjects Data/NDARCT889DMB/evaluation/video1_eLORETA.npy") #marowa's drive video1
videodata2= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/subjects Data/NDARCT889DMB/evaluation/video2_eLORETA.npy") #video2
videodata3= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/subjects Data/NDARCT889DMB/evaluation/video3_eLORETA.npy") #video3
#luca's drive
#pathLuca = "/content/drive/MyDrive/TU Dublin-NN/research/PHD CANDIDATES - AICL research LAB/CURRENT/Carlos Gomez Tapia - PB04482/dataset - full source activation space (31000x11250) for 35 subjects/"
#fileName = "video1_eLORETA.npy"
#videodata= np.load (pathLuca + fileName)
print(videodata1.shape)
print(len(videodata)) #len of points in brain (31553)
print(videodata[0])
print(len(videodata[0]))  #timestamps  (11250=125hz x 90 seconds)
print(type(videodata))
#video2data is a list of list (first list 31553 lenghth,each of this cell contains 11250 point,31553 is represented the number of the brain and the 11250 is the point in time  )
import numpy as np
import pandas as pd
from scipy.stats import f_oneway, ttest_ind
import matplotlib.pyplot as plt


datasets = [videodata, videodata1, videodata2, videodata3]
dataset_labels = ["videodata", "videodata1", "videodata2", "videodata3"]

# Step 2: Calculate Mean, SD, Sum, and Average for each column
summary_stats = {}
for i, dataset in enumerate(datasets):
    means = np.mean(dataset, axis=0)
    sds = np.std(dataset, axis=0)
    sums = np.sum(dataset, axis=0)
    averages = np.mean(dataset, axis=1)

    summary_stats[dataset_labels[i]] = {
        "Mean": means,
        "SD": sds,
        "Sum": sums,
        "Average": averages
    }

# Step 3: Visualize the Mean and Sum for each dataset
for label, stats in summary_stats.items():
    # Mean Visualization
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Mean"])), stats["Mean"], color='skyblue', label="Mean")
    plt.xlabel('Columns')
    plt.ylabel('Mean Values')
    plt.title(f'Mean for {label}')
    plt.legend()
    plt.show()

    # Sum Visualization
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Sum"])), stats["Sum"], color='orange', label="Sum")
    plt.xlabel('Columns')
    plt.ylabel('Sum Values')
    plt.title(f'Sum for {label}')
    plt.legend()
    plt.show()

    # Average Visualization (per subject)
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Average"])), stats["Average"], color='green', label="Average per Subject")
    plt.xlabel('Subjects')
    plt.ylabel('Average Values')
    plt.title(f'Average per Subject in {label}')
    plt.legend()
    plt.show()
# Step 4: Compare Mean and Sum between subjects in each dataset
for label, stats in summary_stats.items():
    # Mean comparison
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Average"])), stats["Average"], color='green', label="Average per Subject")
    plt.xlabel('Subjects')
    plt.ylabel('Average Values')
    plt.title(f'Average in {label}')
    plt.legend()
    plt.show()

# Step 5: Statistical Comparison between Datasets
print("ANOVA Results (Mean Comparison):")
mean_anova_results = f_oneway(
    summary_stats["videodata"]["Mean"],
    summary_stats["videodata1"]["Mean"],
    summary_stats["videodata2"]["Mean"],
    summary_stats["videodata3"]["Mean"]
)
print(f"F-statistic: {mean_anova_results.statistic:.4f}, P-value: {mean_anova_results.pvalue:.4f}")

print("\nANOVA Results (Sum Comparison):")
sum_anova_results = f_oneway(
    summary_stats["videodata"]["Sum"],
    summary_stats["videodata1"]["Sum"],
    summary_stats["videodata2"]["Sum"],
    summary_stats["videodata3"]["Sum"]
)
print(f"F-statistic: {sum_anova_results.statistic:.4f}, P-value: {sum_anova_results.pvalue:.4f}")

# Pairwise Comparison for Means and Sums
print("\nPairwise T-tests (Mean Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Mean"],
            summary_stats[dataset_labels[j]]["Mean"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")

print("\nANOVA Results (Average Comparison):")
average_anova_results = f_oneway(
    summary_stats["videodata"]["Average"],
    summary_stats["videodata1"]["Average"],
    summary_stats["videodata2"]["Average"],
    summary_stats["videodata3"]["Average"]
)
print(f"F-statistic: {average_anova_results.statistic:.4f}, P-value: {average_anova_results.pvalue:.4f}")

print("\nPairwise T-tests (Sum Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Sum"],
            summary_stats[dataset_labels[j]]["Sum"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")
# Pairwise Comparison for Averages
print("\nPairwise T-tests (Average Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Average"],
            summary_stats[dataset_labels[j]]["Average"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")

# Ensure all datasets have the same number of rows
num_subjects = videodata.shape[0]
if any(dataset.shape[0] != num_subjects for dataset in [videodata1, videodata2, videodata3]):
    raise ValueError("All video datasets must have the same number of subjects (rows).")

# Calculate the sum for each subject in each video
sums_video1 = np.sum(videodata, axis=1)
sums_video2 = np.sum(videodata1, axis=1)
sums_video3 = np.sum(videodata2, axis=1)
sums_video4 = np.sum(videodata3, axis=1)

# Create a DataFrame to store the results
subjects = [f"Subject_{i+1}" for i in range(num_subjects)]  # Generate subject labels

sum_data = pd.DataFrame({
    "Subject": subjects,
    "Video1_Sum": sums_video1,
    "Video2_Sum": sums_video2,
    "Video3_Sum": sums_video3,
    "Video4_Sum": sums_video4
})

# Save the data to a CSV file
output_file = "subject_sums.csv"
sum_data.to_csv(output_file, index=False)

# Calculate correlation matrices using different methods
correlation_matrix_pearson = sum_data.drop("Subject", axis=1).corr(method="pearson")
correlation_matrix_spearman = sum_data.drop("Subject", axis=1).corr(method="spearman")
correlation_matrix_kendall = sum_data.drop("Subject", axis=1).corr(method="kendall")

# Print correlation matrices
print("Pearson Correlation Matrix:")
print(correlation_matrix_pearson)
print("\nSpearman Correlation Matrix:")
print(correlation_matrix_spearman)
print("\nKendall Correlation Matrix:")
print(correlation_matrix_kendall)

# Visualize the correlation matrices
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_pearson, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Pearson Correlation Matrix Between Videos (Based on Sums)")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_spearman, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Spearman Correlation Matrix Between Videos (Based on Sums)")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_kendall, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Kendall Correlation Matrix Between Videos (Based on Sums)")
plt.show()

# Determine the best model (pair with the highest correlation in Pearson)
best_pair_pearson = None
max_correlation_pearson = -1

for i, col1 in enumerate(correlation_matrix_pearson.columns):
    for j, col2 in enumerate(correlation_matrix_pearson.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_pearson.loc[col1, col2]
            if correlation > max_correlation_pearson:
                max_correlation_pearson = correlation
                best_pair_pearson = (col1, col2)

print(f"Best Correlated Pair (Pearson): {best_pair_pearson} with Correlation: {max_correlation_pearson:.2f}")

# Determine the best model (pair with the highest correlation in Spearman)
best_pair_spearman = None
max_correlation_spearman = -1

for i, col1 in enumerate(correlation_matrix_spearman.columns):
    for j, col2 in enumerate(correlation_matrix_spearman.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_spearman.loc[col1, col2]
            if correlation > max_correlation_spearman:
                max_correlation_spearman = correlation
                best_pair_spearman = (col1, col2)

print(f"Best Correlated Pair (Spearman): {best_pair_spearman} with Correlation: {max_correlation_spearman:.2f}")

# Determine the best model (pair with the highest correlation in Kendall)
best_pair_kendall = None
max_correlation_kendall = -1

for i, col1 in enumerate(correlation_matrix_kendall.columns):
    for j, col2 in enumerate(correlation_matrix_kendall.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_kendall.loc[col1, col2]
            if correlation > max_correlation_kendall:
                max_correlation_kendall = correlation
                best_pair_kendall = (col1, col2)

print(f"Best Correlated Pair (Kendall): {best_pair_kendall} with Correlation: {max_correlation_kendall:.2f}")

print(f"Sum for each subject saved to {output_file}")

"""NDARCD401HGZ"""

videodata= np.load("/content/drive/MyDrive/Data TU PHD DUBLIN/subjects Data/NDARCD401HGZ/evaluation/baseline_eLORETA.npy")  #base relax
videodata1= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/subjects Data/NDARCD401HGZ/evaluation/video1_eLORETA.npy") #marowa's drive video1
videodata2= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/subjects Data/NDARCD401HGZ/evaluation/video2_eLORETA.npy") #video2
videodata3= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/subjects Data/NDARCD401HGZ/evaluation/video3_eLORETA.npy") #video3
#luca's drive
#pathLuca = "/content/drive/MyDrive/TU Dublin-NN/research/PHD CANDIDATES - AICL research LAB/CURRENT/Carlos Gomez Tapia - PB04482/dataset - full source activation space (31000x11250) for 35 subjects/"
#fileName = "video1_eLORETA.npy"
#videodata= np.load (pathLuca + fileName)
print(videodata1.shape)
print(len(videodata)) #len of points in brain (31553)
print(videodata[0])
print(len(videodata[0]))  #timestamps  (11250=125hz x 90 seconds)
print(type(videodata))
#video2data is a list of list (first list 31553 lenghth,each of this cell contains 11250 point,31553 is represented the number of the brain and the 11250 is the point in time  )
import numpy as np
import pandas as pd
from scipy.stats import f_oneway, ttest_ind
import matplotlib.pyplot as plt


datasets = [videodata, videodata1, videodata2, videodata3]
dataset_labels = ["videodata", "videodata1", "videodata2", "videodata3"]

# Step 2: Calculate Mean, SD, Sum, and Average for each column
summary_stats = {}
for i, dataset in enumerate(datasets):
    means = np.mean(dataset, axis=0)
    sds = np.std(dataset, axis=0)
    sums = np.sum(dataset, axis=0)
    averages = np.mean(dataset, axis=1)

    summary_stats[dataset_labels[i]] = {
        "Mean": means,
        "SD": sds,
        "Sum": sums,
        "Average": averages
    }

# Step 3: Visualize the Mean and Sum for each dataset
for label, stats in summary_stats.items():
    # Mean Visualization
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Mean"])), stats["Mean"], color='skyblue', label="Mean")
    plt.xlabel('Columns')
    plt.ylabel('Mean Values')
    plt.title(f'Mean for {label}')
    plt.legend()
    plt.show()

    # Sum Visualization
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Sum"])), stats["Sum"], color='orange', label="Sum")
    plt.xlabel('Columns')
    plt.ylabel('Sum Values')
    plt.title(f'Sum for {label}')
    plt.legend()
    plt.show()

    # Average Visualization (per subject)
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Average"])), stats["Average"], color='green', label="Average per Subject")
    plt.xlabel('Subjects')
    plt.ylabel('Average Values')
    plt.title(f'Average per Subject in {label}')
    plt.legend()
    plt.show()
# Step 4: Compare Mean and Sum between subjects in each dataset
for label, stats in summary_stats.items():
    # Mean comparison
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Average"])), stats["Average"], color='green', label="Average per Subject")
    plt.xlabel('Subjects')
    plt.ylabel('Average Values')
    plt.title(f'Average in {label}')
    plt.legend()
    plt.show()

# Step 5: Statistical Comparison between Datasets
print("ANOVA Results (Mean Comparison):")
mean_anova_results = f_oneway(
    summary_stats["videodata"]["Mean"],
    summary_stats["videodata1"]["Mean"],
    summary_stats["videodata2"]["Mean"],
    summary_stats["videodata3"]["Mean"]
)
print(f"F-statistic: {mean_anova_results.statistic:.4f}, P-value: {mean_anova_results.pvalue:.4f}")

print("\nANOVA Results (Sum Comparison):")
sum_anova_results = f_oneway(
    summary_stats["videodata"]["Sum"],
    summary_stats["videodata1"]["Sum"],
    summary_stats["videodata2"]["Sum"],
    summary_stats["videodata3"]["Sum"]
)
print(f"F-statistic: {sum_anova_results.statistic:.4f}, P-value: {sum_anova_results.pvalue:.4f}")

# Pairwise Comparison for Means and Sums
print("\nPairwise T-tests (Mean Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Mean"],
            summary_stats[dataset_labels[j]]["Mean"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")

print("\nANOVA Results (Average Comparison):")
average_anova_results = f_oneway(
    summary_stats["videodata"]["Average"],
    summary_stats["videodata1"]["Average"],
    summary_stats["videodata2"]["Average"],
    summary_stats["videodata3"]["Average"]
)
print(f"F-statistic: {average_anova_results.statistic:.4f}, P-value: {average_anova_results.pvalue:.4f}")

print("\nPairwise T-tests (Sum Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Sum"],
            summary_stats[dataset_labels[j]]["Sum"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")
# Pairwise Comparison for Averages
print("\nPairwise T-tests (Average Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Average"],
            summary_stats[dataset_labels[j]]["Average"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")

# Ensure all datasets have the same number of rows
num_subjects = videodata.shape[0]
if any(dataset.shape[0] != num_subjects for dataset in [videodata1, videodata2, videodata3]):
    raise ValueError("All video datasets must have the same number of subjects (rows).")

# Calculate the sum for each subject in each video
sums_video1 = np.sum(videodata, axis=1)
sums_video2 = np.sum(videodata1, axis=1)
sums_video3 = np.sum(videodata2, axis=1)
sums_video4 = np.sum(videodata3, axis=1)

# Create a DataFrame to store the results
subjects = [f"Subject_{i+1}" for i in range(num_subjects)]  # Generate subject labels

sum_data = pd.DataFrame({
    "Subject": subjects,
    "Video1_Sum": sums_video1,
    "Video2_Sum": sums_video2,
    "Video3_Sum": sums_video3,
    "Video4_Sum": sums_video4
})

# Save the data to a CSV file
output_file = "subject_sums.csv"
sum_data.to_csv(output_file, index=False)

# Calculate correlation matrices using different methods
correlation_matrix_pearson = sum_data.drop("Subject", axis=1).corr(method="pearson")
correlation_matrix_spearman = sum_data.drop("Subject", axis=1).corr(method="spearman")
correlation_matrix_kendall = sum_data.drop("Subject", axis=1).corr(method="kendall")

# Print correlation matrices
print("Pearson Correlation Matrix:")
print(correlation_matrix_pearson)
print("\nSpearman Correlation Matrix:")
print(correlation_matrix_spearman)
print("\nKendall Correlation Matrix:")
print(correlation_matrix_kendall)

# Visualize the correlation matrices
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_pearson, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Pearson Correlation Matrix Between Videos (Based on Sums)")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_spearman, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Spearman Correlation Matrix Between Videos (Based on Sums)")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_kendall, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Kendall Correlation Matrix Between Videos (Based on Sums)")
plt.show()

# Determine the best model (pair with the highest correlation in Pearson)
best_pair_pearson = None
max_correlation_pearson = -1

for i, col1 in enumerate(correlation_matrix_pearson.columns):
    for j, col2 in enumerate(correlation_matrix_pearson.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_pearson.loc[col1, col2]
            if correlation > max_correlation_pearson:
                max_correlation_pearson = correlation
                best_pair_pearson = (col1, col2)

print(f"Best Correlated Pair (Pearson): {best_pair_pearson} with Correlation: {max_correlation_pearson:.2f}")

# Determine the best model (pair with the highest correlation in Spearman)
best_pair_spearman = None
max_correlation_spearman = -1

for i, col1 in enumerate(correlation_matrix_spearman.columns):
    for j, col2 in enumerate(correlation_matrix_spearman.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_spearman.loc[col1, col2]
            if correlation > max_correlation_spearman:
                max_correlation_spearman = correlation
                best_pair_spearman = (col1, col2)

print(f"Best Correlated Pair (Spearman): {best_pair_spearman} with Correlation: {max_correlation_spearman:.2f}")

# Determine the best model (pair with the highest correlation in Kendall)
best_pair_kendall = None
max_correlation_kendall = -1

for i, col1 in enumerate(correlation_matrix_kendall.columns):
    for j, col2 in enumerate(correlation_matrix_kendall.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_kendall.loc[col1, col2]
            if correlation > max_correlation_kendall:
                max_correlation_kendall = correlation
                best_pair_kendall = (col1, col2)

print(f"Best Correlated Pair (Kendall): {best_pair_kendall} with Correlation: {max_correlation_kendall:.2f}")

print(f"Sum for each subject saved to {output_file}")

"""NDARAP457WB5"""

videodata= np.load("/content/drive/MyDrive/Data TU PHD DUBLIN/subjects Data/NDARAP457WB5/evaluation/baseline_eLORETA.npy")  #base relax
videodata1= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/subjects Data/NDARAP457WB5/evaluation/video1_eLORETA.npy") #marowa's drive video1
videodata2= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/subjects Data/NDARAP457WB5/evaluation/video2_eLORETA.npy") #video2
videodata3= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/subjects Data/NDARAP457WB5/evaluation/video3_eLORETA.npy") #video3
#luca's drive
#pathLuca = "/content/drive/MyDrive/TU Dublin-NN/research/PHD CANDIDATES - AICL research LAB/CURRENT/Carlos Gomez Tapia - PB04482/dataset - full source activation space (31000x11250) for 35 subjects/"
#fileName = "video1_eLORETA.npy"
#videodata= np.load (pathLuca + fileName)
print(videodata1.shape)
print(len(videodata)) #len of points in brain (31553)
print(videodata[0])
print(len(videodata[0]))  #timestamps  (11250=125hz x 90 seconds)
print(type(videodata))
#video2data is a list of list (first list 31553 lenghth,each of this cell contains 11250 point,31553 is represented the number of the brain and the 11250 is the point in time  )
import numpy as np
import pandas as pd
from scipy.stats import f_oneway, ttest_ind
import matplotlib.pyplot as plt


datasets = [videodata, videodata1, videodata2, videodata3]
dataset_labels = ["videodata", "videodata1", "videodata2", "videodata3"]

# Step 2: Calculate Mean, SD, Sum, and Average for each column
summary_stats = {}
for i, dataset in enumerate(datasets):
    means = np.mean(dataset, axis=0)
    sds = np.std(dataset, axis=0)
    sums = np.sum(dataset, axis=0)
    averages = np.mean(dataset, axis=1)

    summary_stats[dataset_labels[i]] = {
        "Mean": means,
        "SD": sds,
        "Sum": sums,
        "Average": averages
    }

# Step 3: Visualize the Mean and Sum for each dataset
for label, stats in summary_stats.items():
    # Mean Visualization
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Mean"])), stats["Mean"], color='skyblue', label="Mean")
    plt.xlabel('Columns')
    plt.ylabel('Mean Values')
    plt.title(f'Mean for {label}')
    plt.legend()
    plt.show()

    # Sum Visualization
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Sum"])), stats["Sum"], color='orange', label="Sum")
    plt.xlabel('Columns')
    plt.ylabel('Sum Values')
    plt.title(f'Sum for {label}')
    plt.legend()
    plt.show()

    # Average Visualization (per subject)
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Average"])), stats["Average"], color='green', label="Average per Subject")
    plt.xlabel('Subjects')
    plt.ylabel('Average Values')
    plt.title(f'Average per Subject in {label}')
    plt.legend()
    plt.show()
# Step 4: Compare Mean and Sum between subjects in each dataset
for label, stats in summary_stats.items():
    # Mean comparison
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Average"])), stats["Average"], color='green', label="Average per Subject")
    plt.xlabel('Subjects')
    plt.ylabel('Average Values')
    plt.title(f'Average in {label}')
    plt.legend()
    plt.show()

# Step 5: Statistical Comparison between Datasets
print("ANOVA Results (Mean Comparison):")
mean_anova_results = f_oneway(
    summary_stats["videodata"]["Mean"],
    summary_stats["videodata1"]["Mean"],
    summary_stats["videodata2"]["Mean"],
    summary_stats["videodata3"]["Mean"]
)
print(f"F-statistic: {mean_anova_results.statistic:.4f}, P-value: {mean_anova_results.pvalue:.4f}")

print("\nANOVA Results (Sum Comparison):")
sum_anova_results = f_oneway(
    summary_stats["videodata"]["Sum"],
    summary_stats["videodata1"]["Sum"],
    summary_stats["videodata2"]["Sum"],
    summary_stats["videodata3"]["Sum"]
)
print(f"F-statistic: {sum_anova_results.statistic:.4f}, P-value: {sum_anova_results.pvalue:.4f}")

# Pairwise Comparison for Means and Sums
print("\nPairwise T-tests (Mean Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Mean"],
            summary_stats[dataset_labels[j]]["Mean"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")

print("\nANOVA Results (Average Comparison):")
average_anova_results = f_oneway(
    summary_stats["videodata"]["Average"],
    summary_stats["videodata1"]["Average"],
    summary_stats["videodata2"]["Average"],
    summary_stats["videodata3"]["Average"]
)
print(f"F-statistic: {average_anova_results.statistic:.4f}, P-value: {average_anova_results.pvalue:.4f}")

print("\nPairwise T-tests (Sum Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Sum"],
            summary_stats[dataset_labels[j]]["Sum"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")
# Pairwise Comparison for Averages
print("\nPairwise T-tests (Average Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Average"],
            summary_stats[dataset_labels[j]]["Average"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")

# Ensure all datasets have the same number of rows
num_subjects = videodata.shape[0]
if any(dataset.shape[0] != num_subjects for dataset in [videodata1, videodata2, videodata3]):
    raise ValueError("All video datasets must have the same number of subjects (rows).")

# Calculate the sum for each subject in each video
sums_video1 = np.sum(videodata, axis=1)
sums_video2 = np.sum(videodata1, axis=1)
sums_video3 = np.sum(videodata2, axis=1)
sums_video4 = np.sum(videodata3, axis=1)

# Create a DataFrame to store the results
subjects = [f"Subject_{i+1}" for i in range(num_subjects)]  # Generate subject labels

sum_data = pd.DataFrame({
    "Subject": subjects,
    "Video1_Sum": sums_video1,
    "Video2_Sum": sums_video2,
    "Video3_Sum": sums_video3,
    "Video4_Sum": sums_video4
})

# Save the data to a CSV file
output_file = "subject_sums.csv"
sum_data.to_csv(output_file, index=False)

# Calculate correlation matrices using different methods
correlation_matrix_pearson = sum_data.drop("Subject", axis=1).corr(method="pearson")
correlation_matrix_spearman = sum_data.drop("Subject", axis=1).corr(method="spearman")
correlation_matrix_kendall = sum_data.drop("Subject", axis=1).corr(method="kendall")

# Print correlation matrices
print("Pearson Correlation Matrix:")
print(correlation_matrix_pearson)
print("\nSpearman Correlation Matrix:")
print(correlation_matrix_spearman)
print("\nKendall Correlation Matrix:")
print(correlation_matrix_kendall)

# Visualize the correlation matrices
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_pearson, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Pearson Correlation Matrix Between Videos (Based on Sums)")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_spearman, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Spearman Correlation Matrix Between Videos (Based on Sums)")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_kendall, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Kendall Correlation Matrix Between Videos (Based on Sums)")
plt.show()

# Determine the best model (pair with the highest correlation in Pearson)
best_pair_pearson = None
max_correlation_pearson = -1

for i, col1 in enumerate(correlation_matrix_pearson.columns):
    for j, col2 in enumerate(correlation_matrix_pearson.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_pearson.loc[col1, col2]
            if correlation > max_correlation_pearson:
                max_correlation_pearson = correlation
                best_pair_pearson = (col1, col2)

print(f"Best Correlated Pair (Pearson): {best_pair_pearson} with Correlation: {max_correlation_pearson:.2f}")

# Determine the best model (pair with the highest correlation in Spearman)
best_pair_spearman = None
max_correlation_spearman = -1

for i, col1 in enumerate(correlation_matrix_spearman.columns):
    for j, col2 in enumerate(correlation_matrix_spearman.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_spearman.loc[col1, col2]
            if correlation > max_correlation_spearman:
                max_correlation_spearman = correlation
                best_pair_spearman = (col1, col2)

print(f"Best Correlated Pair (Spearman): {best_pair_spearman} with Correlation: {max_correlation_spearman:.2f}")

# Determine the best model (pair with the highest correlation in Kendall)
best_pair_kendall = None
max_correlation_kendall = -1

for i, col1 in enumerate(correlation_matrix_kendall.columns):
    for j, col2 in enumerate(correlation_matrix_kendall.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_kendall.loc[col1, col2]
            if correlation > max_correlation_kendall:
                max_correlation_kendall = correlation
                best_pair_kendall = (col1, col2)

print(f"Best Correlated Pair (Kendall): {best_pair_kendall} with Correlation: {max_correlation_kendall:.2f}")

print(f"Sum for each subject saved to {output_file}")

"""NDARRD720XZK"""

videodata= np.load("/content/drive/MyDrive/Data TU PHD DUBLIN/subjects Data/NDARRD720XZK/evaluation/baseline_eLORETA.npy")  #base relax
videodata1= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/subjects Data/NDARRD720XZK/evaluation/video1_eLORETA.npy") #marowa's drive video1
videodata2= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/subjects Data/NDARRD720XZK/evaluation/video2_eLORETA.npy") #video2
videodata3= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/subjects Data/NDARRD720XZK/evaluation/video3_eLORETA.npy") #video3
#luca's drive
#pathLuca = "/content/drive/MyDrive/TU Dublin-NN/research/PHD CANDIDATES - AICL research LAB/CURRENT/Carlos Gomez Tapia - PB04482/dataset - full source activation space (31000x11250) for 35 subjects/"
#fileName = "video1_eLORETA.npy"
#videodata= np.load (pathLuca + fileName)
print(videodata1.shape)
print(len(videodata)) #len of points in brain (31553)
print(videodata[0])
print(len(videodata[0]))  #timestamps  (11250=125hz x 90 seconds)
print(type(videodata))
#video2data is a list of list (first list 31553 lenghth,each of this cell contains 11250 point,31553 is represented the number of the brain and the 11250 is the point in time  )
import numpy as np
import pandas as pd
from scipy.stats import f_oneway, ttest_ind
import matplotlib.pyplot as plt


datasets = [videodata, videodata1, videodata2, videodata3]
dataset_labels = ["videodata", "videodata1", "videodata2", "videodata3"]

# Step 2: Calculate Mean, SD, Sum, and Average for each column
summary_stats = {}
for i, dataset in enumerate(datasets):
    means = np.mean(dataset, axis=0)
    sds = np.std(dataset, axis=0)
    sums = np.sum(dataset, axis=0)
    averages = np.mean(dataset, axis=1)

    summary_stats[dataset_labels[i]] = {
        "Mean": means,
        "SD": sds,
        "Sum": sums,
        "Average": averages
    }

# Step 3: Visualize the Mean and Sum for each dataset
for label, stats in summary_stats.items():
    # Mean Visualization
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Mean"])), stats["Mean"], color='skyblue', label="Mean")
    plt.xlabel('Columns')
    plt.ylabel('Mean Values')
    plt.title(f'Mean for {label}')
    plt.legend()
    plt.show()

    # Sum Visualization
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Sum"])), stats["Sum"], color='orange', label="Sum")
    plt.xlabel('Columns')
    plt.ylabel('Sum Values')
    plt.title(f'Sum for {label}')
    plt.legend()
    plt.show()

    # Average Visualization (per subject)
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Average"])), stats["Average"], color='green', label="Average per Subject")
    plt.xlabel('Subjects')
    plt.ylabel('Average Values')
    plt.title(f'Average per Subject in {label}')
    plt.legend()
    plt.show()
# Step 4: Compare Mean and Sum between subjects in each dataset
for label, stats in summary_stats.items():
    # Mean comparison
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Average"])), stats["Average"], color='green', label="Average per Subject")
    plt.xlabel('Subjects')
    plt.ylabel('Average Values')
    plt.title(f'Average in {label}')
    plt.legend()
    plt.show()

# Step 5: Statistical Comparison between Datasets
print("ANOVA Results (Mean Comparison):")
mean_anova_results = f_oneway(
    summary_stats["videodata"]["Mean"],
    summary_stats["videodata1"]["Mean"],
    summary_stats["videodata2"]["Mean"],
    summary_stats["videodata3"]["Mean"]
)
print(f"F-statistic: {mean_anova_results.statistic:.4f}, P-value: {mean_anova_results.pvalue:.4f}")

print("\nANOVA Results (Sum Comparison):")
sum_anova_results = f_oneway(
    summary_stats["videodata"]["Sum"],
    summary_stats["videodata1"]["Sum"],
    summary_stats["videodata2"]["Sum"],
    summary_stats["videodata3"]["Sum"]
)
print(f"F-statistic: {sum_anova_results.statistic:.4f}, P-value: {sum_anova_results.pvalue:.4f}")

# Pairwise Comparison for Means and Sums
print("\nPairwise T-tests (Mean Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Mean"],
            summary_stats[dataset_labels[j]]["Mean"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")

print("\nANOVA Results (Average Comparison):")
average_anova_results = f_oneway(
    summary_stats["videodata"]["Average"],
    summary_stats["videodata1"]["Average"],
    summary_stats["videodata2"]["Average"],
    summary_stats["videodata3"]["Average"]
)
print(f"F-statistic: {average_anova_results.statistic:.4f}, P-value: {average_anova_results.pvalue:.4f}")

print("\nPairwise T-tests (Sum Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Sum"],
            summary_stats[dataset_labels[j]]["Sum"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")
# Pairwise Comparison for Averages
print("\nPairwise T-tests (Average Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Average"],
            summary_stats[dataset_labels[j]]["Average"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")

# Ensure all datasets have the same number of rows
num_subjects = videodata.shape[0]
if any(dataset.shape[0] != num_subjects for dataset in [videodata1, videodata2, videodata3]):
    raise ValueError("All video datasets must have the same number of subjects (rows).")

# Calculate the sum for each subject in each video
sums_video1 = np.sum(videodata, axis=1)
sums_video2 = np.sum(videodata1, axis=1)
sums_video3 = np.sum(videodata2, axis=1)
sums_video4 = np.sum(videodata3, axis=1)

# Create a DataFrame to store the results
subjects = [f"Subject_{i+1}" for i in range(num_subjects)]  # Generate subject labels

sum_data = pd.DataFrame({
    "Subject": subjects,
    "Video1_Sum": sums_video1,
    "Video2_Sum": sums_video2,
    "Video3_Sum": sums_video3,
    "Video4_Sum": sums_video4
})

# Save the data to a CSV file
output_file = "subject_sums.csv"
sum_data.to_csv(output_file, index=False)

# Calculate correlation matrices using different methods
correlation_matrix_pearson = sum_data.drop("Subject", axis=1).corr(method="pearson")
correlation_matrix_spearman = sum_data.drop("Subject", axis=1).corr(method="spearman")
correlation_matrix_kendall = sum_data.drop("Subject", axis=1).corr(method="kendall")

# Print correlation matrices
print("Pearson Correlation Matrix:")
print(correlation_matrix_pearson)
print("\nSpearman Correlation Matrix:")
print(correlation_matrix_spearman)
print("\nKendall Correlation Matrix:")
print(correlation_matrix_kendall)

# Visualize the correlation matrices
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_pearson, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Pearson Correlation Matrix Between Videos (Based on Sums)")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_spearman, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Spearman Correlation Matrix Between Videos (Based on Sums)")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_kendall, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Kendall Correlation Matrix Between Videos (Based on Sums)")
plt.show()

# Determine the best model (pair with the highest correlation in Pearson)
best_pair_pearson = None
max_correlation_pearson = -1

for i, col1 in enumerate(correlation_matrix_pearson.columns):
    for j, col2 in enumerate(correlation_matrix_pearson.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_pearson.loc[col1, col2]
            if correlation > max_correlation_pearson:
                max_correlation_pearson = correlation
                best_pair_pearson = (col1, col2)

print(f"Best Correlated Pair (Pearson): {best_pair_pearson} with Correlation: {max_correlation_pearson:.2f}")

# Determine the best model (pair with the highest correlation in Spearman)
best_pair_spearman = None
max_correlation_spearman = -1

for i, col1 in enumerate(correlation_matrix_spearman.columns):
    for j, col2 in enumerate(correlation_matrix_spearman.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_spearman.loc[col1, col2]
            if correlation > max_correlation_spearman:
                max_correlation_spearman = correlation
                best_pair_spearman = (col1, col2)

print(f"Best Correlated Pair (Spearman): {best_pair_spearman} with Correlation: {max_correlation_spearman:.2f}")

# Determine the best model (pair with the highest correlation in Kendall)
best_pair_kendall = None
max_correlation_kendall = -1

for i, col1 in enumerate(correlation_matrix_kendall.columns):
    for j, col2 in enumerate(correlation_matrix_kendall.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_kendall.loc[col1, col2]
            if correlation > max_correlation_kendall:
                max_correlation_kendall = correlation
                best_pair_kendall = (col1, col2)

print(f"Best Correlated Pair (Kendall): {best_pair_kendall} with Correlation: {max_correlation_kendall:.2f}")

print(f"Sum for each subject saved to {output_file}")

"""NDARTR840XP1"""

videodata= np.load("/content/drive/MyDrive/Data TU PHD DUBLIN/subjects Data/NDARTR840XP1/evaluation/baseline_eLORETA.npy")  #base relax
videodata1= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/subjects Data/NDARTR840XP1/evaluation/video1_eLORETA.npy") #marowa's drive video1
videodata2= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/subjects Data/NDARTR840XP1/evaluation/video2_eLORETA.npy") #video2
videodata3= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/subjects Data/NDARTR840XP1/evaluation/video3_eLORETA.npy") #video3
#luca's drive
#pathLuca = "/content/drive/MyDrive/TU Dublin-NN/research/PHD CANDIDATES - AICL research LAB/CURRENT/Carlos Gomez Tapia - PB04482/dataset - full source activation space (31000x11250) for 35 subjects/"
#fileName = "video1_eLORETA.npy"
#videodata= np.load (pathLuca + fileName)
print(videodata1.shape)
print(len(videodata)) #len of points in brain (31553)
print(videodata[0])
print(len(videodata[0]))  #timestamps  (11250=125hz x 90 seconds)
print(type(videodata))
#video2data is a list of list (first list 31553 lenghth,each of this cell contains 11250 point,31553 is represented the number of the brain and the 11250 is the point in time  )
import numpy as np
import pandas as pd
from scipy.stats import f_oneway, ttest_ind
import matplotlib.pyplot as plt


datasets = [videodata, videodata1, videodata2, videodata3]
dataset_labels = ["videodata", "videodata1", "videodata2", "videodata3"]

# Step 2: Calculate Mean, SD, Sum, and Average for each column
summary_stats = {}
for i, dataset in enumerate(datasets):
    means = np.mean(dataset, axis=0)
    sds = np.std(dataset, axis=0)
    sums = np.sum(dataset, axis=0)
    averages = np.mean(dataset, axis=1)

    summary_stats[dataset_labels[i]] = {
        "Mean": means,
        "SD": sds,
        "Sum": sums,
        "Average": averages
    }

# Step 3: Visualize the Mean and Sum for each dataset
for label, stats in summary_stats.items():
    # Mean Visualization
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Mean"])), stats["Mean"], color='skyblue', label="Mean")
    plt.xlabel('Columns')
    plt.ylabel('Mean Values')
    plt.title(f'Mean for {label}')
    plt.legend()
    plt.show()

    # Sum Visualization
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Sum"])), stats["Sum"], color='orange', label="Sum")
    plt.xlabel('Columns')
    plt.ylabel('Sum Values')
    plt.title(f'Sum for {label}')
    plt.legend()
    plt.show()

    # Average Visualization (per subject)
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Average"])), stats["Average"], color='green', label="Average per Subject")
    plt.xlabel('Subjects')
    plt.ylabel('Average Values')
    plt.title(f'Average per Subject in {label}')
    plt.legend()
    plt.show()
# Step 4: Compare Mean and Sum between subjects in each dataset
for label, stats in summary_stats.items():
    # Mean comparison
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Average"])), stats["Average"], color='green', label="Average per Subject")
    plt.xlabel('Subjects')
    plt.ylabel('Average Values')
    plt.title(f'Average in {label}')
    plt.legend()
    plt.show()

# Step 5: Statistical Comparison between Datasets
print("ANOVA Results (Mean Comparison):")
mean_anova_results = f_oneway(
    summary_stats["videodata"]["Mean"],
    summary_stats["videodata1"]["Mean"],
    summary_stats["videodata2"]["Mean"],
    summary_stats["videodata3"]["Mean"]
)
print(f"F-statistic: {mean_anova_results.statistic:.4f}, P-value: {mean_anova_results.pvalue:.4f}")

print("\nANOVA Results (Sum Comparison):")
sum_anova_results = f_oneway(
    summary_stats["videodata"]["Sum"],
    summary_stats["videodata1"]["Sum"],
    summary_stats["videodata2"]["Sum"],
    summary_stats["videodata3"]["Sum"]
)
print(f"F-statistic: {sum_anova_results.statistic:.4f}, P-value: {sum_anova_results.pvalue:.4f}")

# Pairwise Comparison for Means and Sums
print("\nPairwise T-tests (Mean Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Mean"],
            summary_stats[dataset_labels[j]]["Mean"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")

print("\nANOVA Results (Average Comparison):")
average_anova_results = f_oneway(
    summary_stats["videodata"]["Average"],
    summary_stats["videodata1"]["Average"],
    summary_stats["videodata2"]["Average"],
    summary_stats["videodata3"]["Average"]
)
print(f"F-statistic: {average_anova_results.statistic:.4f}, P-value: {average_anova_results.pvalue:.4f}")

print("\nPairwise T-tests (Sum Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Sum"],
            summary_stats[dataset_labels[j]]["Sum"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")
# Pairwise Comparison for Averages
print("\nPairwise T-tests (Average Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Average"],
            summary_stats[dataset_labels[j]]["Average"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")

# Ensure all datasets have the same number of rows
num_subjects = videodata.shape[0]
if any(dataset.shape[0] != num_subjects for dataset in [videodata1, videodata2, videodata3]):
    raise ValueError("All video datasets must have the same number of subjects (rows).")

# Calculate the sum for each subject in each video
sums_video1 = np.sum(videodata, axis=1)
sums_video2 = np.sum(videodata1, axis=1)
sums_video3 = np.sum(videodata2, axis=1)
sums_video4 = np.sum(videodata3, axis=1)

# Create a DataFrame to store the results
subjects = [f"Subject_{i+1}" for i in range(num_subjects)]  # Generate subject labels

sum_data = pd.DataFrame({
    "Subject": subjects,
    "Video1_Sum": sums_video1,
    "Video2_Sum": sums_video2,
    "Video3_Sum": sums_video3,
    "Video4_Sum": sums_video4
})

# Save the data to a CSV file
output_file = "subject_sums.csv"
sum_data.to_csv(output_file, index=False)

# Calculate correlation matrices using different methods
correlation_matrix_pearson = sum_data.drop("Subject", axis=1).corr(method="pearson")
correlation_matrix_spearman = sum_data.drop("Subject", axis=1).corr(method="spearman")
correlation_matrix_kendall = sum_data.drop("Subject", axis=1).corr(method="kendall")

# Print correlation matrices
print("Pearson Correlation Matrix:")
print(correlation_matrix_pearson)
print("\nSpearman Correlation Matrix:")
print(correlation_matrix_spearman)
print("\nKendall Correlation Matrix:")
print(correlation_matrix_kendall)

# Visualize the correlation matrices
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_pearson, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Pearson Correlation Matrix Between Videos (Based on Sums)")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_spearman, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Spearman Correlation Matrix Between Videos (Based on Sums)")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_kendall, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Kendall Correlation Matrix Between Videos (Based on Sums)")
plt.show()

# Determine the best model (pair with the highest correlation in Pearson)
best_pair_pearson = None
max_correlation_pearson = -1

for i, col1 in enumerate(correlation_matrix_pearson.columns):
    for j, col2 in enumerate(correlation_matrix_pearson.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_pearson.loc[col1, col2]
            if correlation > max_correlation_pearson:
                max_correlation_pearson = correlation
                best_pair_pearson = (col1, col2)

print(f"Best Correlated Pair (Pearson): {best_pair_pearson} with Correlation: {max_correlation_pearson:.2f}")

# Determine the best model (pair with the highest correlation in Spearman)
best_pair_spearman = None
max_correlation_spearman = -1

for i, col1 in enumerate(correlation_matrix_spearman.columns):
    for j, col2 in enumerate(correlation_matrix_spearman.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_spearman.loc[col1, col2]
            if correlation > max_correlation_spearman:
                max_correlation_spearman = correlation
                best_pair_spearman = (col1, col2)

print(f"Best Correlated Pair (Spearman): {best_pair_spearman} with Correlation: {max_correlation_spearman:.2f}")

# Determine the best model (pair with the highest correlation in Kendall)
best_pair_kendall = None
max_correlation_kendall = -1

for i, col1 in enumerate(correlation_matrix_kendall.columns):
    for j, col2 in enumerate(correlation_matrix_kendall.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_kendall.loc[col1, col2]
            if correlation > max_correlation_kendall:
                max_correlation_kendall = correlation
                best_pair_kendall = (col1, col2)

print(f"Best Correlated Pair (Kendall): {best_pair_kendall} with Correlation: {max_correlation_kendall:.2f}")

print(f"Sum for each subject saved to {output_file}")

"""NDARTN760YH8"""

videodata= np.load("/content/drive/MyDrive/Data TU PHD DUBLIN/subjects Data/NDARTN760YH8/evaluation/baseline_eLORETA.npy")  #base relax
videodata1= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/subjects Data/NDARTN760YH8/evaluation/video1_eLORETA.npy") #marowa's drive video1
videodata2= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/subjects Data/NDARTN760YH8/evaluation/video2_eLORETA.npy") #video2
videodata3= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/subjects Data/NDARTN760YH8/evaluation/video3_eLORETA.npy") #video3
#luca's drive
#pathLuca = "/content/drive/MyDrive/TU Dublin-NN/research/PHD CANDIDATES - AICL research LAB/CURRENT/Carlos Gomez Tapia - PB04482/dataset - full source activation space (31000x11250) for 35 subjects/"
#fileName = "video1_eLORETA.npy"
#videodata= np.load (pathLuca + fileName)
print(videodata1.shape)
print(len(videodata)) #len of points in brain (31553)
print(videodata[0])
print(len(videodata[0]))  #timestamps  (11250=125hz x 90 seconds)
print(type(videodata))
#video2data is a list of list (first list 31553 lenghth,each of this cell contains 11250 point,31553 is represented the number of the brain and the 11250 is the point in time  )
import numpy as np
import pandas as pd
from scipy.stats import f_oneway, ttest_ind
import matplotlib.pyplot as plt


datasets = [videodata, videodata1, videodata2, videodata3]
dataset_labels = ["videodata", "videodata1", "videodata2", "videodata3"]

# Step 2: Calculate Mean, SD, Sum, and Average for each column
summary_stats = {}
for i, dataset in enumerate(datasets):
    means = np.mean(dataset, axis=0)
    sds = np.std(dataset, axis=0)
    sums = np.sum(dataset, axis=0)
    averages = np.mean(dataset, axis=1)

    summary_stats[dataset_labels[i]] = {
        "Mean": means,
        "SD": sds,
        "Sum": sums,
        "Average": averages
    }

# Step 3: Visualize the Mean and Sum for each dataset
for label, stats in summary_stats.items():
    # Mean Visualization
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Mean"])), stats["Mean"], color='skyblue', label="Mean")
    plt.xlabel('Columns')
    plt.ylabel('Mean Values')
    plt.title(f'Mean for {label}')
    plt.legend()
    plt.show()

    # Sum Visualization
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Sum"])), stats["Sum"], color='orange', label="Sum")
    plt.xlabel('Columns')
    plt.ylabel('Sum Values')
    plt.title(f'Sum for {label}')
    plt.legend()
    plt.show()

    # Average Visualization (per subject)
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Average"])), stats["Average"], color='green', label="Average per Subject")
    plt.xlabel('Subjects')
    plt.ylabel('Average Values')
    plt.title(f'Average per Subject in {label}')
    plt.legend()
    plt.show()
# Step 4: Compare Mean and Sum between subjects in each dataset
for label, stats in summary_stats.items():
    # Mean comparison
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Average"])), stats["Average"], color='green', label="Average per Subject")
    plt.xlabel('Subjects')
    plt.ylabel('Average Values')
    plt.title(f'Average in {label}')
    plt.legend()
    plt.show()

# Step 5: Statistical Comparison between Datasets
print("ANOVA Results (Mean Comparison):")
mean_anova_results = f_oneway(
    summary_stats["videodata"]["Mean"],
    summary_stats["videodata1"]["Mean"],
    summary_stats["videodata2"]["Mean"],
    summary_stats["videodata3"]["Mean"]
)
print(f"F-statistic: {mean_anova_results.statistic:.4f}, P-value: {mean_anova_results.pvalue:.4f}")

print("\nANOVA Results (Sum Comparison):")
sum_anova_results = f_oneway(
    summary_stats["videodata"]["Sum"],
    summary_stats["videodata1"]["Sum"],
    summary_stats["videodata2"]["Sum"],
    summary_stats["videodata3"]["Sum"]
)
print(f"F-statistic: {sum_anova_results.statistic:.4f}, P-value: {sum_anova_results.pvalue:.4f}")

# Pairwise Comparison for Means and Sums
print("\nPairwise T-tests (Mean Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Mean"],
            summary_stats[dataset_labels[j]]["Mean"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")

print("\nANOVA Results (Average Comparison):")
average_anova_results = f_oneway(
    summary_stats["videodata"]["Average"],
    summary_stats["videodata1"]["Average"],
    summary_stats["videodata2"]["Average"],
    summary_stats["videodata3"]["Average"]
)
print(f"F-statistic: {average_anova_results.statistic:.4f}, P-value: {average_anova_results.pvalue:.4f}")

print("\nPairwise T-tests (Sum Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Sum"],
            summary_stats[dataset_labels[j]]["Sum"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")
# Pairwise Comparison for Averages
print("\nPairwise T-tests (Average Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Average"],
            summary_stats[dataset_labels[j]]["Average"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")

# Ensure all datasets have the same number of rows
num_subjects = videodata.shape[0]
if any(dataset.shape[0] != num_subjects for dataset in [videodata1, videodata2, videodata3]):
    raise ValueError("All video datasets must have the same number of subjects (rows).")

# Calculate the sum for each subject in each video
sums_video1 = np.sum(videodata, axis=1)
sums_video2 = np.sum(videodata1, axis=1)
sums_video3 = np.sum(videodata2, axis=1)
sums_video4 = np.sum(videodata3, axis=1)

# Create a DataFrame to store the results
subjects = [f"Subject_{i+1}" for i in range(num_subjects)]  # Generate subject labels

sum_data = pd.DataFrame({
    "Subject": subjects,
    "Video1_Sum": sums_video1,
    "Video2_Sum": sums_video2,
    "Video3_Sum": sums_video3,
    "Video4_Sum": sums_video4
})

# Save the data to a CSV file
output_file = "subject_sums.csv"
sum_data.to_csv(output_file, index=False)

# Calculate correlation matrices using different methods
correlation_matrix_pearson = sum_data.drop("Subject", axis=1).corr(method="pearson")
correlation_matrix_spearman = sum_data.drop("Subject", axis=1).corr(method="spearman")
correlation_matrix_kendall = sum_data.drop("Subject", axis=1).corr(method="kendall")

# Print correlation matrices
print("Pearson Correlation Matrix:")
print(correlation_matrix_pearson)
print("\nSpearman Correlation Matrix:")
print(correlation_matrix_spearman)
print("\nKendall Correlation Matrix:")
print(correlation_matrix_kendall)

# Visualize the correlation matrices
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_pearson, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Pearson Correlation Matrix Between Videos (Based on Sums)")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_spearman, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Spearman Correlation Matrix Between Videos (Based on Sums)")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_kendall, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Kendall Correlation Matrix Between Videos (Based on Sums)")
plt.show()

# Determine the best model (pair with the highest correlation in Pearson)
best_pair_pearson = None
max_correlation_pearson = -1

for i, col1 in enumerate(correlation_matrix_pearson.columns):
    for j, col2 in enumerate(correlation_matrix_pearson.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_pearson.loc[col1, col2]
            if correlation > max_correlation_pearson:
                max_correlation_pearson = correlation
                best_pair_pearson = (col1, col2)

print(f"Best Correlated Pair (Pearson): {best_pair_pearson} with Correlation: {max_correlation_pearson:.2f}")

# Determine the best model (pair with the highest correlation in Spearman)
best_pair_spearman = None
max_correlation_spearman = -1

for i, col1 in enumerate(correlation_matrix_spearman.columns):
    for j, col2 in enumerate(correlation_matrix_spearman.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_spearman.loc[col1, col2]
            if correlation > max_correlation_spearman:
                max_correlation_spearman = correlation
                best_pair_spearman = (col1, col2)

print(f"Best Correlated Pair (Spearman): {best_pair_spearman} with Correlation: {max_correlation_spearman:.2f}")

# Determine the best model (pair with the highest correlation in Kendall)
best_pair_kendall = None
max_correlation_kendall = -1

for i, col1 in enumerate(correlation_matrix_kendall.columns):
    for j, col2 in enumerate(correlation_matrix_kendall.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_kendall.loc[col1, col2]
            if correlation > max_correlation_kendall:
                max_correlation_kendall = correlation
                best_pair_kendall = (col1, col2)

print(f"Best Correlated Pair (Kendall): {best_pair_kendall} with Correlation: {max_correlation_kendall:.2f}")

print(f"Sum for each subject saved to {output_file}")

"""NDARLB017MBJ"""

videodata= np.load("/content/drive/MyDrive/Data TU PHD DUBLIN/subjects Data/NDARLB017MBJ/evaluation/baseline_eLORETA.npy")  #base relax
videodata1= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/subjects Data/NDARLB017MBJ/evaluation/video1_eLORETA.npy") #marowa's drive video1
videodata2= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/subjects Data/NDARLB017MBJ/evaluation/video2_eLORETA.npy") #video2
videodata3= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/subjects Data/NDARLB017MBJ/evaluation/video3_eLORETA.npy") #video3
#luca's drive
#pathLuca = "/content/drive/MyDrive/TU Dublin-NN/research/PHD CANDIDATES - AICL research LAB/CURRENT/Carlos Gomez Tapia - PB04482/dataset - full source activation space (31000x11250) for 35 subjects/"
#fileName = "video1_eLORETA.npy"
#videodata= np.load (pathLuca + fileName)
print(videodata1.shape)
print(len(videodata)) #len of points in brain (31553)
print(videodata[0])
print(len(videodata[0]))  #timestamps  (11250=125hz x 90 seconds)
print(type(videodata))
#video2data is a list of list (first list 31553 lenghth,each of this cell contains 11250 point,31553 is represented the number of the brain and the 11250 is the point in time  )
import numpy as np
import pandas as pd
from scipy.stats import f_oneway, ttest_ind
import matplotlib.pyplot as plt


datasets = [videodata, videodata1, videodata2, videodata3]
dataset_labels = ["videodata", "videodata1", "videodata2", "videodata3"]

# Step 2: Calculate Mean, SD, Sum, and Average for each column
summary_stats = {}
for i, dataset in enumerate(datasets):
    means = np.mean(dataset, axis=0)
    sds = np.std(dataset, axis=0)
    sums = np.sum(dataset, axis=0)
    averages = np.mean(dataset, axis=1)

    summary_stats[dataset_labels[i]] = {
        "Mean": means,
        "SD": sds,
        "Sum": sums,
        "Average": averages
    }

# Step 3: Visualize the Mean and Sum for each dataset
for label, stats in summary_stats.items():
    # Mean Visualization
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Mean"])), stats["Mean"], color='skyblue', label="Mean")
    plt.xlabel('Columns')
    plt.ylabel('Mean Values')
    plt.title(f'Mean for {label}')
    plt.legend()
    plt.show()

    # Sum Visualization
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Sum"])), stats["Sum"], color='orange', label="Sum")
    plt.xlabel('Columns')
    plt.ylabel('Sum Values')
    plt.title(f'Sum for {label}')
    plt.legend()
    plt.show()

    # Average Visualization (per subject)
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Average"])), stats["Average"], color='green', label="Average per Subject")
    plt.xlabel('Subjects')
    plt.ylabel('Average Values')
    plt.title(f'Average per Subject in {label}')
    plt.legend()
    plt.show()
# Step 4: Compare Mean and Sum between subjects in each dataset
for label, stats in summary_stats.items():
    # Mean comparison
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Average"])), stats["Average"], color='green', label="Average per Subject")
    plt.xlabel('Subjects')
    plt.ylabel('Average Values')
    plt.title(f'Average in {label}')
    plt.legend()
    plt.show()

# Step 5: Statistical Comparison between Datasets
print("ANOVA Results (Mean Comparison):")
mean_anova_results = f_oneway(
    summary_stats["videodata"]["Mean"],
    summary_stats["videodata1"]["Mean"],
    summary_stats["videodata2"]["Mean"],
    summary_stats["videodata3"]["Mean"]
)
print(f"F-statistic: {mean_anova_results.statistic:.4f}, P-value: {mean_anova_results.pvalue:.4f}")

print("\nANOVA Results (Sum Comparison):")
sum_anova_results = f_oneway(
    summary_stats["videodata"]["Sum"],
    summary_stats["videodata1"]["Sum"],
    summary_stats["videodata2"]["Sum"],
    summary_stats["videodata3"]["Sum"]
)
print(f"F-statistic: {sum_anova_results.statistic:.4f}, P-value: {sum_anova_results.pvalue:.4f}")

# Pairwise Comparison for Means and Sums
print("\nPairwise T-tests (Mean Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Mean"],
            summary_stats[dataset_labels[j]]["Mean"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")

print("\nANOVA Results (Average Comparison):")
average_anova_results = f_oneway(
    summary_stats["videodata"]["Average"],
    summary_stats["videodata1"]["Average"],
    summary_stats["videodata2"]["Average"],
    summary_stats["videodata3"]["Average"]
)
print(f"F-statistic: {average_anova_results.statistic:.4f}, P-value: {average_anova_results.pvalue:.4f}")

print("\nPairwise T-tests (Sum Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Sum"],
            summary_stats[dataset_labels[j]]["Sum"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")
# Pairwise Comparison for Averages
print("\nPairwise T-tests (Average Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Average"],
            summary_stats[dataset_labels[j]]["Average"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")

# Ensure all datasets have the same number of rows
num_subjects = videodata.shape[0]
if any(dataset.shape[0] != num_subjects for dataset in [videodata1, videodata2, videodata3]):
    raise ValueError("All video datasets must have the same number of subjects (rows).")

# Calculate the sum for each subject in each video
sums_video1 = np.sum(videodata, axis=1)
sums_video2 = np.sum(videodata1, axis=1)
sums_video3 = np.sum(videodata2, axis=1)
sums_video4 = np.sum(videodata3, axis=1)

# Create a DataFrame to store the results
subjects = [f"Subject_{i+1}" for i in range(num_subjects)]  # Generate subject labels

sum_data = pd.DataFrame({
    "Subject": subjects,
    "Video1_Sum": sums_video1,
    "Video2_Sum": sums_video2,
    "Video3_Sum": sums_video3,
    "Video4_Sum": sums_video4
})

# Save the data to a CSV file
output_file = "subject_sums.csv"
sum_data.to_csv(output_file, index=False)

# Calculate correlation matrices using different methods
correlation_matrix_pearson = sum_data.drop("Subject", axis=1).corr(method="pearson")
correlation_matrix_spearman = sum_data.drop("Subject", axis=1).corr(method="spearman")
correlation_matrix_kendall = sum_data.drop("Subject", axis=1).corr(method="kendall")

# Print correlation matrices
print("Pearson Correlation Matrix:")
print(correlation_matrix_pearson)
print("\nSpearman Correlation Matrix:")
print(correlation_matrix_spearman)
print("\nKendall Correlation Matrix:")
print(correlation_matrix_kendall)

# Visualize the correlation matrices
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_pearson, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Pearson Correlation Matrix Between Videos (Based on Sums)")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_spearman, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Spearman Correlation Matrix Between Videos (Based on Sums)")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_kendall, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Kendall Correlation Matrix Between Videos (Based on Sums)")
plt.show()

# Determine the best model (pair with the highest correlation in Pearson)
best_pair_pearson = None
max_correlation_pearson = -1

for i, col1 in enumerate(correlation_matrix_pearson.columns):
    for j, col2 in enumerate(correlation_matrix_pearson.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_pearson.loc[col1, col2]
            if correlation > max_correlation_pearson:
                max_correlation_pearson = correlation
                best_pair_pearson = (col1, col2)

print(f"Best Correlated Pair (Pearson): {best_pair_pearson} with Correlation: {max_correlation_pearson:.2f}")

# Determine the best model (pair with the highest correlation in Spearman)
best_pair_spearman = None
max_correlation_spearman = -1

for i, col1 in enumerate(correlation_matrix_spearman.columns):
    for j, col2 in enumerate(correlation_matrix_spearman.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_spearman.loc[col1, col2]
            if correlation > max_correlation_spearman:
                max_correlation_spearman = correlation
                best_pair_spearman = (col1, col2)

print(f"Best Correlated Pair (Spearman): {best_pair_spearman} with Correlation: {max_correlation_spearman:.2f}")

# Determine the best model (pair with the highest correlation in Kendall)
best_pair_kendall = None
max_correlation_kendall = -1

for i, col1 in enumerate(correlation_matrix_kendall.columns):
    for j, col2 in enumerate(correlation_matrix_kendall.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_kendall.loc[col1, col2]
            if correlation > max_correlation_kendall:
                max_correlation_kendall = correlation
                best_pair_kendall = (col1, col2)

print(f"Best Correlated Pair (Kendall): {best_pair_kendall} with Correlation: {max_correlation_kendall:.2f}")

print(f"Sum for each subject saved to {output_file}")

"""NDARKZ031NJZ"""

videodata= np.load("/content/drive/MyDrive/Data TU PHD DUBLIN/subjects Data/NDARKZ031NJZ/evaluation/baseline_eLORETA.npy")  #base relax
videodata1= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/subjects Data/NDARKZ031NJZ/evaluation/video1_eLORETA.npy") #marowa's drive video1
videodata2= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/subjects Data/NDARKZ031NJZ/evaluation/video2_eLORETA.npy") #video2
videodata3= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/subjects Data/NDARKZ031NJZ/evaluation/video3_eLORETA.npy") #video3
#luca's drive
#pathLuca = "/content/drive/MyDrive/TU Dublin-NN/research/PHD CANDIDATES - AICL research LAB/CURRENT/Carlos Gomez Tapia - PB04482/dataset - full source activation space (31000x11250) for 35 subjects/"
#fileName = "video1_eLORETA.npy"
#videodata= np.load (pathLuca + fileName)
print(videodata1.shape)
print(len(videodata)) #len of points in brain (31553)
print(videodata[0])
print(len(videodata[0]))  #timestamps  (11250=125hz x 90 seconds)
print(type(videodata))
#video2data is a list of list (first list 31553 lenghth,each of this cell contains 11250 point,31553 is represented the number of the brain and the 11250 is the point in time  )
import numpy as np
import pandas as pd
from scipy.stats import f_oneway, ttest_ind
import matplotlib.pyplot as plt


datasets = [videodata, videodata1, videodata2, videodata3]
dataset_labels = ["videodata", "videodata1", "videodata2", "videodata3"]

# Step 2: Calculate Mean, SD, Sum, and Average for each column
summary_stats = {}
for i, dataset in enumerate(datasets):
    means = np.mean(dataset, axis=0)
    sds = np.std(dataset, axis=0)
    sums = np.sum(dataset, axis=0)
    averages = np.mean(dataset, axis=1)

    summary_stats[dataset_labels[i]] = {
        "Mean": means,
        "SD": sds,
        "Sum": sums,
        "Average": averages
    }

# Step 3: Visualize the Mean and Sum for each dataset
for label, stats in summary_stats.items():
    # Mean Visualization
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Mean"])), stats["Mean"], color='skyblue', label="Mean")
    plt.xlabel('Columns')
    plt.ylabel('Mean Values')
    plt.title(f'Mean for {label}')
    plt.legend()
    plt.show()

    # Sum Visualization
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Sum"])), stats["Sum"], color='orange', label="Sum")
    plt.xlabel('Columns')
    plt.ylabel('Sum Values')
    plt.title(f'Sum for {label}')
    plt.legend()
    plt.show()

    # Average Visualization (per subject)
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Average"])), stats["Average"], color='green', label="Average per Subject")
    plt.xlabel('Subjects')
    plt.ylabel('Average Values')
    plt.title(f'Average per Subject in {label}')
    plt.legend()
    plt.show()
# Step 4: Compare Mean and Sum between subjects in each dataset
for label, stats in summary_stats.items():
    # Mean comparison
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Average"])), stats["Average"], color='green', label="Average per Subject")
    plt.xlabel('Subjects')
    plt.ylabel('Average Values')
    plt.title(f'Average in {label}')
    plt.legend()
    plt.show()

# Step 5: Statistical Comparison between Datasets
print("ANOVA Results (Mean Comparison):")
mean_anova_results = f_oneway(
    summary_stats["videodata"]["Mean"],
    summary_stats["videodata1"]["Mean"],
    summary_stats["videodata2"]["Mean"],
    summary_stats["videodata3"]["Mean"]
)
print(f"F-statistic: {mean_anova_results.statistic:.4f}, P-value: {mean_anova_results.pvalue:.4f}")

print("\nANOVA Results (Sum Comparison):")
sum_anova_results = f_oneway(
    summary_stats["videodata"]["Sum"],
    summary_stats["videodata1"]["Sum"],
    summary_stats["videodata2"]["Sum"],
    summary_stats["videodata3"]["Sum"]
)
print(f"F-statistic: {sum_anova_results.statistic:.4f}, P-value: {sum_anova_results.pvalue:.4f}")

# Pairwise Comparison for Means and Sums
print("\nPairwise T-tests (Mean Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Mean"],
            summary_stats[dataset_labels[j]]["Mean"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")

print("\nANOVA Results (Average Comparison):")
average_anova_results = f_oneway(
    summary_stats["videodata"]["Average"],
    summary_stats["videodata1"]["Average"],
    summary_stats["videodata2"]["Average"],
    summary_stats["videodata3"]["Average"]
)
print(f"F-statistic: {average_anova_results.statistic:.4f}, P-value: {average_anova_results.pvalue:.4f}")

print("\nPairwise T-tests (Sum Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Sum"],
            summary_stats[dataset_labels[j]]["Sum"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")
# Pairwise Comparison for Averages
print("\nPairwise T-tests (Average Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Average"],
            summary_stats[dataset_labels[j]]["Average"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")

# Ensure all datasets have the same number of rows
num_subjects = videodata.shape[0]
if any(dataset.shape[0] != num_subjects for dataset in [videodata1, videodata2, videodata3]):
    raise ValueError("All video datasets must have the same number of subjects (rows).")

# Calculate the sum for each subject in each video
sums_video1 = np.sum(videodata, axis=1)
sums_video2 = np.sum(videodata1, axis=1)
sums_video3 = np.sum(videodata2, axis=1)
sums_video4 = np.sum(videodata3, axis=1)

# Create a DataFrame to store the results
subjects = [f"Subject_{i+1}" for i in range(num_subjects)]  # Generate subject labels

sum_data = pd.DataFrame({
    "Subject": subjects,
    "Video1_Sum": sums_video1,
    "Video2_Sum": sums_video2,
    "Video3_Sum": sums_video3,
    "Video4_Sum": sums_video4
})

# Save the data to a CSV file
output_file = "subject_sums.csv"
sum_data.to_csv(output_file, index=False)

# Calculate correlation matrices using different methods
correlation_matrix_pearson = sum_data.drop("Subject", axis=1).corr(method="pearson")
correlation_matrix_spearman = sum_data.drop("Subject", axis=1).corr(method="spearman")
correlation_matrix_kendall = sum_data.drop("Subject", axis=1).corr(method="kendall")

# Print correlation matrices
print("Pearson Correlation Matrix:")
print(correlation_matrix_pearson)
print("\nSpearman Correlation Matrix:")
print(correlation_matrix_spearman)
print("\nKendall Correlation Matrix:")
print(correlation_matrix_kendall)

# Visualize the correlation matrices
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_pearson, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Pearson Correlation Matrix Between Videos (Based on Sums)")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_spearman, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Spearman Correlation Matrix Between Videos (Based on Sums)")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_kendall, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Kendall Correlation Matrix Between Videos (Based on Sums)")
plt.show()

# Determine the best model (pair with the highest correlation in Pearson)
best_pair_pearson = None
max_correlation_pearson = -1

for i, col1 in enumerate(correlation_matrix_pearson.columns):
    for j, col2 in enumerate(correlation_matrix_pearson.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_pearson.loc[col1, col2]
            if correlation > max_correlation_pearson:
                max_correlation_pearson = correlation
                best_pair_pearson = (col1, col2)

print(f"Best Correlated Pair (Pearson): {best_pair_pearson} with Correlation: {max_correlation_pearson:.2f}")

# Determine the best model (pair with the highest correlation in Spearman)
best_pair_spearman = None
max_correlation_spearman = -1

for i, col1 in enumerate(correlation_matrix_spearman.columns):
    for j, col2 in enumerate(correlation_matrix_spearman.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_spearman.loc[col1, col2]
            if correlation > max_correlation_spearman:
                max_correlation_spearman = correlation
                best_pair_spearman = (col1, col2)

print(f"Best Correlated Pair (Spearman): {best_pair_spearman} with Correlation: {max_correlation_spearman:.2f}")

# Determine the best model (pair with the highest correlation in Kendall)
best_pair_kendall = None
max_correlation_kendall = -1

for i, col1 in enumerate(correlation_matrix_kendall.columns):
    for j, col2 in enumerate(correlation_matrix_kendall.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_kendall.loc[col1, col2]
            if correlation > max_correlation_kendall:
                max_correlation_kendall = correlation
                best_pair_kendall = (col1, col2)

print(f"Best Correlated Pair (Kendall): {best_pair_kendall} with Correlation: {max_correlation_kendall:.2f}")

print(f"Sum for each subject saved to {output_file}")

"""NDARKG697CEW"""

videodata= np.load("/content/drive/MyDrive/Data TU PHD DUBLIN/subjects Data/NDARKG697CEW/evaluation/baseline_eLORETA.npy")  #base relax
videodata1= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/subjects Data/NDARKG697CEW/evaluation/video1_eLORETA.npy") #marowa's drive video1
videodata2= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/subjects Data/NDARKG697CEW/evaluation/video2_eLORETA.npy") #video2
videodata3= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/subjects Data/NDARKG697CEW/evaluation/video3_eLORETA.npy") #video3
#luca's drive
#pathLuca = "/content/drive/MyDrive/TU Dublin-NN/research/PHD CANDIDATES - AICL research LAB/CURRENT/Carlos Gomez Tapia - PB04482/dataset - full source activation space (31000x11250) for 35 subjects/"
#fileName = "video1_eLORETA.npy"
#videodata= np.load (pathLuca + fileName)
print(videodata1.shape)
print(len(videodata)) #len of points in brain (31553)
print(videodata[0])
print(len(videodata[0]))  #timestamps  (11250=125hz x 90 seconds)
print(type(videodata))
#video2data is a list of list (first list 31553 lenghth,each of this cell contains 11250 point,31553 is represented the number of the brain and the 11250 is the point in time  )
import numpy as np
import pandas as pd
from scipy.stats import f_oneway, ttest_ind
import matplotlib.pyplot as plt


datasets = [videodata, videodata1, videodata2, videodata3]
dataset_labels = ["videodata", "videodata1", "videodata2", "videodata3"]

# Step 2: Calculate Mean, SD, Sum, and Average for each column
summary_stats = {}
for i, dataset in enumerate(datasets):
    means = np.mean(dataset, axis=0)
    sds = np.std(dataset, axis=0)
    sums = np.sum(dataset, axis=0)
    averages = np.mean(dataset, axis=1)

    summary_stats[dataset_labels[i]] = {
        "Mean": means,
        "SD": sds,
        "Sum": sums,
        "Average": averages
    }

# Step 3: Visualize the Mean and Sum for each dataset
for label, stats in summary_stats.items():
    # Mean Visualization
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Mean"])), stats["Mean"], color='skyblue', label="Mean")
    plt.xlabel('Columns')
    plt.ylabel('Mean Values')
    plt.title(f'Mean for {label}')
    plt.legend()
    plt.show()

    # Sum Visualization
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Sum"])), stats["Sum"], color='orange', label="Sum")
    plt.xlabel('Columns')
    plt.ylabel('Sum Values')
    plt.title(f'Sum for {label}')
    plt.legend()
    plt.show()

    # Average Visualization (per subject)
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Average"])), stats["Average"], color='green', label="Average per Subject")
    plt.xlabel('Subjects')
    plt.ylabel('Average Values')
    plt.title(f'Average per Subject in {label}')
    plt.legend()
    plt.show()
# Step 4: Compare Mean and Sum between subjects in each dataset
for label, stats in summary_stats.items():
    # Mean comparison
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Average"])), stats["Average"], color='green', label="Average per Subject")
    plt.xlabel('Subjects')
    plt.ylabel('Average Values')
    plt.title(f'Average in {label}')
    plt.legend()
    plt.show()

# Step 5: Statistical Comparison between Datasets
print("ANOVA Results (Mean Comparison):")
mean_anova_results = f_oneway(
    summary_stats["videodata"]["Mean"],
    summary_stats["videodata1"]["Mean"],
    summary_stats["videodata2"]["Mean"],
    summary_stats["videodata3"]["Mean"]
)
print(f"F-statistic: {mean_anova_results.statistic:.4f}, P-value: {mean_anova_results.pvalue:.4f}")

print("\nANOVA Results (Sum Comparison):")
sum_anova_results = f_oneway(
    summary_stats["videodata"]["Sum"],
    summary_stats["videodata1"]["Sum"],
    summary_stats["videodata2"]["Sum"],
    summary_stats["videodata3"]["Sum"]
)
print(f"F-statistic: {sum_anova_results.statistic:.4f}, P-value: {sum_anova_results.pvalue:.4f}")

# Pairwise Comparison for Means and Sums
print("\nPairwise T-tests (Mean Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Mean"],
            summary_stats[dataset_labels[j]]["Mean"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")

print("\nANOVA Results (Average Comparison):")
average_anova_results = f_oneway(
    summary_stats["videodata"]["Average"],
    summary_stats["videodata1"]["Average"],
    summary_stats["videodata2"]["Average"],
    summary_stats["videodata3"]["Average"]
)
print(f"F-statistic: {average_anova_results.statistic:.4f}, P-value: {average_anova_results.pvalue:.4f}")

print("\nPairwise T-tests (Sum Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Sum"],
            summary_stats[dataset_labels[j]]["Sum"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")
# Pairwise Comparison for Averages
print("\nPairwise T-tests (Average Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Average"],
            summary_stats[dataset_labels[j]]["Average"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")

# Ensure all datasets have the same number of rows
num_subjects = videodata.shape[0]
if any(dataset.shape[0] != num_subjects for dataset in [videodata1, videodata2, videodata3]):
    raise ValueError("All video datasets must have the same number of subjects (rows).")

# Calculate the sum for each subject in each video
sums_video1 = np.sum(videodata, axis=1)
sums_video2 = np.sum(videodata1, axis=1)
sums_video3 = np.sum(videodata2, axis=1)
sums_video4 = np.sum(videodata3, axis=1)

# Create a DataFrame to store the results
subjects = [f"Subject_{i+1}" for i in range(num_subjects)]  # Generate subject labels

sum_data = pd.DataFrame({
    "Subject": subjects,
    "Video1_Sum": sums_video1,
    "Video2_Sum": sums_video2,
    "Video3_Sum": sums_video3,
    "Video4_Sum": sums_video4
})

# Save the data to a CSV file
output_file = "subject_sums.csv"
sum_data.to_csv(output_file, index=False)

# Calculate correlation matrices using different methods
correlation_matrix_pearson = sum_data.drop("Subject", axis=1).corr(method="pearson")
correlation_matrix_spearman = sum_data.drop("Subject", axis=1).corr(method="spearman")
correlation_matrix_kendall = sum_data.drop("Subject", axis=1).corr(method="kendall")

# Print correlation matrices
print("Pearson Correlation Matrix:")
print(correlation_matrix_pearson)
print("\nSpearman Correlation Matrix:")
print(correlation_matrix_spearman)
print("\nKendall Correlation Matrix:")
print(correlation_matrix_kendall)

# Visualize the correlation matrices
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_pearson, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Pearson Correlation Matrix Between Videos (Based on Sums)")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_spearman, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Spearman Correlation Matrix Between Videos (Based on Sums)")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_kendall, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Kendall Correlation Matrix Between Videos (Based on Sums)")
plt.show()

# Determine the best model (pair with the highest correlation in Pearson)
best_pair_pearson = None
max_correlation_pearson = -1

for i, col1 in enumerate(correlation_matrix_pearson.columns):
    for j, col2 in enumerate(correlation_matrix_pearson.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_pearson.loc[col1, col2]
            if correlation > max_correlation_pearson:
                max_correlation_pearson = correlation
                best_pair_pearson = (col1, col2)

print(f"Best Correlated Pair (Pearson): {best_pair_pearson} with Correlation: {max_correlation_pearson:.2f}")

# Determine the best model (pair with the highest correlation in Spearman)
best_pair_spearman = None
max_correlation_spearman = -1

for i, col1 in enumerate(correlation_matrix_spearman.columns):
    for j, col2 in enumerate(correlation_matrix_spearman.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_spearman.loc[col1, col2]
            if correlation > max_correlation_spearman:
                max_correlation_spearman = correlation
                best_pair_spearman = (col1, col2)

print(f"Best Correlated Pair (Spearman): {best_pair_spearman} with Correlation: {max_correlation_spearman:.2f}")

# Determine the best model (pair with the highest correlation in Kendall)
best_pair_kendall = None
max_correlation_kendall = -1

for i, col1 in enumerate(correlation_matrix_kendall.columns):
    for j, col2 in enumerate(correlation_matrix_kendall.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_kendall.loc[col1, col2]
            if correlation > max_correlation_kendall:
                max_correlation_kendall = correlation
                best_pair_kendall = (col1, col2)

print(f"Best Correlated Pair (Kendall): {best_pair_kendall} with Correlation: {max_correlation_kendall:.2f}")

print(f"Sum for each subject saved to {output_file}")

"""NDARDJ970ELG"""

videodata= np.load("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARDJ970ELG/evaluation/baseline_eLORETA.npy")  #base relax
videodata1= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARDJ970ELG/evaluation/video1_eLORETA.npy") #marowa's drive video1
videodata2= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARDJ970ELG/evaluation/video2_eLORETA.npy") #video2
videodata3= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARDJ970ELG/evaluation/video3_eLORETA.npy") #video3
#luca's drive
#pathLuca = "/content/drive/MyDrive/TU Dublin-NN/research/PHD CANDIDATES - AICL research LAB/CURRENT/Carlos Gomez Tapia - PB04482/dataset - full source activation space (31000x11250) for 35 subjects/"
#fileName = "video1_eLORETA.npy"
#videodata= np.load (pathLuca + fileName)
print(videodata1.shape)
print(len(videodata)) #len of points in brain (31553)
print(videodata[0])
print(len(videodata[0]))  #timestamps  (11250=125hz x 90 seconds)
print(type(videodata))
#video2data is a list of list (first list 31553 lenghth,each of this cell contains 11250 point,31553 is represented the number of the brain and the 11250 is the point in time  )
import numpy as np
import pandas as pd
from scipy.stats import f_oneway, ttest_ind
import matplotlib.pyplot as plt


datasets = [videodata, videodata1, videodata2, videodata3]
dataset_labels = ["videodata", "videodata1", "videodata2", "videodata3"]

# Step 2: Calculate Mean, SD, Sum, and Average for each column
summary_stats = {}
for i, dataset in enumerate(datasets):
    means = np.mean(dataset, axis=0)
    sds = np.std(dataset, axis=0)
    sums = np.sum(dataset, axis=0)
    averages = np.mean(dataset, axis=1)

    summary_stats[dataset_labels[i]] = {
        "Mean": means,
        "SD": sds,
        "Sum": sums,
        "Average": averages
    }

# Step 3: Visualize the Mean and Sum for each dataset
for label, stats in summary_stats.items():
    # Mean Visualization
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Mean"])), stats["Mean"], color='skyblue', label="Mean")
    plt.xlabel('Columns')
    plt.ylabel('Mean Values')
    plt.title(f'Mean for {label}')
    plt.legend()
    plt.show()

    # Sum Visualization
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Sum"])), stats["Sum"], color='orange', label="Sum")
    plt.xlabel('Columns')
    plt.ylabel('Sum Values')
    plt.title(f'Sum for {label}')
    plt.legend()
    plt.show()

    # Average Visualization (per subject)
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Average"])), stats["Average"], color='green', label="Average per Subject")
    plt.xlabel('Subjects')
    plt.ylabel('Average Values')
    plt.title(f'Average per Subject in {label}')
    plt.legend()
    plt.show()
# Step 4: Compare Mean and Sum between subjects in each dataset
for label, stats in summary_stats.items():
    # Mean comparison
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Average"])), stats["Average"], color='green', label="Average per Subject")
    plt.xlabel('Subjects')
    plt.ylabel('Average Values')
    plt.title(f'Average in {label}')
    plt.legend()
    plt.show()

# Step 5: Statistical Comparison between Datasets
print("ANOVA Results (Mean Comparison):")
mean_anova_results = f_oneway(
    summary_stats["videodata"]["Mean"],
    summary_stats["videodata1"]["Mean"],
    summary_stats["videodata2"]["Mean"],
    summary_stats["videodata3"]["Mean"]
)
print(f"F-statistic: {mean_anova_results.statistic:.4f}, P-value: {mean_anova_results.pvalue:.4f}")

print("\nANOVA Results (Sum Comparison):")
sum_anova_results = f_oneway(
    summary_stats["videodata"]["Sum"],
    summary_stats["videodata1"]["Sum"],
    summary_stats["videodata2"]["Sum"],
    summary_stats["videodata3"]["Sum"]
)
print(f"F-statistic: {sum_anova_results.statistic:.4f}, P-value: {sum_anova_results.pvalue:.4f}")

# Pairwise Comparison for Means and Sums
print("\nPairwise T-tests (Mean Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Mean"],
            summary_stats[dataset_labels[j]]["Mean"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")

print("\nANOVA Results (Average Comparison):")
average_anova_results = f_oneway(
    summary_stats["videodata"]["Average"],
    summary_stats["videodata1"]["Average"],
    summary_stats["videodata2"]["Average"],
    summary_stats["videodata3"]["Average"]
)
print(f"F-statistic: {average_anova_results.statistic:.4f}, P-value: {average_anova_results.pvalue:.4f}")

print("\nPairwise T-tests (Sum Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Sum"],
            summary_stats[dataset_labels[j]]["Sum"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")
# Pairwise Comparison for Averages
print("\nPairwise T-tests (Average Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Average"],
            summary_stats[dataset_labels[j]]["Average"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")

# Ensure all datasets have the same number of rows
num_subjects = videodata.shape[0]
if any(dataset.shape[0] != num_subjects for dataset in [videodata1, videodata2, videodata3]):
    raise ValueError("All video datasets must have the same number of subjects (rows).")

# Calculate the sum for each subject in each video
sums_video1 = np.sum(videodata, axis=1)
sums_video2 = np.sum(videodata1, axis=1)
sums_video3 = np.sum(videodata2, axis=1)
sums_video4 = np.sum(videodata3, axis=1)

# Create a DataFrame to store the results
subjects = [f"Subject_{i+1}" for i in range(num_subjects)]  # Generate subject labels

sum_data = pd.DataFrame({
    "Subject": subjects,
    "Video1_Sum": sums_video1,
    "Video2_Sum": sums_video2,
    "Video3_Sum": sums_video3,
    "Video4_Sum": sums_video4
})

# Save the data to a CSV file
output_file = "subject_sums.csv"
sum_data.to_csv(output_file, index=False)

# Calculate correlation matrices using different methods
correlation_matrix_pearson = sum_data.drop("Subject", axis=1).corr(method="pearson")
correlation_matrix_spearman = sum_data.drop("Subject", axis=1).corr(method="spearman")
correlation_matrix_kendall = sum_data.drop("Subject", axis=1).corr(method="kendall")

# Print correlation matrices
print("Pearson Correlation Matrix:")
print(correlation_matrix_pearson)
print("\nSpearman Correlation Matrix:")
print(correlation_matrix_spearman)
print("\nKendall Correlation Matrix:")
print(correlation_matrix_kendall)

# Visualize the correlation matrices
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_pearson, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Pearson Correlation Matrix Between Videos (Based on Sums)")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_spearman, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Spearman Correlation Matrix Between Videos (Based on Sums)")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_kendall, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Kendall Correlation Matrix Between Videos (Based on Sums)")
plt.show()

# Determine the best model (pair with the highest correlation in Pearson)
best_pair_pearson = None
max_correlation_pearson = -1

for i, col1 in enumerate(correlation_matrix_pearson.columns):
    for j, col2 in enumerate(correlation_matrix_pearson.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_pearson.loc[col1, col2]
            if correlation > max_correlation_pearson:
                max_correlation_pearson = correlation
                best_pair_pearson = (col1, col2)

print(f"Best Correlated Pair (Pearson): {best_pair_pearson} with Correlation: {max_correlation_pearson:.2f}")

# Determine the best model (pair with the highest correlation in Spearman)
best_pair_spearman = None
max_correlation_spearman = -1

for i, col1 in enumerate(correlation_matrix_spearman.columns):
    for j, col2 in enumerate(correlation_matrix_spearman.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_spearman.loc[col1, col2]
            if correlation > max_correlation_spearman:
                max_correlation_spearman = correlation
                best_pair_spearman = (col1, col2)

print(f"Best Correlated Pair (Spearman): {best_pair_spearman} with Correlation: {max_correlation_spearman:.2f}")

# Determine the best model (pair with the highest correlation in Kendall)
best_pair_kendall = None
max_correlation_kendall = -1

for i, col1 in enumerate(correlation_matrix_kendall.columns):
    for j, col2 in enumerate(correlation_matrix_kendall.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_kendall.loc[col1, col2]
            if correlation > max_correlation_kendall:
                max_correlation_kendall = correlation
                best_pair_kendall = (col1, col2)

print(f"Best Correlated Pair (Kendall): {best_pair_kendall} with Correlation: {max_correlation_kendall:.2f}")

print(f"Sum for each subject saved to {output_file}")

"""NDARDX770PJK"""

videodata= np.load("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARDX770PJK/evaluation/baseline_eLORETA.npy")  #base relax
videodata1= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARDX770PJK/evaluation/video1_eLORETA.npy") #marowa's drive video1
videodata2= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARDX770PJK/evaluation/video2_eLORETA.npy") #video2
videodata3= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARDX770PJK/evaluation/video3_eLORETA.npy") #video3
#luca's drive
#pathLuca = "/content/drive/MyDrive/TU Dublin-NN/research/PHD CANDIDATES - AICL research LAB/CURRENT/Carlos Gomez Tapia - PB04482/dataset - full source activation space (31000x11250) for 35 subjects/"
#fileName = "video1_eLORETA.npy"
#videodata= np.load (pathLuca + fileName)
print(videodata1.shape)
print(len(videodata)) #len of points in brain (31553)
print(videodata[0])
print(len(videodata[0]))  #timestamps  (11250=125hz x 90 seconds)
print(type(videodata))
#video2data is a list of list (first list 31553 lenghth,each of this cell contains 11250 point,31553 is represented the number of the brain and the 11250 is the point in time  )
import numpy as np
import pandas as pd
from scipy.stats import f_oneway, ttest_ind
import matplotlib.pyplot as plt


datasets = [videodata, videodata1, videodata2, videodata3]
dataset_labels = ["videodata", "videodata1", "videodata2", "videodata3"]

# Step 2: Calculate Mean, SD, Sum, and Average for each column
summary_stats = {}
for i, dataset in enumerate(datasets):
    means = np.mean(dataset, axis=0)
    sds = np.std(dataset, axis=0)
    sums = np.sum(dataset, axis=0)
    averages = np.mean(dataset, axis=1)

    summary_stats[dataset_labels[i]] = {
        "Mean": means,
        "SD": sds,
        "Sum": sums,
        "Average": averages
    }

# Step 3: Visualize the Mean and Sum for each dataset
for label, stats in summary_stats.items():
    # Mean Visualization
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Mean"])), stats["Mean"], color='skyblue', label="Mean")
    plt.xlabel('Columns')
    plt.ylabel('Mean Values')
    plt.title(f'Mean for {label}')
    plt.legend()
    plt.show()

    # Sum Visualization
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Sum"])), stats["Sum"], color='orange', label="Sum")
    plt.xlabel('Columns')
    plt.ylabel('Sum Values')
    plt.title(f'Sum for {label}')
    plt.legend()
    plt.show()

    # Average Visualization (per subject)
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Average"])), stats["Average"], color='green', label="Average per Subject")
    plt.xlabel('Subjects')
    plt.ylabel('Average Values')
    plt.title(f'Average per Subject in {label}')
    plt.legend()
    plt.show()
# Step 4: Compare Mean and Sum between subjects in each dataset
for label, stats in summary_stats.items():
    # Mean comparison
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Average"])), stats["Average"], color='green', label="Average per Subject")
    plt.xlabel('Subjects')
    plt.ylabel('Average Values')
    plt.title(f'Average in {label}')
    plt.legend()
    plt.show()

# Step 5: Statistical Comparison between Datasets
print("ANOVA Results (Mean Comparison):")
mean_anova_results = f_oneway(
    summary_stats["videodata"]["Mean"],
    summary_stats["videodata1"]["Mean"],
    summary_stats["videodata2"]["Mean"],
    summary_stats["videodata3"]["Mean"]
)
print(f"F-statistic: {mean_anova_results.statistic:.4f}, P-value: {mean_anova_results.pvalue:.4f}")

print("\nANOVA Results (Sum Comparison):")
sum_anova_results = f_oneway(
    summary_stats["videodata"]["Sum"],
    summary_stats["videodata1"]["Sum"],
    summary_stats["videodata2"]["Sum"],
    summary_stats["videodata3"]["Sum"]
)
print(f"F-statistic: {sum_anova_results.statistic:.4f}, P-value: {sum_anova_results.pvalue:.4f}")

# Pairwise Comparison for Means and Sums
print("\nPairwise T-tests (Mean Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Mean"],
            summary_stats[dataset_labels[j]]["Mean"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")

print("\nANOVA Results (Average Comparison):")
average_anova_results = f_oneway(
    summary_stats["videodata"]["Average"],
    summary_stats["videodata1"]["Average"],
    summary_stats["videodata2"]["Average"],
    summary_stats["videodata3"]["Average"]
)
print(f"F-statistic: {average_anova_results.statistic:.4f}, P-value: {average_anova_results.pvalue:.4f}")

print("\nPairwise T-tests (Sum Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Sum"],
            summary_stats[dataset_labels[j]]["Sum"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")
# Pairwise Comparison for Averages
print("\nPairwise T-tests (Average Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Average"],
            summary_stats[dataset_labels[j]]["Average"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")

# Ensure all datasets have the same number of rows
num_subjects = videodata.shape[0]
if any(dataset.shape[0] != num_subjects for dataset in [videodata1, videodata2, videodata3]):
    raise ValueError("All video datasets must have the same number of subjects (rows).")

# Calculate the sum for each subject in each video
sums_video1 = np.sum(videodata, axis=1)
sums_video2 = np.sum(videodata1, axis=1)
sums_video3 = np.sum(videodata2, axis=1)
sums_video4 = np.sum(videodata3, axis=1)

# Create a DataFrame to store the results
subjects = [f"Subject_{i+1}" for i in range(num_subjects)]  # Generate subject labels

sum_data = pd.DataFrame({
    "Subject": subjects,
    "Video1_Sum": sums_video1,
    "Video2_Sum": sums_video2,
    "Video3_Sum": sums_video3,
    "Video4_Sum": sums_video4
})

# Save the data to a CSV file
output_file = "subject_sums.csv"
sum_data.to_csv(output_file, index=False)

# Calculate correlation matrices using different methods
correlation_matrix_pearson = sum_data.drop("Subject", axis=1).corr(method="pearson")
correlation_matrix_spearman = sum_data.drop("Subject", axis=1).corr(method="spearman")
correlation_matrix_kendall = sum_data.drop("Subject", axis=1).corr(method="kendall")

# Print correlation matrices
print("Pearson Correlation Matrix:")
print(correlation_matrix_pearson)
print("\nSpearman Correlation Matrix:")
print(correlation_matrix_spearman)
print("\nKendall Correlation Matrix:")
print(correlation_matrix_kendall)

# Visualize the correlation matrices
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_pearson, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Pearson Correlation Matrix Between Videos (Based on Sums)")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_spearman, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Spearman Correlation Matrix Between Videos (Based on Sums)")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_kendall, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Kendall Correlation Matrix Between Videos (Based on Sums)")
plt.show()

# Determine the best model (pair with the highest correlation in Pearson)
best_pair_pearson = None
max_correlation_pearson = -1

for i, col1 in enumerate(correlation_matrix_pearson.columns):
    for j, col2 in enumerate(correlation_matrix_pearson.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_pearson.loc[col1, col2]
            if correlation > max_correlation_pearson:
                max_correlation_pearson = correlation
                best_pair_pearson = (col1, col2)

print(f"Best Correlated Pair (Pearson): {best_pair_pearson} with Correlation: {max_correlation_pearson:.2f}")

# Determine the best model (pair with the highest correlation in Spearman)
best_pair_spearman = None
max_correlation_spearman = -1

for i, col1 in enumerate(correlation_matrix_spearman.columns):
    for j, col2 in enumerate(correlation_matrix_spearman.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_spearman.loc[col1, col2]
            if correlation > max_correlation_spearman:
                max_correlation_spearman = correlation
                best_pair_spearman = (col1, col2)

print(f"Best Correlated Pair (Spearman): {best_pair_spearman} with Correlation: {max_correlation_spearman:.2f}")

# Determine the best model (pair with the highest correlation in Kendall)
best_pair_kendall = None
max_correlation_kendall = -1

for i, col1 in enumerate(correlation_matrix_kendall.columns):
    for j, col2 in enumerate(correlation_matrix_kendall.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_kendall.loc[col1, col2]
            if correlation > max_correlation_kendall:
                max_correlation_kendall = correlation
                best_pair_kendall = (col1, col2)

print(f"Best Correlated Pair (Kendall): {best_pair_kendall} with Correlation: {max_correlation_kendall:.2f}")

print(f"Sum for each subject saved to {output_file}")

"""NDARHR443EHF"""

videodata= np.load("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARHR443EHF/evaluation/baseline_eLORETA.npy")  #base relax
videodata1= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARHR443EHF/evaluation/video1_eLORETA.npy") #marowa's drive video1
videodata2= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARHR443EHF/evaluation/video2_eLORETA.npy") #video2
videodata3= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARHR443EHF/evaluation/video3_eLORETA.npy") #video3
#luca's drive
#pathLuca = "/content/drive/MyDrive/TU Dublin-NN/research/PHD CANDIDATES - AICL research LAB/CURRENT/Carlos Gomez Tapia - PB04482/dataset - full source activation space (31000x11250) for 35 subjects/"
#fileName = "video1_eLORETA.npy"
#videodata= np.load (pathLuca + fileName)
print(videodata1.shape)
print(len(videodata)) #len of points in brain (31553)
print(videodata[0])
print(len(videodata[0]))  #timestamps  (11250=125hz x 90 seconds)
print(type(videodata))
#video2data is a list of list (first list 31553 lenghth,each of this cell contains 11250 point,31553 is represented the number of the brain and the 11250 is the point in time  )
import numpy as np
import pandas as pd
from scipy.stats import f_oneway, ttest_ind
import matplotlib.pyplot as plt


datasets = [videodata, videodata1, videodata2, videodata3]
dataset_labels = ["videodata", "videodata1", "videodata2", "videodata3"]

# Step 2: Calculate Mean, SD, Sum, and Average for each column
summary_stats = {}
for i, dataset in enumerate(datasets):
    means = np.mean(dataset, axis=0)
    sds = np.std(dataset, axis=0)
    sums = np.sum(dataset, axis=0)
    averages = np.mean(dataset, axis=1)

    summary_stats[dataset_labels[i]] = {
        "Mean": means,
        "SD": sds,
        "Sum": sums,
        "Average": averages
    }

# Step 3: Visualize the Mean and Sum for each dataset
for label, stats in summary_stats.items():
    # Mean Visualization
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Mean"])), stats["Mean"], color='skyblue', label="Mean")
    plt.xlabel('Columns')
    plt.ylabel('Mean Values')
    plt.title(f'Mean for {label}')
    plt.legend()
    plt.show()

    # Sum Visualization
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Sum"])), stats["Sum"], color='orange', label="Sum")
    plt.xlabel('Columns')
    plt.ylabel('Sum Values')
    plt.title(f'Sum for {label}')
    plt.legend()
    plt.show()

    # Average Visualization (per subject)
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Average"])), stats["Average"], color='green', label="Average per Subject")
    plt.xlabel('Subjects')
    plt.ylabel('Average Values')
    plt.title(f'Average per Subject in {label}')
    plt.legend()
    plt.show()
# Step 4: Compare Mean and Sum between subjects in each dataset
for label, stats in summary_stats.items():
    # Mean comparison
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Average"])), stats["Average"], color='green', label="Average per Subject")
    plt.xlabel('Subjects')
    plt.ylabel('Average Values')
    plt.title(f'Average in {label}')
    plt.legend()
    plt.show()

# Step 5: Statistical Comparison between Datasets
print("ANOVA Results (Mean Comparison):")
mean_anova_results = f_oneway(
    summary_stats["videodata"]["Mean"],
    summary_stats["videodata1"]["Mean"],
    summary_stats["videodata2"]["Mean"],
    summary_stats["videodata3"]["Mean"]
)
print(f"F-statistic: {mean_anova_results.statistic:.4f}, P-value: {mean_anova_results.pvalue:.4f}")

print("\nANOVA Results (Sum Comparison):")
sum_anova_results = f_oneway(
    summary_stats["videodata"]["Sum"],
    summary_stats["videodata1"]["Sum"],
    summary_stats["videodata2"]["Sum"],
    summary_stats["videodata3"]["Sum"]
)
print(f"F-statistic: {sum_anova_results.statistic:.4f}, P-value: {sum_anova_results.pvalue:.4f}")

# Pairwise Comparison for Means and Sums
print("\nPairwise T-tests (Mean Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Mean"],
            summary_stats[dataset_labels[j]]["Mean"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")

print("\nANOVA Results (Average Comparison):")
average_anova_results = f_oneway(
    summary_stats["videodata"]["Average"],
    summary_stats["videodata1"]["Average"],
    summary_stats["videodata2"]["Average"],
    summary_stats["videodata3"]["Average"]
)
print(f"F-statistic: {average_anova_results.statistic:.4f}, P-value: {average_anova_results.pvalue:.4f}")

print("\nPairwise T-tests (Sum Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Sum"],
            summary_stats[dataset_labels[j]]["Sum"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")
# Pairwise Comparison for Averages
print("\nPairwise T-tests (Average Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Average"],
            summary_stats[dataset_labels[j]]["Average"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")

# Ensure all datasets have the same number of rows
num_subjects = videodata.shape[0]
if any(dataset.shape[0] != num_subjects for dataset in [videodata1, videodata2, videodata3]):
    raise ValueError("All video datasets must have the same number of subjects (rows).")

# Calculate the sum for each subject in each video
sums_video1 = np.sum(videodata, axis=1)
sums_video2 = np.sum(videodata1, axis=1)
sums_video3 = np.sum(videodata2, axis=1)
sums_video4 = np.sum(videodata3, axis=1)

# Create a DataFrame to store the results
subjects = [f"Subject_{i+1}" for i in range(num_subjects)]  # Generate subject labels

sum_data = pd.DataFrame({
    "Subject": subjects,
    "Video1_Sum": sums_video1,
    "Video2_Sum": sums_video2,
    "Video3_Sum": sums_video3,
    "Video4_Sum": sums_video4
})

# Save the data to a CSV file
output_file = "subject_sums.csv"
sum_data.to_csv(output_file, index=False)

# Calculate correlation matrices using different methods
correlation_matrix_pearson = sum_data.drop("Subject", axis=1).corr(method="pearson")
correlation_matrix_spearman = sum_data.drop("Subject", axis=1).corr(method="spearman")
correlation_matrix_kendall = sum_data.drop("Subject", axis=1).corr(method="kendall")

# Print correlation matrices
print("Pearson Correlation Matrix:")
print(correlation_matrix_pearson)
print("\nSpearman Correlation Matrix:")
print(correlation_matrix_spearman)
print("\nKendall Correlation Matrix:")
print(correlation_matrix_kendall)

# Visualize the correlation matrices
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_pearson, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Pearson Correlation Matrix Between Videos (Based on Sums)")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_spearman, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Spearman Correlation Matrix Between Videos (Based on Sums)")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_kendall, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Kendall Correlation Matrix Between Videos (Based on Sums)")
plt.show()

# Determine the best model (pair with the highest correlation in Pearson)
best_pair_pearson = None
max_correlation_pearson = -1

for i, col1 in enumerate(correlation_matrix_pearson.columns):
    for j, col2 in enumerate(correlation_matrix_pearson.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_pearson.loc[col1, col2]
            if correlation > max_correlation_pearson:
                max_correlation_pearson = correlation
                best_pair_pearson = (col1, col2)

print(f"Best Correlated Pair (Pearson): {best_pair_pearson} with Correlation: {max_correlation_pearson:.2f}")

# Determine the best model (pair with the highest correlation in Spearman)
best_pair_spearman = None
max_correlation_spearman = -1

for i, col1 in enumerate(correlation_matrix_spearman.columns):
    for j, col2 in enumerate(correlation_matrix_spearman.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_spearman.loc[col1, col2]
            if correlation > max_correlation_spearman:
                max_correlation_spearman = correlation
                best_pair_spearman = (col1, col2)

print(f"Best Correlated Pair (Spearman): {best_pair_spearman} with Correlation: {max_correlation_spearman:.2f}")

# Determine the best model (pair with the highest correlation in Kendall)
best_pair_kendall = None
max_correlation_kendall = -1

for i, col1 in enumerate(correlation_matrix_kendall.columns):
    for j, col2 in enumerate(correlation_matrix_kendall.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_kendall.loc[col1, col2]
            if correlation > max_correlation_kendall:
                max_correlation_kendall = correlation
                best_pair_kendall = (col1, col2)

print(f"Best Correlated Pair (Kendall): {best_pair_kendall} with Correlation: {max_correlation_kendall:.2f}")

print(f"Sum for each subject saved to {output_file}")

"""NDARHL164JHH"""

videodata= np.load("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARHL164JHH/evaluation/baseline_eLORETA.npy")  #base relax
videodata1= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARHL164JHH/evaluation/video1_eLORETA.npy") #marowa's drive video1
videodata2= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARHL164JHH/evaluation/video2_eLORETA.npy") #video2
videodata3= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARHL164JHH/evaluation/video3_eLORETA.npy") #video3
#luca's drive
#pathLuca = "/content/drive/MyDrive/TU Dublin-NN/research/PHD CANDIDATES - AICL research LAB/CURRENT/Carlos Gomez Tapia - PB04482/dataset - full source activation space (31000x11250) for 35 subjects/"
#fileName = "video1_eLORETA.npy"
#videodata= np.load (pathLuca + fileName)
print(videodata1.shape)
print(len(videodata)) #len of points in brain (31553)
print(videodata[0])
print(len(videodata[0]))  #timestamps  (11250=125hz x 90 seconds)
print(type(videodata))
#video2data is a list of list (first list 31553 lenghth,each of this cell contains 11250 point,31553 is represented the number of the brain and the 11250 is the point in time  )
import numpy as np
import pandas as pd
from scipy.stats import f_oneway, ttest_ind
import matplotlib.pyplot as plt


datasets = [videodata, videodata1, videodata2, videodata3]
dataset_labels = ["videodata", "videodata1", "videodata2", "videodata3"]

# Step 2: Calculate Mean, SD, Sum, and Average for each column
summary_stats = {}
for i, dataset in enumerate(datasets):
    means = np.mean(dataset, axis=0)
    sds = np.std(dataset, axis=0)
    sums = np.sum(dataset, axis=0)
    averages = np.mean(dataset, axis=1)

    summary_stats[dataset_labels[i]] = {
        "Mean": means,
        "SD": sds,
        "Sum": sums,
        "Average": averages
    }

# Step 3: Visualize the Mean and Sum for each dataset
for label, stats in summary_stats.items():
    # Mean Visualization
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Mean"])), stats["Mean"], color='skyblue', label="Mean")
    plt.xlabel('Columns')
    plt.ylabel('Mean Values')
    plt.title(f'Mean for {label}')
    plt.legend()
    plt.show()

    # Sum Visualization
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Sum"])), stats["Sum"], color='orange', label="Sum")
    plt.xlabel('Columns')
    plt.ylabel('Sum Values')
    plt.title(f'Sum for {label}')
    plt.legend()
    plt.show()

    # Average Visualization (per subject)
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Average"])), stats["Average"], color='green', label="Average per Subject")
    plt.xlabel('Subjects')
    plt.ylabel('Average Values')
    plt.title(f'Average per Subject in {label}')
    plt.legend()
    plt.show()
# Step 4: Compare Mean and Sum between subjects in each dataset
for label, stats in summary_stats.items():
    # Mean comparison
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Average"])), stats["Average"], color='green', label="Average per Subject")
    plt.xlabel('Subjects')
    plt.ylabel('Average Values')
    plt.title(f'Average in {label}')
    plt.legend()
    plt.show()

# Step 5: Statistical Comparison between Datasets
print("ANOVA Results (Mean Comparison):")
mean_anova_results = f_oneway(
    summary_stats["videodata"]["Mean"],
    summary_stats["videodata1"]["Mean"],
    summary_stats["videodata2"]["Mean"],
    summary_stats["videodata3"]["Mean"]
)
print(f"F-statistic: {mean_anova_results.statistic:.4f}, P-value: {mean_anova_results.pvalue:.4f}")

print("\nANOVA Results (Sum Comparison):")
sum_anova_results = f_oneway(
    summary_stats["videodata"]["Sum"],
    summary_stats["videodata1"]["Sum"],
    summary_stats["videodata2"]["Sum"],
    summary_stats["videodata3"]["Sum"]
)
print(f"F-statistic: {sum_anova_results.statistic:.4f}, P-value: {sum_anova_results.pvalue:.4f}")

# Pairwise Comparison for Means and Sums
print("\nPairwise T-tests (Mean Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Mean"],
            summary_stats[dataset_labels[j]]["Mean"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")

print("\nANOVA Results (Average Comparison):")
average_anova_results = f_oneway(
    summary_stats["videodata"]["Average"],
    summary_stats["videodata1"]["Average"],
    summary_stats["videodata2"]["Average"],
    summary_stats["videodata3"]["Average"]
)
print(f"F-statistic: {average_anova_results.statistic:.4f}, P-value: {average_anova_results.pvalue:.4f}")

print("\nPairwise T-tests (Sum Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Sum"],
            summary_stats[dataset_labels[j]]["Sum"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")
# Pairwise Comparison for Averages
print("\nPairwise T-tests (Average Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Average"],
            summary_stats[dataset_labels[j]]["Average"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")

# Ensure all datasets have the same number of rows
num_subjects = videodata.shape[0]
if any(dataset.shape[0] != num_subjects for dataset in [videodata1, videodata2, videodata3]):
    raise ValueError("All video datasets must have the same number of subjects (rows).")

# Calculate the sum for each subject in each video
sums_video1 = np.sum(videodata, axis=1)
sums_video2 = np.sum(videodata1, axis=1)
sums_video3 = np.sum(videodata2, axis=1)
sums_video4 = np.sum(videodata3, axis=1)

# Create a DataFrame to store the results
subjects = [f"Subject_{i+1}" for i in range(num_subjects)]  # Generate subject labels

sum_data = pd.DataFrame({
    "Subject": subjects,
    "Video1_Sum": sums_video1,
    "Video2_Sum": sums_video2,
    "Video3_Sum": sums_video3,
    "Video4_Sum": sums_video4
})

# Save the data to a CSV file
output_file = "subject_sums.csv"
sum_data.to_csv(output_file, index=False)

# Calculate correlation matrices using different methods
correlation_matrix_pearson = sum_data.drop("Subject", axis=1).corr(method="pearson")
correlation_matrix_spearman = sum_data.drop("Subject", axis=1).corr(method="spearman")
correlation_matrix_kendall = sum_data.drop("Subject", axis=1).corr(method="kendall")

# Print correlation matrices
print("Pearson Correlation Matrix:")
print(correlation_matrix_pearson)
print("\nSpearman Correlation Matrix:")
print(correlation_matrix_spearman)
print("\nKendall Correlation Matrix:")
print(correlation_matrix_kendall)

# Visualize the correlation matrices
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_pearson, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Pearson Correlation Matrix Between Videos (Based on Sums)")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_spearman, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Spearman Correlation Matrix Between Videos (Based on Sums)")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_kendall, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Kendall Correlation Matrix Between Videos (Based on Sums)")
plt.show()

# Determine the best model (pair with the highest correlation in Pearson)
best_pair_pearson = None
max_correlation_pearson = -1

for i, col1 in enumerate(correlation_matrix_pearson.columns):
    for j, col2 in enumerate(correlation_matrix_pearson.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_pearson.loc[col1, col2]
            if correlation > max_correlation_pearson:
                max_correlation_pearson = correlation
                best_pair_pearson = (col1, col2)

print(f"Best Correlated Pair (Pearson): {best_pair_pearson} with Correlation: {max_correlation_pearson:.2f}")

# Determine the best model (pair with the highest correlation in Spearman)
best_pair_spearman = None
max_correlation_spearman = -1

for i, col1 in enumerate(correlation_matrix_spearman.columns):
    for j, col2 in enumerate(correlation_matrix_spearman.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_spearman.loc[col1, col2]
            if correlation > max_correlation_spearman:
                max_correlation_spearman = correlation
                best_pair_spearman = (col1, col2)

print(f"Best Correlated Pair (Spearman): {best_pair_spearman} with Correlation: {max_correlation_spearman:.2f}")

# Determine the best model (pair with the highest correlation in Kendall)
best_pair_kendall = None
max_correlation_kendall = -1

for i, col1 in enumerate(correlation_matrix_kendall.columns):
    for j, col2 in enumerate(correlation_matrix_kendall.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_kendall.loc[col1, col2]
            if correlation > max_correlation_kendall:
                max_correlation_kendall = correlation
                best_pair_kendall = (col1, col2)

print(f"Best Correlated Pair (Kendall): {best_pair_kendall} with Correlation: {max_correlation_kendall:.2f}")

print(f"Sum for each subject saved to {output_file}")

"""NDARHZ413DZL"""

videodata= np.load("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARHZ413DZL/evaluation/baseline_eLORETA.npy")  #base relax
videodata1= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARHZ413DZL/evaluation/video1_eLORETA.npy") #marowa's drive video1
videodata2= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARHZ413DZL/evaluation/video2_eLORETA.npy") #video2
videodata3= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARHZ413DZL/evaluation/video3_eLORETA.npy") #video3
#luca's drive
#pathLuca = "/content/drive/MyDrive/TU Dublin-NN/research/PHD CANDIDATES - AICL research LAB/CURRENT/Carlos Gomez Tapia - PB04482/dataset - full source activation space (31000x11250) for 35 subjects/"
#fileName = "video1_eLORETA.npy"
#videodata= np.load (pathLuca + fileName)
print(videodata1.shape)
print(len(videodata)) #len of points in brain (31553)
print(videodata[0])
print(len(videodata[0]))  #timestamps  (11250=125hz x 90 seconds)
print(type(videodata))
#video2data is a list of list (first list 31553 lenghth,each of this cell contains 11250 point,31553 is represented the number of the brain and the 11250 is the point in time  )
import numpy as np
import pandas as pd
from scipy.stats import f_oneway, ttest_ind
import matplotlib.pyplot as plt


datasets = [videodata, videodata1, videodata2, videodata3]
dataset_labels = ["videodata", "videodata1", "videodata2", "videodata3"]

# Step 2: Calculate Mean, SD, Sum, and Average for each column
summary_stats = {}
for i, dataset in enumerate(datasets):
    means = np.mean(dataset, axis=0)
    sds = np.std(dataset, axis=0)
    sums = np.sum(dataset, axis=0)
    averages = np.mean(dataset, axis=1)

    summary_stats[dataset_labels[i]] = {
        "Mean": means,
        "SD": sds,
        "Sum": sums,
        "Average": averages
    }

# Step 3: Visualize the Mean and Sum for each dataset
for label, stats in summary_stats.items():
    # Mean Visualization
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Mean"])), stats["Mean"], color='skyblue', label="Mean")
    plt.xlabel('Columns')
    plt.ylabel('Mean Values')
    plt.title(f'Mean for {label}')
    plt.legend()
    plt.show()

    # Sum Visualization
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Sum"])), stats["Sum"], color='orange', label="Sum")
    plt.xlabel('Columns')
    plt.ylabel('Sum Values')
    plt.title(f'Sum for {label}')
    plt.legend()
    plt.show()

    # Average Visualization (per subject)
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Average"])), stats["Average"], color='green', label="Average per Subject")
    plt.xlabel('Subjects')
    plt.ylabel('Average Values')
    plt.title(f'Average per Subject in {label}')
    plt.legend()
    plt.show()
# Step 4: Compare Mean and Sum between subjects in each dataset
for label, stats in summary_stats.items():
    # Mean comparison
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Average"])), stats["Average"], color='green', label="Average per Subject")
    plt.xlabel('Subjects')
    plt.ylabel('Average Values')
    plt.title(f'Average in {label}')
    plt.legend()
    plt.show()

# Step 5: Statistical Comparison between Datasets
print("ANOVA Results (Mean Comparison):")
mean_anova_results = f_oneway(
    summary_stats["videodata"]["Mean"],
    summary_stats["videodata1"]["Mean"],
    summary_stats["videodata2"]["Mean"],
    summary_stats["videodata3"]["Mean"]
)
print(f"F-statistic: {mean_anova_results.statistic:.4f}, P-value: {mean_anova_results.pvalue:.4f}")

print("\nANOVA Results (Sum Comparison):")
sum_anova_results = f_oneway(
    summary_stats["videodata"]["Sum"],
    summary_stats["videodata1"]["Sum"],
    summary_stats["videodata2"]["Sum"],
    summary_stats["videodata3"]["Sum"]
)
print(f"F-statistic: {sum_anova_results.statistic:.4f}, P-value: {sum_anova_results.pvalue:.4f}")

# Pairwise Comparison for Means and Sums
print("\nPairwise T-tests (Mean Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Mean"],
            summary_stats[dataset_labels[j]]["Mean"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")

print("\nANOVA Results (Average Comparison):")
average_anova_results = f_oneway(
    summary_stats["videodata"]["Average"],
    summary_stats["videodata1"]["Average"],
    summary_stats["videodata2"]["Average"],
    summary_stats["videodata3"]["Average"]
)
print(f"F-statistic: {average_anova_results.statistic:.4f}, P-value: {average_anova_results.pvalue:.4f}")

print("\nPairwise T-tests (Sum Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Sum"],
            summary_stats[dataset_labels[j]]["Sum"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")
# Pairwise Comparison for Averages
print("\nPairwise T-tests (Average Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Average"],
            summary_stats[dataset_labels[j]]["Average"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")

# Ensure all datasets have the same number of rows
num_subjects = videodata.shape[0]
if any(dataset.shape[0] != num_subjects for dataset in [videodata1, videodata2, videodata3]):
    raise ValueError("All video datasets must have the same number of subjects (rows).")

# Calculate the sum for each subject in each video
sums_video1 = np.sum(videodata, axis=1)
sums_video2 = np.sum(videodata1, axis=1)
sums_video3 = np.sum(videodata2, axis=1)
sums_video4 = np.sum(videodata3, axis=1)

# Create a DataFrame to store the results
subjects = [f"Subject_{i+1}" for i in range(num_subjects)]  # Generate subject labels

sum_data = pd.DataFrame({
    "Subject": subjects,
    "Video1_Sum": sums_video1,
    "Video2_Sum": sums_video2,
    "Video3_Sum": sums_video3,
    "Video4_Sum": sums_video4
})

# Save the data to a CSV file
output_file = "subject_sums.csv"
sum_data.to_csv(output_file, index=False)

# Calculate correlation matrices using different methods
correlation_matrix_pearson = sum_data.drop("Subject", axis=1).corr(method="pearson")
correlation_matrix_spearman = sum_data.drop("Subject", axis=1).corr(method="spearman")
correlation_matrix_kendall = sum_data.drop("Subject", axis=1).corr(method="kendall")

# Print correlation matrices
print("Pearson Correlation Matrix:")
print(correlation_matrix_pearson)
print("\nSpearman Correlation Matrix:")
print(correlation_matrix_spearman)
print("\nKendall Correlation Matrix:")
print(correlation_matrix_kendall)

# Visualize the correlation matrices
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_pearson, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Pearson Correlation Matrix Between Videos (Based on Sums)")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_spearman, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Spearman Correlation Matrix Between Videos (Based on Sums)")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_kendall, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Kendall Correlation Matrix Between Videos (Based on Sums)")
plt.show()

# Determine the best model (pair with the highest correlation in Pearson)
best_pair_pearson = None
max_correlation_pearson = -1

for i, col1 in enumerate(correlation_matrix_pearson.columns):
    for j, col2 in enumerate(correlation_matrix_pearson.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_pearson.loc[col1, col2]
            if correlation > max_correlation_pearson:
                max_correlation_pearson = correlation
                best_pair_pearson = (col1, col2)

print(f"Best Correlated Pair (Pearson): {best_pair_pearson} with Correlation: {max_correlation_pearson:.2f}")

# Determine the best model (pair with the highest correlation in Spearman)
best_pair_spearman = None
max_correlation_spearman = -1

for i, col1 in enumerate(correlation_matrix_spearman.columns):
    for j, col2 in enumerate(correlation_matrix_spearman.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_spearman.loc[col1, col2]
            if correlation > max_correlation_spearman:
                max_correlation_spearman = correlation
                best_pair_spearman = (col1, col2)

print(f"Best Correlated Pair (Spearman): {best_pair_spearman} with Correlation: {max_correlation_spearman:.2f}")

# Determine the best model (pair with the highest correlation in Kendall)
best_pair_kendall = None
max_correlation_kendall = -1

for i, col1 in enumerate(correlation_matrix_kendall.columns):
    for j, col2 in enumerate(correlation_matrix_kendall.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_kendall.loc[col1, col2]
            if correlation > max_correlation_kendall:
                max_correlation_kendall = correlation
                best_pair_kendall = (col1, col2)

print(f"Best Correlated Pair (Kendall): {best_pair_kendall} with Correlation: {max_correlation_kendall:.2f}")

print(f"Sum for each subject saved to {output_file}")

"""NDARJG740PM8"""

videodata= np.load("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARJG740PM8/evaluation/baseline_eLORETA.npy")  #base relax
videodata1= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARJG740PM8/evaluation/video1_eLORETA.npy") #marowa's drive video1
videodata2= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARJG740PM8/evaluation/video2_eLORETA.npy") #video2
videodata3= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARJG740PM8/evaluation/video3_eLORETA.npy") #video3
#luca's drive
#pathLuca = "/content/drive/MyDrive/TU Dublin-NN/research/PHD CANDIDATES - AICL research LAB/CURRENT/Carlos Gomez Tapia - PB04482/dataset - full source activation space (31000x11250) for 35 subjects/"
#fileName = "video1_eLORETA.npy"
#videodata= np.load (pathLuca + fileName)
print(videodata1.shape)
print(len(videodata)) #len of points in brain (31553)
print(videodata[0])
print(len(videodata[0]))  #timestamps  (11250=125hz x 90 seconds)
print(type(videodata))
#video2data is a list of list (first list 31553 lenghth,each of this cell contains 11250 point,31553 is represented the number of the brain and the 11250 is the point in time  )
import numpy as np
import pandas as pd
from scipy.stats import f_oneway, ttest_ind
import matplotlib.pyplot as plt


datasets = [videodata, videodata1, videodata2, videodata3]
dataset_labels = ["videodata", "videodata1", "videodata2", "videodata3"]

# Step 2: Calculate Mean, SD, Sum, and Average for each column
summary_stats = {}
for i, dataset in enumerate(datasets):
    means = np.mean(dataset, axis=0)
    sds = np.std(dataset, axis=0)
    sums = np.sum(dataset, axis=0)
    averages = np.mean(dataset, axis=1)

    summary_stats[dataset_labels[i]] = {
        "Mean": means,
        "SD": sds,
        "Sum": sums,
        "Average": averages
    }

# Step 3: Visualize the Mean and Sum for each dataset
for label, stats in summary_stats.items():
    # Mean Visualization
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Mean"])), stats["Mean"], color='skyblue', label="Mean")
    plt.xlabel('Columns')
    plt.ylabel('Mean Values')
    plt.title(f'Mean for {label}')
    plt.legend()
    plt.show()

    # Sum Visualization
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Sum"])), stats["Sum"], color='orange', label="Sum")
    plt.xlabel('Columns')
    plt.ylabel('Sum Values')
    plt.title(f'Sum for {label}')
    plt.legend()
    plt.show()

    # Average Visualization (per subject)
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Average"])), stats["Average"], color='green', label="Average per Subject")
    plt.xlabel('Subjects')
    plt.ylabel('Average Values')
    plt.title(f'Average per Subject in {label}')
    plt.legend()
    plt.show()
# Step 4: Compare Mean and Sum between subjects in each dataset
for label, stats in summary_stats.items():
    # Mean comparison
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Average"])), stats["Average"], color='green', label="Average per Subject")
    plt.xlabel('Subjects')
    plt.ylabel('Average Values')
    plt.title(f'Average in {label}')
    plt.legend()
    plt.show()

# Step 5: Statistical Comparison between Datasets
print("ANOVA Results (Mean Comparison):")
mean_anova_results = f_oneway(
    summary_stats["videodata"]["Mean"],
    summary_stats["videodata1"]["Mean"],
    summary_stats["videodata2"]["Mean"],
    summary_stats["videodata3"]["Mean"]
)
print(f"F-statistic: {mean_anova_results.statistic:.4f}, P-value: {mean_anova_results.pvalue:.4f}")

print("\nANOVA Results (Sum Comparison):")
sum_anova_results = f_oneway(
    summary_stats["videodata"]["Sum"],
    summary_stats["videodata1"]["Sum"],
    summary_stats["videodata2"]["Sum"],
    summary_stats["videodata3"]["Sum"]
)
print(f"F-statistic: {sum_anova_results.statistic:.4f}, P-value: {sum_anova_results.pvalue:.4f}")

# Pairwise Comparison for Means and Sums
print("\nPairwise T-tests (Mean Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Mean"],
            summary_stats[dataset_labels[j]]["Mean"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")

print("\nANOVA Results (Average Comparison):")
average_anova_results = f_oneway(
    summary_stats["videodata"]["Average"],
    summary_stats["videodata1"]["Average"],
    summary_stats["videodata2"]["Average"],
    summary_stats["videodata3"]["Average"]
)
print(f"F-statistic: {average_anova_results.statistic:.4f}, P-value: {average_anova_results.pvalue:.4f}")

print("\nPairwise T-tests (Sum Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Sum"],
            summary_stats[dataset_labels[j]]["Sum"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")
# Pairwise Comparison for Averages
print("\nPairwise T-tests (Average Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Average"],
            summary_stats[dataset_labels[j]]["Average"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")

# Ensure all datasets have the same number of rows
num_subjects = videodata.shape[0]
if any(dataset.shape[0] != num_subjects for dataset in [videodata1, videodata2, videodata3]):
    raise ValueError("All video datasets must have the same number of subjects (rows).")

# Calculate the sum for each subject in each video
sums_video1 = np.sum(videodata, axis=1)
sums_video2 = np.sum(videodata1, axis=1)
sums_video3 = np.sum(videodata2, axis=1)
sums_video4 = np.sum(videodata3, axis=1)

# Create a DataFrame to store the results
subjects = [f"Subject_{i+1}" for i in range(num_subjects)]  # Generate subject labels

sum_data = pd.DataFrame({
    "Subject": subjects,
    "Video1_Sum": sums_video1,
    "Video2_Sum": sums_video2,
    "Video3_Sum": sums_video3,
    "Video4_Sum": sums_video4
})

# Save the data to a CSV file
output_file = "subject_sums.csv"
sum_data.to_csv(output_file, index=False)

# Calculate correlation matrices using different methods
correlation_matrix_pearson = sum_data.drop("Subject", axis=1).corr(method="pearson")
correlation_matrix_spearman = sum_data.drop("Subject", axis=1).corr(method="spearman")
correlation_matrix_kendall = sum_data.drop("Subject", axis=1).corr(method="kendall")

# Print correlation matrices
print("Pearson Correlation Matrix:")
print(correlation_matrix_pearson)
print("\nSpearman Correlation Matrix:")
print(correlation_matrix_spearman)
print("\nKendall Correlation Matrix:")
print(correlation_matrix_kendall)

# Visualize the correlation matrices
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_pearson, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Pearson Correlation Matrix Between Videos (Based on Sums)")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_spearman, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Spearman Correlation Matrix Between Videos (Based on Sums)")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_kendall, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Kendall Correlation Matrix Between Videos (Based on Sums)")
plt.show()

# Determine the best model (pair with the highest correlation in Pearson)
best_pair_pearson = None
max_correlation_pearson = -1

for i, col1 in enumerate(correlation_matrix_pearson.columns):
    for j, col2 in enumerate(correlation_matrix_pearson.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_pearson.loc[col1, col2]
            if correlation > max_correlation_pearson:
                max_correlation_pearson = correlation
                best_pair_pearson = (col1, col2)

print(f"Best Correlated Pair (Pearson): {best_pair_pearson} with Correlation: {max_correlation_pearson:.2f}")

# Determine the best model (pair with the highest correlation in Spearman)
best_pair_spearman = None
max_correlation_spearman = -1

for i, col1 in enumerate(correlation_matrix_spearman.columns):
    for j, col2 in enumerate(correlation_matrix_spearman.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_spearman.loc[col1, col2]
            if correlation > max_correlation_spearman:
                max_correlation_spearman = correlation
                best_pair_spearman = (col1, col2)

print(f"Best Correlated Pair (Spearman): {best_pair_spearman} with Correlation: {max_correlation_spearman:.2f}")

# Determine the best model (pair with the highest correlation in Kendall)
best_pair_kendall = None
max_correlation_kendall = -1

for i, col1 in enumerate(correlation_matrix_kendall.columns):
    for j, col2 in enumerate(correlation_matrix_kendall.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_kendall.loc[col1, col2]
            if correlation > max_correlation_kendall:
                max_correlation_kendall = correlation
                best_pair_kendall = (col1, col2)

print(f"Best Correlated Pair (Kendall): {best_pair_kendall} with Correlation: {max_correlation_kendall:.2f}")

print(f"Sum for each subject saved to {output_file}")

"""NDARKG697CEW"""

videodata= np.load("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARKG697CEW/evaluation/baseline_eLORETA.npy")  #base relax
videodata1= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARKG697CEW/evaluation/video1_eLORETA.npy") #marowa's drive video1
videodata2= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARKG697CEW/evaluation/video2_eLORETA.npy") #video2
videodata3= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARKG697CEW/evaluation/video3_eLORETA.npy") #video3
#luca's drive
#pathLuca = "/content/drive/MyDrive/TU Dublin-NN/research/PHD CANDIDATES - AICL research LAB/CURRENT/Carlos Gomez Tapia - PB04482/dataset - full source activation space (31000x11250) for 35 subjects/"
#fileName = "video1_eLORETA.npy"
#videodata= np.load (pathLuca + fileName)
print(videodata1.shape)
print(len(videodata)) #len of points in brain (31553)
print(videodata[0])
print(len(videodata[0]))  #timestamps  (11250=125hz x 90 seconds)
print(type(videodata))
#video2data is a list of list (first list 31553 lenghth,each of this cell contains 11250 point,31553 is represented the number of the brain and the 11250 is the point in time  )
import numpy as np
import pandas as pd
from scipy.stats import f_oneway, ttest_ind
import matplotlib.pyplot as plt


datasets = [videodata, videodata1, videodata2, videodata3]
dataset_labels = ["videodata", "videodata1", "videodata2", "videodata3"]

# Step 2: Calculate Mean, SD, Sum, and Average for each column
summary_stats = {}
for i, dataset in enumerate(datasets):
    means = np.mean(dataset, axis=0)
    sds = np.std(dataset, axis=0)
    sums = np.sum(dataset, axis=0)
    averages = np.mean(dataset, axis=1)

    summary_stats[dataset_labels[i]] = {
        "Mean": means,
        "SD": sds,
        "Sum": sums,
        "Average": averages
    }

# Step 3: Visualize the Mean and Sum for each dataset
for label, stats in summary_stats.items():
    # Mean Visualization
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Mean"])), stats["Mean"], color='skyblue', label="Mean")
    plt.xlabel('Columns')
    plt.ylabel('Mean Values')
    plt.title(f'Mean for {label}')
    plt.legend()
    plt.show()

    # Sum Visualization
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Sum"])), stats["Sum"], color='orange', label="Sum")
    plt.xlabel('Columns')
    plt.ylabel('Sum Values')
    plt.title(f'Sum for {label}')
    plt.legend()
    plt.show()

    # Average Visualization (per subject)
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Average"])), stats["Average"], color='green', label="Average per Subject")
    plt.xlabel('Subjects')
    plt.ylabel('Average Values')
    plt.title(f'Average per Subject in {label}')
    plt.legend()
    plt.show()
# Step 4: Compare Mean and Sum between subjects in each dataset
for label, stats in summary_stats.items():
    # Mean comparison
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Average"])), stats["Average"], color='green', label="Average per Subject")
    plt.xlabel('Subjects')
    plt.ylabel('Average Values')
    plt.title(f'Average in {label}')
    plt.legend()
    plt.show()

# Step 5: Statistical Comparison between Datasets
print("ANOVA Results (Mean Comparison):")
mean_anova_results = f_oneway(
    summary_stats["videodata"]["Mean"],
    summary_stats["videodata1"]["Mean"],
    summary_stats["videodata2"]["Mean"],
    summary_stats["videodata3"]["Mean"]
)
print(f"F-statistic: {mean_anova_results.statistic:.4f}, P-value: {mean_anova_results.pvalue:.4f}")

print("\nANOVA Results (Sum Comparison):")
sum_anova_results = f_oneway(
    summary_stats["videodata"]["Sum"],
    summary_stats["videodata1"]["Sum"],
    summary_stats["videodata2"]["Sum"],
    summary_stats["videodata3"]["Sum"]
)
print(f"F-statistic: {sum_anova_results.statistic:.4f}, P-value: {sum_anova_results.pvalue:.4f}")

# Pairwise Comparison for Means and Sums
print("\nPairwise T-tests (Mean Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Mean"],
            summary_stats[dataset_labels[j]]["Mean"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")

print("\nANOVA Results (Average Comparison):")
average_anova_results = f_oneway(
    summary_stats["videodata"]["Average"],
    summary_stats["videodata1"]["Average"],
    summary_stats["videodata2"]["Average"],
    summary_stats["videodata3"]["Average"]
)
print(f"F-statistic: {average_anova_results.statistic:.4f}, P-value: {average_anova_results.pvalue:.4f}")

print("\nPairwise T-tests (Sum Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Sum"],
            summary_stats[dataset_labels[j]]["Sum"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")
# Pairwise Comparison for Averages
print("\nPairwise T-tests (Average Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Average"],
            summary_stats[dataset_labels[j]]["Average"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")

# Ensure all datasets have the same number of rows
num_subjects = videodata.shape[0]
if any(dataset.shape[0] != num_subjects for dataset in [videodata1, videodata2, videodata3]):
    raise ValueError("All video datasets must have the same number of subjects (rows).")

# Calculate the sum for each subject in each video
sums_video1 = np.sum(videodata, axis=1)
sums_video2 = np.sum(videodata1, axis=1)
sums_video3 = np.sum(videodata2, axis=1)
sums_video4 = np.sum(videodata3, axis=1)

# Create a DataFrame to store the results
subjects = [f"Subject_{i+1}" for i in range(num_subjects)]  # Generate subject labels

sum_data = pd.DataFrame({
    "Subject": subjects,
    "Video1_Sum": sums_video1,
    "Video2_Sum": sums_video2,
    "Video3_Sum": sums_video3,
    "Video4_Sum": sums_video4
})

# Save the data to a CSV file
output_file = "subject_sums.csv"
sum_data.to_csv(output_file, index=False)

# Calculate correlation matrices using different methods
correlation_matrix_pearson = sum_data.drop("Subject", axis=1).corr(method="pearson")
correlation_matrix_spearman = sum_data.drop("Subject", axis=1).corr(method="spearman")
correlation_matrix_kendall = sum_data.drop("Subject", axis=1).corr(method="kendall")

# Print correlation matrices
print("Pearson Correlation Matrix:")
print(correlation_matrix_pearson)
print("\nSpearman Correlation Matrix:")
print(correlation_matrix_spearman)
print("\nKendall Correlation Matrix:")
print(correlation_matrix_kendall)

# Visualize the correlation matrices
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_pearson, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Pearson Correlation Matrix Between Videos (Based on Sums)")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_spearman, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Spearman Correlation Matrix Between Videos (Based on Sums)")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_kendall, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Kendall Correlation Matrix Between Videos (Based on Sums)")
plt.show()

# Determine the best model (pair with the highest correlation in Pearson)
best_pair_pearson = None
max_correlation_pearson = -1

for i, col1 in enumerate(correlation_matrix_pearson.columns):
    for j, col2 in enumerate(correlation_matrix_pearson.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_pearson.loc[col1, col2]
            if correlation > max_correlation_pearson:
                max_correlation_pearson = correlation
                best_pair_pearson = (col1, col2)

print(f"Best Correlated Pair (Pearson): {best_pair_pearson} with Correlation: {max_correlation_pearson:.2f}")

# Determine the best model (pair with the highest correlation in Spearman)
best_pair_spearman = None
max_correlation_spearman = -1

for i, col1 in enumerate(correlation_matrix_spearman.columns):
    for j, col2 in enumerate(correlation_matrix_spearman.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_spearman.loc[col1, col2]
            if correlation > max_correlation_spearman:
                max_correlation_spearman = correlation
                best_pair_spearman = (col1, col2)

print(f"Best Correlated Pair (Spearman): {best_pair_spearman} with Correlation: {max_correlation_spearman:.2f}")

# Determine the best model (pair with the highest correlation in Kendall)
best_pair_kendall = None
max_correlation_kendall = -1

for i, col1 in enumerate(correlation_matrix_kendall.columns):
    for j, col2 in enumerate(correlation_matrix_kendall.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_kendall.loc[col1, col2]
            if correlation > max_correlation_kendall:
                max_correlation_kendall = correlation
                best_pair_kendall = (col1, col2)

print(f"Best Correlated Pair (Kendall): {best_pair_kendall} with Correlation: {max_correlation_kendall:.2f}")

print(f"Sum for each subject saved to {output_file}")

"""NDARKZ031NJZ"""

videodata= np.load("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARKZ031NJZ/evaluation/baseline_eLORETA.npy")  #base relax
videodata1= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARKZ031NJZ/evaluation/video1_eLORETA.npy") #marowa's drive video1
videodata2= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARKZ031NJZ/evaluation/video2_eLORETA.npy") #video2
videodata3= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARKZ031NJZ/evaluation/video3_eLORETA.npy") #video3
#luca's drive
#pathLuca = "/content/drive/MyDrive/TU Dublin-NN/research/PHD CANDIDATES - AICL research LAB/CURRENT/Carlos Gomez Tapia - PB04482/dataset - full source activation space (31000x11250) for 35 subjects/"
#fileName = "video1_eLORETA.npy"
#videodata= np.load (pathLuca + fileName)
print(videodata1.shape)
print(len(videodata)) #len of points in brain (31553)
print(videodata[0])
print(len(videodata[0]))  #timestamps  (11250=125hz x 90 seconds)
print(type(videodata))
#video2data is a list of list (first list 31553 lenghth,each of this cell contains 11250 point,31553 is represented the number of the brain and the 11250 is the point in time  )
import numpy as np
import pandas as pd
from scipy.stats import f_oneway, ttest_ind
import matplotlib.pyplot as plt


datasets = [videodata, videodata1, videodata2, videodata3]
dataset_labels = ["videodata", "videodata1", "videodata2", "videodata3"]

# Step 2: Calculate Mean, SD, Sum, and Average for each column
summary_stats = {}
for i, dataset in enumerate(datasets):
    means = np.mean(dataset, axis=0)
    sds = np.std(dataset, axis=0)
    sums = np.sum(dataset, axis=0)
    averages = np.mean(dataset, axis=1)

    summary_stats[dataset_labels[i]] = {
        "Mean": means,
        "SD": sds,
        "Sum": sums,
        "Average": averages
    }

# Step 3: Visualize the Mean and Sum for each dataset
for label, stats in summary_stats.items():
    # Mean Visualization
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Mean"])), stats["Mean"], color='skyblue', label="Mean")
    plt.xlabel('Columns')
    plt.ylabel('Mean Values')
    plt.title(f'Mean for {label}')
    plt.legend()
    plt.show()

    # Sum Visualization
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Sum"])), stats["Sum"], color='orange', label="Sum")
    plt.xlabel('Columns')
    plt.ylabel('Sum Values')
    plt.title(f'Sum for {label}')
    plt.legend()
    plt.show()

    # Average Visualization (per subject)
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Average"])), stats["Average"], color='green', label="Average per Subject")
    plt.xlabel('Subjects')
    plt.ylabel('Average Values')
    plt.title(f'Average per Subject in {label}')
    plt.legend()
    plt.show()
# Step 4: Compare Mean and Sum between subjects in each dataset
for label, stats in summary_stats.items():
    # Mean comparison
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Average"])), stats["Average"], color='green', label="Average per Subject")
    plt.xlabel('Subjects')
    plt.ylabel('Average Values')
    plt.title(f'Average in {label}')
    plt.legend()
    plt.show()

# Step 5: Statistical Comparison between Datasets
print("ANOVA Results (Mean Comparison):")
mean_anova_results = f_oneway(
    summary_stats["videodata"]["Mean"],
    summary_stats["videodata1"]["Mean"],
    summary_stats["videodata2"]["Mean"],
    summary_stats["videodata3"]["Mean"]
)
print(f"F-statistic: {mean_anova_results.statistic:.4f}, P-value: {mean_anova_results.pvalue:.4f}")

print("\nANOVA Results (Sum Comparison):")
sum_anova_results = f_oneway(
    summary_stats["videodata"]["Sum"],
    summary_stats["videodata1"]["Sum"],
    summary_stats["videodata2"]["Sum"],
    summary_stats["videodata3"]["Sum"]
)
print(f"F-statistic: {sum_anova_results.statistic:.4f}, P-value: {sum_anova_results.pvalue:.4f}")

# Pairwise Comparison for Means and Sums
print("\nPairwise T-tests (Mean Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Mean"],
            summary_stats[dataset_labels[j]]["Mean"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")

print("\nANOVA Results (Average Comparison):")
average_anova_results = f_oneway(
    summary_stats["videodata"]["Average"],
    summary_stats["videodata1"]["Average"],
    summary_stats["videodata2"]["Average"],
    summary_stats["videodata3"]["Average"]
)
print(f"F-statistic: {average_anova_results.statistic:.4f}, P-value: {average_anova_results.pvalue:.4f}")

print("\nPairwise T-tests (Sum Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Sum"],
            summary_stats[dataset_labels[j]]["Sum"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")
# Pairwise Comparison for Averages
print("\nPairwise T-tests (Average Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Average"],
            summary_stats[dataset_labels[j]]["Average"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")

# Ensure all datasets have the same number of rows
num_subjects = videodata.shape[0]
if any(dataset.shape[0] != num_subjects for dataset in [videodata1, videodata2, videodata3]):
    raise ValueError("All video datasets must have the same number of subjects (rows).")

# Calculate the sum for each subject in each video
sums_video1 = np.sum(videodata, axis=1)
sums_video2 = np.sum(videodata1, axis=1)
sums_video3 = np.sum(videodata2, axis=1)
sums_video4 = np.sum(videodata3, axis=1)

# Create a DataFrame to store the results
subjects = [f"Subject_{i+1}" for i in range(num_subjects)]  # Generate subject labels

sum_data = pd.DataFrame({
    "Subject": subjects,
    "Video1_Sum": sums_video1,
    "Video2_Sum": sums_video2,
    "Video3_Sum": sums_video3,
    "Video4_Sum": sums_video4
})

# Save the data to a CSV file
output_file = "subject_sums.csv"
sum_data.to_csv(output_file, index=False)

# Calculate correlation matrices using different methods
correlation_matrix_pearson = sum_data.drop("Subject", axis=1).corr(method="pearson")
correlation_matrix_spearman = sum_data.drop("Subject", axis=1).corr(method="spearman")
correlation_matrix_kendall = sum_data.drop("Subject", axis=1).corr(method="kendall")

# Print correlation matrices
print("Pearson Correlation Matrix:")
print(correlation_matrix_pearson)
print("\nSpearman Correlation Matrix:")
print(correlation_matrix_spearman)
print("\nKendall Correlation Matrix:")
print(correlation_matrix_kendall)

# Visualize the correlation matrices
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_pearson, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Pearson Correlation Matrix Between Videos (Based on Sums)")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_spearman, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Spearman Correlation Matrix Between Videos (Based on Sums)")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_kendall, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Kendall Correlation Matrix Between Videos (Based on Sums)")
plt.show()

# Determine the best model (pair with the highest correlation in Pearson)
best_pair_pearson = None
max_correlation_pearson = -1

for i, col1 in enumerate(correlation_matrix_pearson.columns):
    for j, col2 in enumerate(correlation_matrix_pearson.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_pearson.loc[col1, col2]
            if correlation > max_correlation_pearson:
                max_correlation_pearson = correlation
                best_pair_pearson = (col1, col2)

print(f"Best Correlated Pair (Pearson): {best_pair_pearson} with Correlation: {max_correlation_pearson:.2f}")

# Determine the best model (pair with the highest correlation in Spearman)
best_pair_spearman = None
max_correlation_spearman = -1

for i, col1 in enumerate(correlation_matrix_spearman.columns):
    for j, col2 in enumerate(correlation_matrix_spearman.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_spearman.loc[col1, col2]
            if correlation > max_correlation_spearman:
                max_correlation_spearman = correlation
                best_pair_spearman = (col1, col2)

print(f"Best Correlated Pair (Spearman): {best_pair_spearman} with Correlation: {max_correlation_spearman:.2f}")

# Determine the best model (pair with the highest correlation in Kendall)
best_pair_kendall = None
max_correlation_kendall = -1

for i, col1 in enumerate(correlation_matrix_kendall.columns):
    for j, col2 in enumerate(correlation_matrix_kendall.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_kendall.loc[col1, col2]
            if correlation > max_correlation_kendall:
                max_correlation_kendall = correlation
                best_pair_kendall = (col1, col2)

print(f"Best Correlated Pair (Kendall): {best_pair_kendall} with Correlation: {max_correlation_kendall:.2f}")

print(f"Sum for each subject saved to {output_file}")

"""NDARLB017MBJ"""

videodata= np.load("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARLB017MBJ/evaluation/baseline_eLORETA.npy")  #base relax
videodata1= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARLB017MBJ/evaluation/video1_eLORETA.npy") #marowa's drive video1
videodata2= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARLB017MBJ/evaluation/video2_eLORETA.npy") #video2
videodata3= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARLB017MBJ/evaluation/video3_eLORETA.npy") #video3
#luca's drive
#pathLuca = "/content/drive/MyDrive/TU Dublin-NN/research/PHD CANDIDATES - AICL research LAB/CURRENT/Carlos Gomez Tapia - PB04482/dataset - full source activation space (31000x11250) for 35 subjects/"
#fileName = "video1_eLORETA.npy"
#videodata= np.load (pathLuca + fileName)
print(videodata1.shape)
print(len(videodata)) #len of points in brain (31553)
print(videodata[0])
print(len(videodata[0]))  #timestamps  (11250=125hz x 90 seconds)
print(type(videodata))
#video2data is a list of list (first list 31553 lenghth,each of this cell contains 11250 point,31553 is represented the number of the brain and the 11250 is the point in time  )
import numpy as np
import pandas as pd
from scipy.stats import f_oneway, ttest_ind
import matplotlib.pyplot as plt


datasets = [videodata, videodata1, videodata2, videodata3]
dataset_labels = ["videodata", "videodata1", "videodata2", "videodata3"]

# Step 2: Calculate Mean, SD, Sum, and Average for each column
summary_stats = {}
for i, dataset in enumerate(datasets):
    means = np.mean(dataset, axis=0)
    sds = np.std(dataset, axis=0)
    sums = np.sum(dataset, axis=0)
    averages = np.mean(dataset, axis=1)

    summary_stats[dataset_labels[i]] = {
        "Mean": means,
        "SD": sds,
        "Sum": sums,
        "Average": averages
    }

# Step 3: Visualize the Mean and Sum for each dataset
for label, stats in summary_stats.items():
    # Mean Visualization
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Mean"])), stats["Mean"], color='skyblue', label="Mean")
    plt.xlabel('Columns')
    plt.ylabel('Mean Values')
    plt.title(f'Mean for {label}')
    plt.legend()
    plt.show()

    # Sum Visualization
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Sum"])), stats["Sum"], color='orange', label="Sum")
    plt.xlabel('Columns')
    plt.ylabel('Sum Values')
    plt.title(f'Sum for {label}')
    plt.legend()
    plt.show()

    # Average Visualization (per subject)
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Average"])), stats["Average"], color='green', label="Average per Subject")
    plt.xlabel('Subjects')
    plt.ylabel('Average Values')
    plt.title(f'Average per Subject in {label}')
    plt.legend()
    plt.show()
# Step 4: Compare Mean and Sum between subjects in each dataset
for label, stats in summary_stats.items():
    # Mean comparison
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Average"])), stats["Average"], color='green', label="Average per Subject")
    plt.xlabel('Subjects')
    plt.ylabel('Average Values')
    plt.title(f'Average in {label}')
    plt.legend()
    plt.show()

# Step 5: Statistical Comparison between Datasets
print("ANOVA Results (Mean Comparison):")
mean_anova_results = f_oneway(
    summary_stats["videodata"]["Mean"],
    summary_stats["videodata1"]["Mean"],
    summary_stats["videodata2"]["Mean"],
    summary_stats["videodata3"]["Mean"]
)
print(f"F-statistic: {mean_anova_results.statistic:.4f}, P-value: {mean_anova_results.pvalue:.4f}")

print("\nANOVA Results (Sum Comparison):")
sum_anova_results = f_oneway(
    summary_stats["videodata"]["Sum"],
    summary_stats["videodata1"]["Sum"],
    summary_stats["videodata2"]["Sum"],
    summary_stats["videodata3"]["Sum"]
)
print(f"F-statistic: {sum_anova_results.statistic:.4f}, P-value: {sum_anova_results.pvalue:.4f}")

# Pairwise Comparison for Means and Sums
print("\nPairwise T-tests (Mean Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Mean"],
            summary_stats[dataset_labels[j]]["Mean"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")

print("\nANOVA Results (Average Comparison):")
average_anova_results = f_oneway(
    summary_stats["videodata"]["Average"],
    summary_stats["videodata1"]["Average"],
    summary_stats["videodata2"]["Average"],
    summary_stats["videodata3"]["Average"]
)
print(f"F-statistic: {average_anova_results.statistic:.4f}, P-value: {average_anova_results.pvalue:.4f}")

print("\nPairwise T-tests (Sum Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Sum"],
            summary_stats[dataset_labels[j]]["Sum"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")
# Pairwise Comparison for Averages
print("\nPairwise T-tests (Average Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Average"],
            summary_stats[dataset_labels[j]]["Average"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")

# Ensure all datasets have the same number of rows
num_subjects = videodata.shape[0]
if any(dataset.shape[0] != num_subjects for dataset in [videodata1, videodata2, videodata3]):
    raise ValueError("All video datasets must have the same number of subjects (rows).")

# Calculate the sum for each subject in each video
sums_video1 = np.sum(videodata, axis=1)
sums_video2 = np.sum(videodata1, axis=1)
sums_video3 = np.sum(videodata2, axis=1)
sums_video4 = np.sum(videodata3, axis=1)

# Create a DataFrame to store the results
subjects = [f"Subject_{i+1}" for i in range(num_subjects)]  # Generate subject labels

sum_data = pd.DataFrame({
    "Subject": subjects,
    "Video1_Sum": sums_video1,
    "Video2_Sum": sums_video2,
    "Video3_Sum": sums_video3,
    "Video4_Sum": sums_video4
})

# Save the data to a CSV file
output_file = "subject_sums.csv"
sum_data.to_csv(output_file, index=False)

# Calculate correlation matrices using different methods
correlation_matrix_pearson = sum_data.drop("Subject", axis=1).corr(method="pearson")
correlation_matrix_spearman = sum_data.drop("Subject", axis=1).corr(method="spearman")
correlation_matrix_kendall = sum_data.drop("Subject", axis=1).corr(method="kendall")

# Print correlation matrices
print("Pearson Correlation Matrix:")
print(correlation_matrix_pearson)
print("\nSpearman Correlation Matrix:")
print(correlation_matrix_spearman)
print("\nKendall Correlation Matrix:")
print(correlation_matrix_kendall)

# Visualize the correlation matrices
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_pearson, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Pearson Correlation Matrix Between Videos (Based on Sums)")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_spearman, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Spearman Correlation Matrix Between Videos (Based on Sums)")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_kendall, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Kendall Correlation Matrix Between Videos (Based on Sums)")
plt.show()

# Determine the best model (pair with the highest correlation in Pearson)
best_pair_pearson = None
max_correlation_pearson = -1

for i, col1 in enumerate(correlation_matrix_pearson.columns):
    for j, col2 in enumerate(correlation_matrix_pearson.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_pearson.loc[col1, col2]
            if correlation > max_correlation_pearson:
                max_correlation_pearson = correlation
                best_pair_pearson = (col1, col2)

print(f"Best Correlated Pair (Pearson): {best_pair_pearson} with Correlation: {max_correlation_pearson:.2f}")

# Determine the best model (pair with the highest correlation in Spearman)
best_pair_spearman = None
max_correlation_spearman = -1

for i, col1 in enumerate(correlation_matrix_spearman.columns):
    for j, col2 in enumerate(correlation_matrix_spearman.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_spearman.loc[col1, col2]
            if correlation > max_correlation_spearman:
                max_correlation_spearman = correlation
                best_pair_spearman = (col1, col2)

print(f"Best Correlated Pair (Spearman): {best_pair_spearman} with Correlation: {max_correlation_spearman:.2f}")

# Determine the best model (pair with the highest correlation in Kendall)
best_pair_kendall = None
max_correlation_kendall = -1

for i, col1 in enumerate(correlation_matrix_kendall.columns):
    for j, col2 in enumerate(correlation_matrix_kendall.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_kendall.loc[col1, col2]
            if correlation > max_correlation_kendall:
                max_correlation_kendall = correlation
                best_pair_kendall = (col1, col2)

print(f"Best Correlated Pair (Kendall): {best_pair_kendall} with Correlation: {max_correlation_kendall:.2f}")

print(f"Sum for each subject saved to {output_file}")

"""NDARML926NEG"""

videodata= np.load("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARML926NEG/evaluation/baseline_eLORETA.npy")  #base relax
videodata1= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARML926NEG/evaluation/video1_eLORETA.npy") #marowa's drive video1
videodata2= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARML926NEG/evaluation/video2_eLORETA.npy") #video2
videodata3= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARML926NEG/evaluation/video3_eLORETA.npy") #video3
#luca's drive
#pathLuca = "/content/drive/MyDrive/TU Dublin-NN/research/PHD CANDIDATES - AICL research LAB/CURRENT/Carlos Gomez Tapia - PB04482/dataset - full source activation space (31000x11250) for 35 subjects/"
#fileName = "video1_eLORETA.npy"
#videodata= np.load (pathLuca + fileName)
print(videodata1.shape)
print(len(videodata)) #len of points in brain (31553)
print(videodata[0])
print(len(videodata[0]))  #timestamps  (11250=125hz x 90 seconds)
print(type(videodata))
#video2data is a list of list (first list 31553 lenghth,each of this cell contains 11250 point,31553 is represented the number of the brain and the 11250 is the point in time  )
import numpy as np
import pandas as pd
from scipy.stats import f_oneway, ttest_ind
import matplotlib.pyplot as plt


datasets = [videodata, videodata1, videodata2, videodata3]
dataset_labels = ["videodata", "videodata1", "videodata2", "videodata3"]

# Step 2: Calculate Mean, SD, Sum, and Average for each column
summary_stats = {}
for i, dataset in enumerate(datasets):
    means = np.mean(dataset, axis=0)
    sds = np.std(dataset, axis=0)
    sums = np.sum(dataset, axis=0)
    averages = np.mean(dataset, axis=1)

    summary_stats[dataset_labels[i]] = {
        "Mean": means,
        "SD": sds,
        "Sum": sums,
        "Average": averages
    }

# Step 3: Visualize the Mean and Sum for each dataset
for label, stats in summary_stats.items():
    # Mean Visualization
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Mean"])), stats["Mean"], color='skyblue', label="Mean")
    plt.xlabel('Columns')
    plt.ylabel('Mean Values')
    plt.title(f'Mean for {label}')
    plt.legend()
    plt.show()

    # Sum Visualization
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Sum"])), stats["Sum"], color='orange', label="Sum")
    plt.xlabel('Columns')
    plt.ylabel('Sum Values')
    plt.title(f'Sum for {label}')
    plt.legend()
    plt.show()

    # Average Visualization (per subject)
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Average"])), stats["Average"], color='green', label="Average per Subject")
    plt.xlabel('Subjects')
    plt.ylabel('Average Values')
    plt.title(f'Average per Subject in {label}')
    plt.legend()
    plt.show()
# Step 4: Compare Mean and Sum between subjects in each dataset
for label, stats in summary_stats.items():
    # Mean comparison
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Average"])), stats["Average"], color='green', label="Average per Subject")
    plt.xlabel('Subjects')
    plt.ylabel('Average Values')
    plt.title(f'Average in {label}')
    plt.legend()
    plt.show()

# Step 5: Statistical Comparison between Datasets
print("ANOVA Results (Mean Comparison):")
mean_anova_results = f_oneway(
    summary_stats["videodata"]["Mean"],
    summary_stats["videodata1"]["Mean"],
    summary_stats["videodata2"]["Mean"],
    summary_stats["videodata3"]["Mean"]
)
print(f"F-statistic: {mean_anova_results.statistic:.4f}, P-value: {mean_anova_results.pvalue:.4f}")

print("\nANOVA Results (Sum Comparison):")
sum_anova_results = f_oneway(
    summary_stats["videodata"]["Sum"],
    summary_stats["videodata1"]["Sum"],
    summary_stats["videodata2"]["Sum"],
    summary_stats["videodata3"]["Sum"]
)
print(f"F-statistic: {sum_anova_results.statistic:.4f}, P-value: {sum_anova_results.pvalue:.4f}")

# Pairwise Comparison for Means and Sums
print("\nPairwise T-tests (Mean Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Mean"],
            summary_stats[dataset_labels[j]]["Mean"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")

print("\nANOVA Results (Average Comparison):")
average_anova_results = f_oneway(
    summary_stats["videodata"]["Average"],
    summary_stats["videodata1"]["Average"],
    summary_stats["videodata2"]["Average"],
    summary_stats["videodata3"]["Average"]
)
print(f"F-statistic: {average_anova_results.statistic:.4f}, P-value: {average_anova_results.pvalue:.4f}")

print("\nPairwise T-tests (Sum Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Sum"],
            summary_stats[dataset_labels[j]]["Sum"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")
# Pairwise Comparison for Averages
print("\nPairwise T-tests (Average Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Average"],
            summary_stats[dataset_labels[j]]["Average"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")

# Ensure all datasets have the same number of rows
num_subjects = videodata.shape[0]
if any(dataset.shape[0] != num_subjects for dataset in [videodata1, videodata2, videodata3]):
    raise ValueError("All video datasets must have the same number of subjects (rows).")

# Calculate the sum for each subject in each video
sums_video1 = np.sum(videodata, axis=1)
sums_video2 = np.sum(videodata1, axis=1)
sums_video3 = np.sum(videodata2, axis=1)
sums_video4 = np.sum(videodata3, axis=1)

# Create a DataFrame to store the results
subjects = [f"Subject_{i+1}" for i in range(num_subjects)]  # Generate subject labels

sum_data = pd.DataFrame({
    "Subject": subjects,
    "Video1_Sum": sums_video1,
    "Video2_Sum": sums_video2,
    "Video3_Sum": sums_video3,
    "Video4_Sum": sums_video4
})

# Save the data to a CSV file
output_file = "subject_sums.csv"
sum_data.to_csv(output_file, index=False)

# Calculate correlation matrices using different methods
correlation_matrix_pearson = sum_data.drop("Subject", axis=1).corr(method="pearson")
correlation_matrix_spearman = sum_data.drop("Subject", axis=1).corr(method="spearman")
correlation_matrix_kendall = sum_data.drop("Subject", axis=1).corr(method="kendall")

# Print correlation matrices
print("Pearson Correlation Matrix:")
print(correlation_matrix_pearson)
print("\nSpearman Correlation Matrix:")
print(correlation_matrix_spearman)
print("\nKendall Correlation Matrix:")
print(correlation_matrix_kendall)

# Visualize the correlation matrices
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_pearson, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Pearson Correlation Matrix Between Videos (Based on Sums)")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_spearman, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Spearman Correlation Matrix Between Videos (Based on Sums)")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_kendall, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Kendall Correlation Matrix Between Videos (Based on Sums)")
plt.show()

# Determine the best model (pair with the highest correlation in Pearson)
best_pair_pearson = None
max_correlation_pearson = -1

for i, col1 in enumerate(correlation_matrix_pearson.columns):
    for j, col2 in enumerate(correlation_matrix_pearson.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_pearson.loc[col1, col2]
            if correlation > max_correlation_pearson:
                max_correlation_pearson = correlation
                best_pair_pearson = (col1, col2)

print(f"Best Correlated Pair (Pearson): {best_pair_pearson} with Correlation: {max_correlation_pearson:.2f}")

# Determine the best model (pair with the highest correlation in Spearman)
best_pair_spearman = None
max_correlation_spearman = -1

for i, col1 in enumerate(correlation_matrix_spearman.columns):
    for j, col2 in enumerate(correlation_matrix_spearman.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_spearman.loc[col1, col2]
            if correlation > max_correlation_spearman:
                max_correlation_spearman = correlation
                best_pair_spearman = (col1, col2)

print(f"Best Correlated Pair (Spearman): {best_pair_spearman} with Correlation: {max_correlation_spearman:.2f}")

# Determine the best model (pair with the highest correlation in Kendall)
best_pair_kendall = None
max_correlation_kendall = -1

for i, col1 in enumerate(correlation_matrix_kendall.columns):
    for j, col2 in enumerate(correlation_matrix_kendall.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_kendall.loc[col1, col2]
            if correlation > max_correlation_kendall:
                max_correlation_kendall = correlation
                best_pair_kendall = (col1, col2)

print(f"Best Correlated Pair (Kendall): {best_pair_kendall} with Correlation: {max_correlation_kendall:.2f}")

print(f"Sum for each subject saved to {output_file}")

"""NDARMR242UKQ"""

videodata= np.load("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARMR242UKQ/evaluation/baseline_eLORETA.npy")  #base relax
videodata1= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARMR242UKQ/evaluation/video1_eLORETA.npy") #marowa's drive video1
videodata2= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARMR242UKQ/evaluation/video2_eLORETA.npy") #video2
videodata3= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARMR242UKQ/evaluation/video3_eLORETA.npy") #video3
#luca's drive
#pathLuca = "/content/drive/MyDrive/TU Dublin-NN/research/PHD CANDIDATES - AICL research LAB/CURRENT/Carlos Gomez Tapia - PB04482/dataset - full source activation space (31000x11250) for 35 subjects/"
#fileName = "video1_eLORETA.npy"
#videodata= np.load (pathLuca + fileName)
print(videodata1.shape)
print(len(videodata)) #len of points in brain (31553)
print(videodata[0])
print(len(videodata[0]))  #timestamps  (11250=125hz x 90 seconds)
print(type(videodata))
#video2data is a list of list (first list 31553 lenghth,each of this cell contains 11250 point,31553 is represented the number of the brain and the 11250 is the point in time  )
import numpy as np
import pandas as pd
from scipy.stats import f_oneway, ttest_ind
import matplotlib.pyplot as plt


datasets = [videodata, videodata1, videodata2, videodata3]
dataset_labels = ["videodata", "videodata1", "videodata2", "videodata3"]

# Step 2: Calculate Mean, SD, Sum, and Average for each column
summary_stats = {}
for i, dataset in enumerate(datasets):
    means = np.mean(dataset, axis=0)
    sds = np.std(dataset, axis=0)
    sums = np.sum(dataset, axis=0)
    averages = np.mean(dataset, axis=1)

    summary_stats[dataset_labels[i]] = {
        "Mean": means,
        "SD": sds,
        "Sum": sums,
        "Average": averages
    }

# Step 3: Visualize the Mean and Sum for each dataset
for label, stats in summary_stats.items():
    # Mean Visualization
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Mean"])), stats["Mean"], color='skyblue', label="Mean")
    plt.xlabel('Columns')
    plt.ylabel('Mean Values')
    plt.title(f'Mean for {label}')
    plt.legend()
    plt.show()

    # Sum Visualization
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Sum"])), stats["Sum"], color='orange', label="Sum")
    plt.xlabel('Columns')
    plt.ylabel('Sum Values')
    plt.title(f'Sum for {label}')
    plt.legend()
    plt.show()

    # Average Visualization (per subject)
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Average"])), stats["Average"], color='green', label="Average per Subject")
    plt.xlabel('Subjects')
    plt.ylabel('Average Values')
    plt.title(f'Average per Subject in {label}')
    plt.legend()
    plt.show()
# Step 4: Compare Mean and Sum between subjects in each dataset
for label, stats in summary_stats.items():
    # Mean comparison
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Average"])), stats["Average"], color='green', label="Average per Subject")
    plt.xlabel('Subjects')
    plt.ylabel('Average Values')
    plt.title(f'Average in {label}')
    plt.legend()
    plt.show()

# Step 5: Statistical Comparison between Datasets
print("ANOVA Results (Mean Comparison):")
mean_anova_results = f_oneway(
    summary_stats["videodata"]["Mean"],
    summary_stats["videodata1"]["Mean"],
    summary_stats["videodata2"]["Mean"],
    summary_stats["videodata3"]["Mean"]
)
print(f"F-statistic: {mean_anova_results.statistic:.4f}, P-value: {mean_anova_results.pvalue:.4f}")

print("\nANOVA Results (Sum Comparison):")
sum_anova_results = f_oneway(
    summary_stats["videodata"]["Sum"],
    summary_stats["videodata1"]["Sum"],
    summary_stats["videodata2"]["Sum"],
    summary_stats["videodata3"]["Sum"]
)
print(f"F-statistic: {sum_anova_results.statistic:.4f}, P-value: {sum_anova_results.pvalue:.4f}")

# Pairwise Comparison for Means and Sums
print("\nPairwise T-tests (Mean Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Mean"],
            summary_stats[dataset_labels[j]]["Mean"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")

print("\nANOVA Results (Average Comparison):")
average_anova_results = f_oneway(
    summary_stats["videodata"]["Average"],
    summary_stats["videodata1"]["Average"],
    summary_stats["videodata2"]["Average"],
    summary_stats["videodata3"]["Average"]
)
print(f"F-statistic: {average_anova_results.statistic:.4f}, P-value: {average_anova_results.pvalue:.4f}")

print("\nPairwise T-tests (Sum Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Sum"],
            summary_stats[dataset_labels[j]]["Sum"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")
# Pairwise Comparison for Averages
print("\nPairwise T-tests (Average Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Average"],
            summary_stats[dataset_labels[j]]["Average"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")

# Ensure all datasets have the same number of rows
num_subjects = videodata.shape[0]
if any(dataset.shape[0] != num_subjects for dataset in [videodata1, videodata2, videodata3]):
    raise ValueError("All video datasets must have the same number of subjects (rows).")

# Calculate the sum for each subject in each video
sums_video1 = np.sum(videodata, axis=1)
sums_video2 = np.sum(videodata1, axis=1)
sums_video3 = np.sum(videodata2, axis=1)
sums_video4 = np.sum(videodata3, axis=1)

# Create a DataFrame to store the results
subjects = [f"Subject_{i+1}" for i in range(num_subjects)]  # Generate subject labels

sum_data = pd.DataFrame({
    "Subject": subjects,
    "Video1_Sum": sums_video1,
    "Video2_Sum": sums_video2,
    "Video3_Sum": sums_video3,
    "Video4_Sum": sums_video4
})

# Save the data to a CSV file
output_file = "subject_sums.csv"
sum_data.to_csv(output_file, index=False)

# Calculate correlation matrices using different methods
correlation_matrix_pearson = sum_data.drop("Subject", axis=1).corr(method="pearson")
correlation_matrix_spearman = sum_data.drop("Subject", axis=1).corr(method="spearman")
correlation_matrix_kendall = sum_data.drop("Subject", axis=1).corr(method="kendall")

# Print correlation matrices
print("Pearson Correlation Matrix:")
print(correlation_matrix_pearson)
print("\nSpearman Correlation Matrix:")
print(correlation_matrix_spearman)
print("\nKendall Correlation Matrix:")
print(correlation_matrix_kendall)

# Visualize the correlation matrices
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_pearson, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Pearson Correlation Matrix Between Videos (Based on Sums)")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_spearman, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Spearman Correlation Matrix Between Videos (Based on Sums)")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_kendall, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Kendall Correlation Matrix Between Videos (Based on Sums)")
plt.show()

# Determine the best model (pair with the highest correlation in Pearson)
best_pair_pearson = None
max_correlation_pearson = -1

for i, col1 in enumerate(correlation_matrix_pearson.columns):
    for j, col2 in enumerate(correlation_matrix_pearson.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_pearson.loc[col1, col2]
            if correlation > max_correlation_pearson:
                max_correlation_pearson = correlation
                best_pair_pearson = (col1, col2)

print(f"Best Correlated Pair (Pearson): {best_pair_pearson} with Correlation: {max_correlation_pearson:.2f}")

# Determine the best model (pair with the highest correlation in Spearman)
best_pair_spearman = None
max_correlation_spearman = -1

for i, col1 in enumerate(correlation_matrix_spearman.columns):
    for j, col2 in enumerate(correlation_matrix_spearman.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_spearman.loc[col1, col2]
            if correlation > max_correlation_spearman:
                max_correlation_spearman = correlation
                best_pair_spearman = (col1, col2)

print(f"Best Correlated Pair (Spearman): {best_pair_spearman} with Correlation: {max_correlation_spearman:.2f}")

# Determine the best model (pair with the highest correlation in Kendall)
best_pair_kendall = None
max_correlation_kendall = -1

for i, col1 in enumerate(correlation_matrix_kendall.columns):
    for j, col2 in enumerate(correlation_matrix_kendall.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_kendall.loc[col1, col2]
            if correlation > max_correlation_kendall:
                max_correlation_kendall = correlation
                best_pair_kendall = (col1, col2)

print(f"Best Correlated Pair (Kendall): {best_pair_kendall} with Correlation: {max_correlation_kendall:.2f}")

print(f"Sum for each subject saved to {output_file}")

"""NDARPN886HH9"""

videodata= np.load("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARPN886HH9/evaluation/baseline_eLORETA.npy")  #base relax
videodata1= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARPN886HH9/evaluation/video1_eLORETA.npy") #marowa's drive video1
videodata2= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARPN886HH9/evaluation/video2_eLORETA.npy") #video2
videodata3= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARPN886HH9/evaluation/video3_eLORETA.npy") #video3
#luca's drive
#pathLuca = "/content/drive/MyDrive/TU Dublin-NN/research/PHD CANDIDATES - AICL research LAB/CURRENT/Carlos Gomez Tapia - PB04482/dataset - full source activation space (31000x11250) for 35 subjects/"
#fileName = "video1_eLORETA.npy"
#videodata= np.load (pathLuca + fileName)
print(videodata1.shape)
print(len(videodata)) #len of points in brain (31553)
print(videodata[0])
print(len(videodata[0]))  #timestamps  (11250=125hz x 90 seconds)
print(type(videodata))
#video2data is a list of list (first list 31553 lenghth,each of this cell contains 11250 point,31553 is represented the number of the brain and the 11250 is the point in time  )
import numpy as np
import pandas as pd
from scipy.stats import f_oneway, ttest_ind
import matplotlib.pyplot as plt


datasets = [videodata, videodata1, videodata2, videodata3]
dataset_labels = ["videodata", "videodata1", "videodata2", "videodata3"]

# Step 2: Calculate Mean, SD, Sum, and Average for each column
summary_stats = {}
for i, dataset in enumerate(datasets):
    means = np.mean(dataset, axis=0)
    sds = np.std(dataset, axis=0)
    sums = np.sum(dataset, axis=0)
    averages = np.mean(dataset, axis=1)

    summary_stats[dataset_labels[i]] = {
        "Mean": means,
        "SD": sds,
        "Sum": sums,
        "Average": averages
    }

# Step 3: Visualize the Mean and Sum for each dataset
for label, stats in summary_stats.items():
    # Mean Visualization
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Mean"])), stats["Mean"], color='skyblue', label="Mean")
    plt.xlabel('Columns')
    plt.ylabel('Mean Values')
    plt.title(f'Mean for {label}')
    plt.legend()
    plt.show()

    # Sum Visualization
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Sum"])), stats["Sum"], color='orange', label="Sum")
    plt.xlabel('Columns')
    plt.ylabel('Sum Values')
    plt.title(f'Sum for {label}')
    plt.legend()
    plt.show()

    # Average Visualization (per subject)
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Average"])), stats["Average"], color='green', label="Average per Subject")
    plt.xlabel('Subjects')
    plt.ylabel('Average Values')
    plt.title(f'Average per Subject in {label}')
    plt.legend()
    plt.show()
# Step 4: Compare Mean and Sum between subjects in each dataset
for label, stats in summary_stats.items():
    # Mean comparison
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Average"])), stats["Average"], color='green', label="Average per Subject")
    plt.xlabel('Subjects')
    plt.ylabel('Average Values')
    plt.title(f'Average in {label}')
    plt.legend()
    plt.show()

# Step 5: Statistical Comparison between Datasets
print("ANOVA Results (Mean Comparison):")
mean_anova_results = f_oneway(
    summary_stats["videodata"]["Mean"],
    summary_stats["videodata1"]["Mean"],
    summary_stats["videodata2"]["Mean"],
    summary_stats["videodata3"]["Mean"]
)
print(f"F-statistic: {mean_anova_results.statistic:.4f}, P-value: {mean_anova_results.pvalue:.4f}")

print("\nANOVA Results (Sum Comparison):")
sum_anova_results = f_oneway(
    summary_stats["videodata"]["Sum"],
    summary_stats["videodata1"]["Sum"],
    summary_stats["videodata2"]["Sum"],
    summary_stats["videodata3"]["Sum"]
)
print(f"F-statistic: {sum_anova_results.statistic:.4f}, P-value: {sum_anova_results.pvalue:.4f}")

# Pairwise Comparison for Means and Sums
print("\nPairwise T-tests (Mean Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Mean"],
            summary_stats[dataset_labels[j]]["Mean"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")

print("\nANOVA Results (Average Comparison):")
average_anova_results = f_oneway(
    summary_stats["videodata"]["Average"],
    summary_stats["videodata1"]["Average"],
    summary_stats["videodata2"]["Average"],
    summary_stats["videodata3"]["Average"]
)
print(f"F-statistic: {average_anova_results.statistic:.4f}, P-value: {average_anova_results.pvalue:.4f}")

print("\nPairwise T-tests (Sum Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Sum"],
            summary_stats[dataset_labels[j]]["Sum"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")
# Pairwise Comparison for Averages
print("\nPairwise T-tests (Average Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Average"],
            summary_stats[dataset_labels[j]]["Average"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")

# Ensure all datasets have the same number of rows
num_subjects = videodata.shape[0]
if any(dataset.shape[0] != num_subjects for dataset in [videodata1, videodata2, videodata3]):
    raise ValueError("All video datasets must have the same number of subjects (rows).")

# Calculate the sum for each subject in each video
sums_video1 = np.sum(videodata, axis=1)
sums_video2 = np.sum(videodata1, axis=1)
sums_video3 = np.sum(videodata2, axis=1)
sums_video4 = np.sum(videodata3, axis=1)

# Create a DataFrame to store the results
subjects = [f"Subject_{i+1}" for i in range(num_subjects)]  # Generate subject labels

sum_data = pd.DataFrame({
    "Subject": subjects,
    "Video1_Sum": sums_video1,
    "Video2_Sum": sums_video2,
    "Video3_Sum": sums_video3,
    "Video4_Sum": sums_video4
})

# Save the data to a CSV file
output_file = "subject_sums.csv"
sum_data.to_csv(output_file, index=False)

# Calculate correlation matrices using different methods
correlation_matrix_pearson = sum_data.drop("Subject", axis=1).corr(method="pearson")
correlation_matrix_spearman = sum_data.drop("Subject", axis=1).corr(method="spearman")
correlation_matrix_kendall = sum_data.drop("Subject", axis=1).corr(method="kendall")

# Print correlation matrices
print("Pearson Correlation Matrix:")
print(correlation_matrix_pearson)
print("\nSpearman Correlation Matrix:")
print(correlation_matrix_spearman)
print("\nKendall Correlation Matrix:")
print(correlation_matrix_kendall)

# Visualize the correlation matrices
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_pearson, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Pearson Correlation Matrix Between Videos (Based on Sums)")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_spearman, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Spearman Correlation Matrix Between Videos (Based on Sums)")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_kendall, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Kendall Correlation Matrix Between Videos (Based on Sums)")
plt.show()

# Determine the best model (pair with the highest correlation in Pearson)
best_pair_pearson = None
max_correlation_pearson = -1

for i, col1 in enumerate(correlation_matrix_pearson.columns):
    for j, col2 in enumerate(correlation_matrix_pearson.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_pearson.loc[col1, col2]
            if correlation > max_correlation_pearson:
                max_correlation_pearson = correlation
                best_pair_pearson = (col1, col2)

print(f"Best Correlated Pair (Pearson): {best_pair_pearson} with Correlation: {max_correlation_pearson:.2f}")

# Determine the best model (pair with the highest correlation in Spearman)
best_pair_spearman = None
max_correlation_spearman = -1

for i, col1 in enumerate(correlation_matrix_spearman.columns):
    for j, col2 in enumerate(correlation_matrix_spearman.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_spearman.loc[col1, col2]
            if correlation > max_correlation_spearman:
                max_correlation_spearman = correlation
                best_pair_spearman = (col1, col2)

print(f"Best Correlated Pair (Spearman): {best_pair_spearman} with Correlation: {max_correlation_spearman:.2f}")

# Determine the best model (pair with the highest correlation in Kendall)
best_pair_kendall = None
max_correlation_kendall = -1

for i, col1 in enumerate(correlation_matrix_kendall.columns):
    for j, col2 in enumerate(correlation_matrix_kendall.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_kendall.loc[col1, col2]
            if correlation > max_correlation_kendall:
                max_correlation_kendall = correlation
                best_pair_kendall = (col1, col2)

print(f"Best Correlated Pair (Kendall): {best_pair_kendall} with Correlation: {max_correlation_kendall:.2f}")

print(f"Sum for each subject saved to {output_file}")

"""NDARPY478YM0"""

videodata= np.load("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARPY478YM0/evaluation/baseline_eLORETA.npy")  #base relax
videodata1= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARPY478YM0/evaluation/video1_eLORETA.npy") #marowa's drive video1
videodata2= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARPY478YM0/evaluation/video2_eLORETA.npy") #video2
videodata3= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARPY478YM0/evaluation/video3_eLORETA.npy") #video3
#luca's drive
#pathLuca = "/content/drive/MyDrive/TU Dublin-NN/research/PHD CANDIDATES - AICL research LAB/CURRENT/Carlos Gomez Tapia - PB04482/dataset - full source activation space (31000x11250) for 35 subjects/"
#fileName = "video1_eLORETA.npy"
#videodata= np.load (pathLuca + fileName)
print(videodata1.shape)
print(len(videodata)) #len of points in brain (31553)
print(videodata[0])
print(len(videodata[0]))  #timestamps  (11250=125hz x 90 seconds)
print(type(videodata))
#video2data is a list of list (first list 31553 lenghth,each of this cell contains 11250 point,31553 is represented the number of the brain and the 11250 is the point in time  )
import numpy as np
import pandas as pd
from scipy.stats import f_oneway, ttest_ind
import matplotlib.pyplot as plt


datasets = [videodata, videodata1, videodata2, videodata3]
dataset_labels = ["videodata", "videodata1", "videodata2", "videodata3"]

# Step 2: Calculate Mean, SD, Sum, and Average for each column
summary_stats = {}
for i, dataset in enumerate(datasets):
    means = np.mean(dataset, axis=0)
    sds = np.std(dataset, axis=0)
    sums = np.sum(dataset, axis=0)
    averages = np.mean(dataset, axis=1)

    summary_stats[dataset_labels[i]] = {
        "Mean": means,
        "SD": sds,
        "Sum": sums,
        "Average": averages
    }

# Step 3: Visualize the Mean and Sum for each dataset
for label, stats in summary_stats.items():
    # Mean Visualization
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Mean"])), stats["Mean"], color='skyblue', label="Mean")
    plt.xlabel('Columns')
    plt.ylabel('Mean Values')
    plt.title(f'Mean for {label}')
    plt.legend()
    plt.show()

    # Sum Visualization
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Sum"])), stats["Sum"], color='orange', label="Sum")
    plt.xlabel('Columns')
    plt.ylabel('Sum Values')
    plt.title(f'Sum for {label}')
    plt.legend()
    plt.show()

    # Average Visualization (per subject)
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Average"])), stats["Average"], color='green', label="Average per Subject")
    plt.xlabel('Subjects')
    plt.ylabel('Average Values')
    plt.title(f'Average per Subject in {label}')
    plt.legend()
    plt.show()
# Step 4: Compare Mean and Sum between subjects in each dataset
for label, stats in summary_stats.items():
    # Mean comparison
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Average"])), stats["Average"], color='green', label="Average per Subject")
    plt.xlabel('Subjects')
    plt.ylabel('Average Values')
    plt.title(f'Average in {label}')
    plt.legend()
    plt.show()

# Step 5: Statistical Comparison between Datasets
print("ANOVA Results (Mean Comparison):")
mean_anova_results = f_oneway(
    summary_stats["videodata"]["Mean"],
    summary_stats["videodata1"]["Mean"],
    summary_stats["videodata2"]["Mean"],
    summary_stats["videodata3"]["Mean"]
)
print(f"F-statistic: {mean_anova_results.statistic:.4f}, P-value: {mean_anova_results.pvalue:.4f}")

print("\nANOVA Results (Sum Comparison):")
sum_anova_results = f_oneway(
    summary_stats["videodata"]["Sum"],
    summary_stats["videodata1"]["Sum"],
    summary_stats["videodata2"]["Sum"],
    summary_stats["videodata3"]["Sum"]
)
print(f"F-statistic: {sum_anova_results.statistic:.4f}, P-value: {sum_anova_results.pvalue:.4f}")

# Pairwise Comparison for Means and Sums
print("\nPairwise T-tests (Mean Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Mean"],
            summary_stats[dataset_labels[j]]["Mean"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")

print("\nANOVA Results (Average Comparison):")
average_anova_results = f_oneway(
    summary_stats["videodata"]["Average"],
    summary_stats["videodata1"]["Average"],
    summary_stats["videodata2"]["Average"],
    summary_stats["videodata3"]["Average"]
)
print(f"F-statistic: {average_anova_results.statistic:.4f}, P-value: {average_anova_results.pvalue:.4f}")

print("\nPairwise T-tests (Sum Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Sum"],
            summary_stats[dataset_labels[j]]["Sum"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")
# Pairwise Comparison for Averages
print("\nPairwise T-tests (Average Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Average"],
            summary_stats[dataset_labels[j]]["Average"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")

# Ensure all datasets have the same number of rows
num_subjects = videodata.shape[0]
if any(dataset.shape[0] != num_subjects for dataset in [videodata1, videodata2, videodata3]):
    raise ValueError("All video datasets must have the same number of subjects (rows).")

# Calculate the sum for each subject in each video
sums_video1 = np.sum(videodata, axis=1)
sums_video2 = np.sum(videodata1, axis=1)
sums_video3 = np.sum(videodata2, axis=1)
sums_video4 = np.sum(videodata3, axis=1)

# Create a DataFrame to store the results
subjects = [f"Subject_{i+1}" for i in range(num_subjects)]  # Generate subject labels

sum_data = pd.DataFrame({
    "Subject": subjects,
    "Video1_Sum": sums_video1,
    "Video2_Sum": sums_video2,
    "Video3_Sum": sums_video3,
    "Video4_Sum": sums_video4
})

# Save the data to a CSV file
output_file = "subject_sums.csv"
sum_data.to_csv(output_file, index=False)

# Calculate correlation matrices using different methods
correlation_matrix_pearson = sum_data.drop("Subject", axis=1).corr(method="pearson")
correlation_matrix_spearman = sum_data.drop("Subject", axis=1).corr(method="spearman")
correlation_matrix_kendall = sum_data.drop("Subject", axis=1).corr(method="kendall")

# Print correlation matrices
print("Pearson Correlation Matrix:")
print(correlation_matrix_pearson)
print("\nSpearman Correlation Matrix:")
print(correlation_matrix_spearman)
print("\nKendall Correlation Matrix:")
print(correlation_matrix_kendall)

# Visualize the correlation matrices
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_pearson, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Pearson Correlation Matrix Between Videos (Based on Sums)")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_spearman, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Spearman Correlation Matrix Between Videos (Based on Sums)")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_kendall, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Kendall Correlation Matrix Between Videos (Based on Sums)")
plt.show()

# Determine the best model (pair with the highest correlation in Pearson)
best_pair_pearson = None
max_correlation_pearson = -1

for i, col1 in enumerate(correlation_matrix_pearson.columns):
    for j, col2 in enumerate(correlation_matrix_pearson.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_pearson.loc[col1, col2]
            if correlation > max_correlation_pearson:
                max_correlation_pearson = correlation
                best_pair_pearson = (col1, col2)

print(f"Best Correlated Pair (Pearson): {best_pair_pearson} with Correlation: {max_correlation_pearson:.2f}")

# Determine the best model (pair with the highest correlation in Spearman)
best_pair_spearman = None
max_correlation_spearman = -1

for i, col1 in enumerate(correlation_matrix_spearman.columns):
    for j, col2 in enumerate(correlation_matrix_spearman.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_spearman.loc[col1, col2]
            if correlation > max_correlation_spearman:
                max_correlation_spearman = correlation
                best_pair_spearman = (col1, col2)

print(f"Best Correlated Pair (Spearman): {best_pair_spearman} with Correlation: {max_correlation_spearman:.2f}")

# Determine the best model (pair with the highest correlation in Kendall)
best_pair_kendall = None
max_correlation_kendall = -1

for i, col1 in enumerate(correlation_matrix_kendall.columns):
    for j, col2 in enumerate(correlation_matrix_kendall.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_kendall.loc[col1, col2]
            if correlation > max_correlation_kendall:
                max_correlation_kendall = correlation
                best_pair_kendall = (col1, col2)

print(f"Best Correlated Pair (Kendall): {best_pair_kendall} with Correlation: {max_correlation_kendall:.2f}")

print(f"Sum for each subject saved to {output_file}")

"""NDARRA733VWX"""

videodata= np.load("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARRA733VWX/evaluation/baseline_eLORETA.npy")  #base relax
videodata1= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARRA733VWX/evaluation/video1_eLORETA.npy") #marowa's drive video1
videodata2= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARRA733VWX/evaluation/video2_eLORETA.npy") #video2
videodata3= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARRA733VWX/evaluation/video3_eLORETA.npy") #video3
#luca's drive
#pathLuca = "/content/drive/MyDrive/TU Dublin-NN/research/PHD CANDIDATES - AICL research LAB/CURRENT/Carlos Gomez Tapia - PB04482/dataset - full source activation space (31000x11250) for 35 subjects/"
#fileName = "video1_eLORETA.npy"
#videodata= np.load (pathLuca + fileName)
print(videodata1.shape)
print(len(videodata)) #len of points in brain (31553)
print(videodata[0])
print(len(videodata[0]))  #timestamps  (11250=125hz x 90 seconds)
print(type(videodata))
#video2data is a list of list (first list 31553 lenghth,each of this cell contains 11250 point,31553 is represented the number of the brain and the 11250 is the point in time  )
import numpy as np
import pandas as pd
from scipy.stats import f_oneway, ttest_ind
import matplotlib.pyplot as plt


datasets = [videodata, videodata1, videodata2, videodata3]
dataset_labels = ["videodata", "videodata1", "videodata2", "videodata3"]

# Step 2: Calculate Mean, SD, Sum, and Average for each column
summary_stats = {}
for i, dataset in enumerate(datasets):
    means = np.mean(dataset, axis=0)
    sds = np.std(dataset, axis=0)
    sums = np.sum(dataset, axis=0)
    averages = np.mean(dataset, axis=1)

    summary_stats[dataset_labels[i]] = {
        "Mean": means,
        "SD": sds,
        "Sum": sums,
        "Average": averages
    }

# Step 3: Visualize the Mean and Sum for each dataset
for label, stats in summary_stats.items():
    # Mean Visualization
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Mean"])), stats["Mean"], color='skyblue', label="Mean")
    plt.xlabel('Columns')
    plt.ylabel('Mean Values')
    plt.title(f'Mean for {label}')
    plt.legend()
    plt.show()

    # Sum Visualization
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Sum"])), stats["Sum"], color='orange', label="Sum")
    plt.xlabel('Columns')
    plt.ylabel('Sum Values')
    plt.title(f'Sum for {label}')
    plt.legend()
    plt.show()

    # Average Visualization (per subject)
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Average"])), stats["Average"], color='green', label="Average per Subject")
    plt.xlabel('Subjects')
    plt.ylabel('Average Values')
    plt.title(f'Average per Subject in {label}')
    plt.legend()
    plt.show()
# Step 4: Compare Mean and Sum between subjects in each dataset
for label, stats in summary_stats.items():
    # Mean comparison
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Average"])), stats["Average"], color='green', label="Average per Subject")
    plt.xlabel('Subjects')
    plt.ylabel('Average Values')
    plt.title(f'Average in {label}')
    plt.legend()
    plt.show()

# Step 5: Statistical Comparison between Datasets
print("ANOVA Results (Mean Comparison):")
mean_anova_results = f_oneway(
    summary_stats["videodata"]["Mean"],
    summary_stats["videodata1"]["Mean"],
    summary_stats["videodata2"]["Mean"],
    summary_stats["videodata3"]["Mean"]
)
print(f"F-statistic: {mean_anova_results.statistic:.4f}, P-value: {mean_anova_results.pvalue:.4f}")

print("\nANOVA Results (Sum Comparison):")
sum_anova_results = f_oneway(
    summary_stats["videodata"]["Sum"],
    summary_stats["videodata1"]["Sum"],
    summary_stats["videodata2"]["Sum"],
    summary_stats["videodata3"]["Sum"]
)
print(f"F-statistic: {sum_anova_results.statistic:.4f}, P-value: {sum_anova_results.pvalue:.4f}")

# Pairwise Comparison for Means and Sums
print("\nPairwise T-tests (Mean Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Mean"],
            summary_stats[dataset_labels[j]]["Mean"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")

print("\nANOVA Results (Average Comparison):")
average_anova_results = f_oneway(
    summary_stats["videodata"]["Average"],
    summary_stats["videodata1"]["Average"],
    summary_stats["videodata2"]["Average"],
    summary_stats["videodata3"]["Average"]
)
print(f"F-statistic: {average_anova_results.statistic:.4f}, P-value: {average_anova_results.pvalue:.4f}")

print("\nPairwise T-tests (Sum Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Sum"],
            summary_stats[dataset_labels[j]]["Sum"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")
# Pairwise Comparison for Averages
print("\nPairwise T-tests (Average Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Average"],
            summary_stats[dataset_labels[j]]["Average"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")

# Ensure all datasets have the same number of rows
num_subjects = videodata.shape[0]
if any(dataset.shape[0] != num_subjects for dataset in [videodata1, videodata2, videodata3]):
    raise ValueError("All video datasets must have the same number of subjects (rows).")

# Calculate the sum for each subject in each video
sums_video1 = np.sum(videodata, axis=1)
sums_video2 = np.sum(videodata1, axis=1)
sums_video3 = np.sum(videodata2, axis=1)
sums_video4 = np.sum(videodata3, axis=1)

# Create a DataFrame to store the results
subjects = [f"Subject_{i+1}" for i in range(num_subjects)]  # Generate subject labels

sum_data = pd.DataFrame({
    "Subject": subjects,
    "Video1_Sum": sums_video1,
    "Video2_Sum": sums_video2,
    "Video3_Sum": sums_video3,
    "Video4_Sum": sums_video4
})

# Save the data to a CSV file
output_file = "subject_sums.csv"
sum_data.to_csv(output_file, index=False)

# Calculate correlation matrices using different methods
correlation_matrix_pearson = sum_data.drop("Subject", axis=1).corr(method="pearson")
correlation_matrix_spearman = sum_data.drop("Subject", axis=1).corr(method="spearman")
correlation_matrix_kendall = sum_data.drop("Subject", axis=1).corr(method="kendall")

# Print correlation matrices
print("Pearson Correlation Matrix:")
print(correlation_matrix_pearson)
print("\nSpearman Correlation Matrix:")
print(correlation_matrix_spearman)
print("\nKendall Correlation Matrix:")
print(correlation_matrix_kendall)

# Visualize the correlation matrices
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_pearson, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Pearson Correlation Matrix Between Videos (Based on Sums)")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_spearman, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Spearman Correlation Matrix Between Videos (Based on Sums)")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_kendall, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Kendall Correlation Matrix Between Videos (Based on Sums)")
plt.show()

# Determine the best model (pair with the highest correlation in Pearson)
best_pair_pearson = None
max_correlation_pearson = -1

for i, col1 in enumerate(correlation_matrix_pearson.columns):
    for j, col2 in enumerate(correlation_matrix_pearson.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_pearson.loc[col1, col2]
            if correlation > max_correlation_pearson:
                max_correlation_pearson = correlation
                best_pair_pearson = (col1, col2)

print(f"Best Correlated Pair (Pearson): {best_pair_pearson} with Correlation: {max_correlation_pearson:.2f}")

# Determine the best model (pair with the highest correlation in Spearman)
best_pair_spearman = None
max_correlation_spearman = -1

for i, col1 in enumerate(correlation_matrix_spearman.columns):
    for j, col2 in enumerate(correlation_matrix_spearman.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_spearman.loc[col1, col2]
            if correlation > max_correlation_spearman:
                max_correlation_spearman = correlation
                best_pair_spearman = (col1, col2)

print(f"Best Correlated Pair (Spearman): {best_pair_spearman} with Correlation: {max_correlation_spearman:.2f}")

# Determine the best model (pair with the highest correlation in Kendall)
best_pair_kendall = None
max_correlation_kendall = -1

for i, col1 in enumerate(correlation_matrix_kendall.columns):
    for j, col2 in enumerate(correlation_matrix_kendall.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_kendall.loc[col1, col2]
            if correlation > max_correlation_kendall:
                max_correlation_kendall = correlation
                best_pair_kendall = (col1, col2)

print(f"Best Correlated Pair (Kendall): {best_pair_kendall} with Correlation: {max_correlation_kendall:.2f}")

print(f"Sum for each subject saved to {output_file}")

"""NDARRD720XZK"""

videodata= np.load("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARRD720XZK/evaluation/baseline_eLORETA.npyy")  #base relax
videodata1= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARRD720XZK/evaluation/video1_eLORETA.npy") #marowa's drive video1
videodata2= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARRD720XZK/evaluation/video2_eLORETA.npy") #video2
videodata3= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARRD720XZK/evaluation/video3_eLORETA.npy") #video3
#luca's drive
#pathLuca = "/content/drive/MyDrive/TU Dublin-NN/research/PHD CANDIDATES - AICL research LAB/CURRENT/Carlos Gomez Tapia - PB04482/dataset - full source activation space (31000x11250) for 35 subjects/"
#fileName = "video1_eLORETA.npy"
#videodata= np.load (pathLuca + fileName)
print(videodata1.shape)
print(len(videodata)) #len of points in brain (31553)
print(videodata[0])
print(len(videodata[0]))  #timestamps  (11250=125hz x 90 seconds)
print(type(videodata))
#video2data is a list of list (first list 31553 lenghth,each of this cell contains 11250 point,31553 is represented the number of the brain and the 11250 is the point in time  )
import numpy as np
import pandas as pd
from scipy.stats import f_oneway, ttest_ind
import matplotlib.pyplot as plt


datasets = [videodata, videodata1, videodata2, videodata3]
dataset_labels = ["videodata", "videodata1", "videodata2", "videodata3"]

# Step 2: Calculate Mean, SD, Sum, and Average for each column
summary_stats = {}
for i, dataset in enumerate(datasets):
    means = np.mean(dataset, axis=0)
    sds = np.std(dataset, axis=0)
    sums = np.sum(dataset, axis=0)
    averages = np.mean(dataset, axis=1)

    summary_stats[dataset_labels[i]] = {
        "Mean": means,
        "SD": sds,
        "Sum": sums,
        "Average": averages
    }

# Step 3: Visualize the Mean and Sum for each dataset
for label, stats in summary_stats.items():
    # Mean Visualization
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Mean"])), stats["Mean"], color='skyblue', label="Mean")
    plt.xlabel('Columns')
    plt.ylabel('Mean Values')
    plt.title(f'Mean for {label}')
    plt.legend()
    plt.show()

    # Sum Visualization
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Sum"])), stats["Sum"], color='orange', label="Sum")
    plt.xlabel('Columns')
    plt.ylabel('Sum Values')
    plt.title(f'Sum for {label}')
    plt.legend()
    plt.show()

    # Average Visualization (per subject)
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Average"])), stats["Average"], color='green', label="Average per Subject")
    plt.xlabel('Subjects')
    plt.ylabel('Average Values')
    plt.title(f'Average per Subject in {label}')
    plt.legend()
    plt.show()
# Step 4: Compare Mean and Sum between subjects in each dataset
for label, stats in summary_stats.items():
    # Mean comparison
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Average"])), stats["Average"], color='green', label="Average per Subject")
    plt.xlabel('Subjects')
    plt.ylabel('Average Values')
    plt.title(f'Average in {label}')
    plt.legend()
    plt.show()

# Step 5: Statistical Comparison between Datasets
print("ANOVA Results (Mean Comparison):")
mean_anova_results = f_oneway(
    summary_stats["videodata"]["Mean"],
    summary_stats["videodata1"]["Mean"],
    summary_stats["videodata2"]["Mean"],
    summary_stats["videodata3"]["Mean"]
)
print(f"F-statistic: {mean_anova_results.statistic:.4f}, P-value: {mean_anova_results.pvalue:.4f}")

print("\nANOVA Results (Sum Comparison):")
sum_anova_results = f_oneway(
    summary_stats["videodata"]["Sum"],
    summary_stats["videodata1"]["Sum"],
    summary_stats["videodata2"]["Sum"],
    summary_stats["videodata3"]["Sum"]
)
print(f"F-statistic: {sum_anova_results.statistic:.4f}, P-value: {sum_anova_results.pvalue:.4f}")

# Pairwise Comparison for Means and Sums
print("\nPairwise T-tests (Mean Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Mean"],
            summary_stats[dataset_labels[j]]["Mean"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")

print("\nANOVA Results (Average Comparison):")
average_anova_results = f_oneway(
    summary_stats["videodata"]["Average"],
    summary_stats["videodata1"]["Average"],
    summary_stats["videodata2"]["Average"],
    summary_stats["videodata3"]["Average"]
)
print(f"F-statistic: {average_anova_results.statistic:.4f}, P-value: {average_anova_results.pvalue:.4f}")

print("\nPairwise T-tests (Sum Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Sum"],
            summary_stats[dataset_labels[j]]["Sum"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")
# Pairwise Comparison for Averages
print("\nPairwise T-tests (Average Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Average"],
            summary_stats[dataset_labels[j]]["Average"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")

# Ensure all datasets have the same number of rows
num_subjects = videodata.shape[0]
if any(dataset.shape[0] != num_subjects for dataset in [videodata1, videodata2, videodata3]):
    raise ValueError("All video datasets must have the same number of subjects (rows).")

# Calculate the sum for each subject in each video
sums_video1 = np.sum(videodata, axis=1)
sums_video2 = np.sum(videodata1, axis=1)
sums_video3 = np.sum(videodata2, axis=1)
sums_video4 = np.sum(videodata3, axis=1)

# Create a DataFrame to store the results
subjects = [f"Subject_{i+1}" for i in range(num_subjects)]  # Generate subject labels

sum_data = pd.DataFrame({
    "Subject": subjects,
    "Video1_Sum": sums_video1,
    "Video2_Sum": sums_video2,
    "Video3_Sum": sums_video3,
    "Video4_Sum": sums_video4
})

# Save the data to a CSV file
output_file = "subject_sums.csv"
sum_data.to_csv(output_file, index=False)

# Calculate correlation matrices using different methods
correlation_matrix_pearson = sum_data.drop("Subject", axis=1).corr(method="pearson")
correlation_matrix_spearman = sum_data.drop("Subject", axis=1).corr(method="spearman")
correlation_matrix_kendall = sum_data.drop("Subject", axis=1).corr(method="kendall")

# Print correlation matrices
print("Pearson Correlation Matrix:")
print(correlation_matrix_pearson)
print("\nSpearman Correlation Matrix:")
print(correlation_matrix_spearman)
print("\nKendall Correlation Matrix:")
print(correlation_matrix_kendall)

# Visualize the correlation matrices
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_pearson, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Pearson Correlation Matrix Between Videos (Based on Sums)")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_spearman, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Spearman Correlation Matrix Between Videos (Based on Sums)")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_kendall, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Kendall Correlation Matrix Between Videos (Based on Sums)")
plt.show()

# Determine the best model (pair with the highest correlation in Pearson)
best_pair_pearson = None
max_correlation_pearson = -1

for i, col1 in enumerate(correlation_matrix_pearson.columns):
    for j, col2 in enumerate(correlation_matrix_pearson.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_pearson.loc[col1, col2]
            if correlation > max_correlation_pearson:
                max_correlation_pearson = correlation
                best_pair_pearson = (col1, col2)

print(f"Best Correlated Pair (Pearson): {best_pair_pearson} with Correlation: {max_correlation_pearson:.2f}")

# Determine the best model (pair with the highest correlation in Spearman)
best_pair_spearman = None
max_correlation_spearman = -1

for i, col1 in enumerate(correlation_matrix_spearman.columns):
    for j, col2 in enumerate(correlation_matrix_spearman.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_spearman.loc[col1, col2]
            if correlation > max_correlation_spearman:
                max_correlation_spearman = correlation
                best_pair_spearman = (col1, col2)

print(f"Best Correlated Pair (Spearman): {best_pair_spearman} with Correlation: {max_correlation_spearman:.2f}")

# Determine the best model (pair with the highest correlation in Kendall)
best_pair_kendall = None
max_correlation_kendall = -1

for i, col1 in enumerate(correlation_matrix_kendall.columns):
    for j, col2 in enumerate(correlation_matrix_kendall.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_kendall.loc[col1, col2]
            if correlation > max_correlation_kendall:
                max_correlation_kendall = correlation
                best_pair_kendall = (col1, col2)

print(f"Best Correlated Pair (Kendall): {best_pair_kendall} with Correlation: {max_correlation_kendall:.2f}")

print(f"Sum for each subject saved to {output_file}")

"""NDARTN760YH8"""

videodata= np.load("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARTN760YH8/evaluation/baseline_eLORETA.npy")  #base relax
videodata1= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARTN760YH8/evaluation/video1_eLORETA.npy") #marowa's drive video1
videodata2= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARTN760YH8/evaluation/video2_eLORETA.npy") #video2
videodata3= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARTN760YH8/evaluation/video3_eLORETA.npy") #video3
#luca's drive
#pathLuca = "/content/drive/MyDrive/TU Dublin-NN/research/PHD CANDIDATES - AICL research LAB/CURRENT/Carlos Gomez Tapia - PB04482/dataset - full source activation space (31000x11250) for 35 subjects/"
#fileName = "video1_eLORETA.npy"
#videodata= np.load (pathLuca + fileName)
print(videodata1.shape)
print(len(videodata)) #len of points in brain (31553)
print(videodata[0])
print(len(videodata[0]))  #timestamps  (11250=125hz x 90 seconds)
print(type(videodata))
#video2data is a list of list (first list 31553 lenghth,each of this cell contains 11250 point,31553 is represented the number of the brain and the 11250 is the point in time  )
import numpy as np
import pandas as pd
from scipy.stats import f_oneway, ttest_ind
import matplotlib.pyplot as plt


datasets = [videodata, videodata1, videodata2, videodata3]
dataset_labels = ["videodata", "videodata1", "videodata2", "videodata3"]

# Step 2: Calculate Mean, SD, Sum, and Average for each column
summary_stats = {}
for i, dataset in enumerate(datasets):
    means = np.mean(dataset, axis=0)
    sds = np.std(dataset, axis=0)
    sums = np.sum(dataset, axis=0)
    averages = np.mean(dataset, axis=1)

    summary_stats[dataset_labels[i]] = {
        "Mean": means,
        "SD": sds,
        "Sum": sums,
        "Average": averages
    }

# Step 3: Visualize the Mean and Sum for each dataset
for label, stats in summary_stats.items():
    # Mean Visualization
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Mean"])), stats["Mean"], color='skyblue', label="Mean")
    plt.xlabel('Columns')
    plt.ylabel('Mean Values')
    plt.title(f'Mean for {label}')
    plt.legend()
    plt.show()

    # Sum Visualization
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Sum"])), stats["Sum"], color='orange', label="Sum")
    plt.xlabel('Columns')
    plt.ylabel('Sum Values')
    plt.title(f'Sum for {label}')
    plt.legend()
    plt.show()

    # Average Visualization (per subject)
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Average"])), stats["Average"], color='green', label="Average per Subject")
    plt.xlabel('Subjects')
    plt.ylabel('Average Values')
    plt.title(f'Average per Subject in {label}')
    plt.legend()
    plt.show()
# Step 4: Compare Mean and Sum between subjects in each dataset
for label, stats in summary_stats.items():
    # Mean comparison
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Average"])), stats["Average"], color='green', label="Average per Subject")
    plt.xlabel('Subjects')
    plt.ylabel('Average Values')
    plt.title(f'Average in {label}')
    plt.legend()
    plt.show()

# Step 5: Statistical Comparison between Datasets
print("ANOVA Results (Mean Comparison):")
mean_anova_results = f_oneway(
    summary_stats["videodata"]["Mean"],
    summary_stats["videodata1"]["Mean"],
    summary_stats["videodata2"]["Mean"],
    summary_stats["videodata3"]["Mean"]
)
print(f"F-statistic: {mean_anova_results.statistic:.4f}, P-value: {mean_anova_results.pvalue:.4f}")

print("\nANOVA Results (Sum Comparison):")
sum_anova_results = f_oneway(
    summary_stats["videodata"]["Sum"],
    summary_stats["videodata1"]["Sum"],
    summary_stats["videodata2"]["Sum"],
    summary_stats["videodata3"]["Sum"]
)
print(f"F-statistic: {sum_anova_results.statistic:.4f}, P-value: {sum_anova_results.pvalue:.4f}")

# Pairwise Comparison for Means and Sums
print("\nPairwise T-tests (Mean Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Mean"],
            summary_stats[dataset_labels[j]]["Mean"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")

print("\nANOVA Results (Average Comparison):")
average_anova_results = f_oneway(
    summary_stats["videodata"]["Average"],
    summary_stats["videodata1"]["Average"],
    summary_stats["videodata2"]["Average"],
    summary_stats["videodata3"]["Average"]
)
print(f"F-statistic: {average_anova_results.statistic:.4f}, P-value: {average_anova_results.pvalue:.4f}")

print("\nPairwise T-tests (Sum Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Sum"],
            summary_stats[dataset_labels[j]]["Sum"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")
# Pairwise Comparison for Averages
print("\nPairwise T-tests (Average Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Average"],
            summary_stats[dataset_labels[j]]["Average"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")

# Ensure all datasets have the same number of rows
num_subjects = videodata.shape[0]
if any(dataset.shape[0] != num_subjects for dataset in [videodata1, videodata2, videodata3]):
    raise ValueError("All video datasets must have the same number of subjects (rows).")

# Calculate the sum for each subject in each video
sums_video1 = np.sum(videodata, axis=1)
sums_video2 = np.sum(videodata1, axis=1)
sums_video3 = np.sum(videodata2, axis=1)
sums_video4 = np.sum(videodata3, axis=1)

# Create a DataFrame to store the results
subjects = [f"Subject_{i+1}" for i in range(num_subjects)]  # Generate subject labels

sum_data = pd.DataFrame({
    "Subject": subjects,
    "Video1_Sum": sums_video1,
    "Video2_Sum": sums_video2,
    "Video3_Sum": sums_video3,
    "Video4_Sum": sums_video4
})

# Save the data to a CSV file
output_file = "subject_sums.csv"
sum_data.to_csv(output_file, index=False)

# Calculate correlation matrices using different methods
correlation_matrix_pearson = sum_data.drop("Subject", axis=1).corr(method="pearson")
correlation_matrix_spearman = sum_data.drop("Subject", axis=1).corr(method="spearman")
correlation_matrix_kendall = sum_data.drop("Subject", axis=1).corr(method="kendall")

# Print correlation matrices
print("Pearson Correlation Matrix:")
print(correlation_matrix_pearson)
print("\nSpearman Correlation Matrix:")
print(correlation_matrix_spearman)
print("\nKendall Correlation Matrix:")
print(correlation_matrix_kendall)

# Visualize the correlation matrices
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_pearson, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Pearson Correlation Matrix Between Videos (Based on Sums)")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_spearman, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Spearman Correlation Matrix Between Videos (Based on Sums)")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_kendall, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Kendall Correlation Matrix Between Videos (Based on Sums)")
plt.show()

# Determine the best model (pair with the highest correlation in Pearson)
best_pair_pearson = None
max_correlation_pearson = -1

for i, col1 in enumerate(correlation_matrix_pearson.columns):
    for j, col2 in enumerate(correlation_matrix_pearson.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_pearson.loc[col1, col2]
            if correlation > max_correlation_pearson:
                max_correlation_pearson = correlation
                best_pair_pearson = (col1, col2)

print(f"Best Correlated Pair (Pearson): {best_pair_pearson} with Correlation: {max_correlation_pearson:.2f}")

# Determine the best model (pair with the highest correlation in Spearman)
best_pair_spearman = None
max_correlation_spearman = -1

for i, col1 in enumerate(correlation_matrix_spearman.columns):
    for j, col2 in enumerate(correlation_matrix_spearman.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_spearman.loc[col1, col2]
            if correlation > max_correlation_spearman:
                max_correlation_spearman = correlation
                best_pair_spearman = (col1, col2)

print(f"Best Correlated Pair (Spearman): {best_pair_spearman} with Correlation: {max_correlation_spearman:.2f}")

# Determine the best model (pair with the highest correlation in Kendall)
best_pair_kendall = None
max_correlation_kendall = -1

for i, col1 in enumerate(correlation_matrix_kendall.columns):
    for j, col2 in enumerate(correlation_matrix_kendall.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_kendall.loc[col1, col2]
            if correlation > max_correlation_kendall:
                max_correlation_kendall = correlation
                best_pair_kendall = (col1, col2)

print(f"Best Correlated Pair (Kendall): {best_pair_kendall} with Correlation: {max_correlation_kendall:.2f}")

print(f"Sum for each subject saved to {output_file}")

"""NDARTR840XP1"""

videodata= np.load("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARTR840XP1/evaluation/baseline_eLORETA.npy")  #base relax
videodata1= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARTR840XP1/evaluation/video1_eLORETA.npy") #marowa's drive video1
videodata2= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARTR840XP1/evaluation/video2_eLORETA.npy") #video2
videodata3= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARTR840XP1/evaluation/video3_eLORETA.npy") #video3
#luca's drive
#pathLuca = "/content/drive/MyDrive/TU Dublin-NN/research/PHD CANDIDATES - AICL research LAB/CURRENT/Carlos Gomez Tapia - PB04482/dataset - full source activation space (31000x11250) for 35 subjects/"
#fileName = "video1_eLORETA.npy"
#videodata= np.load (pathLuca + fileName)
print(videodata1.shape)
print(len(videodata)) #len of points in brain (31553)
print(videodata[0])
print(len(videodata[0]))  #timestamps  (11250=125hz x 90 seconds)
print(type(videodata))
#video2data is a list of list (first list 31553 lenghth,each of this cell contains 11250 point,31553 is represented the number of the brain and the 11250 is the point in time  )
import numpy as np
import pandas as pd
from scipy.stats import f_oneway, ttest_ind
import matplotlib.pyplot as plt


datasets = [videodata, videodata1, videodata2, videodata3]
dataset_labels = ["videodata", "videodata1", "videodata2", "videodata3"]

# Step 2: Calculate Mean, SD, Sum, and Average for each column
summary_stats = {}
for i, dataset in enumerate(datasets):
    means = np.mean(dataset, axis=0)
    sds = np.std(dataset, axis=0)
    sums = np.sum(dataset, axis=0)
    averages = np.mean(dataset, axis=1)

    summary_stats[dataset_labels[i]] = {
        "Mean": means,
        "SD": sds,
        "Sum": sums,
        "Average": averages
    }

# Step 3: Visualize the Mean and Sum for each dataset
for label, stats in summary_stats.items():
    # Mean Visualization
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Mean"])), stats["Mean"], color='skyblue', label="Mean")
    plt.xlabel('Columns')
    plt.ylabel('Mean Values')
    plt.title(f'Mean for {label}')
    plt.legend()
    plt.show()

    # Sum Visualization
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Sum"])), stats["Sum"], color='orange', label="Sum")
    plt.xlabel('Columns')
    plt.ylabel('Sum Values')
    plt.title(f'Sum for {label}')
    plt.legend()
    plt.show()

    # Average Visualization (per subject)
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Average"])), stats["Average"], color='green', label="Average per Subject")
    plt.xlabel('Subjects')
    plt.ylabel('Average Values')
    plt.title(f'Average per Subject in {label}')
    plt.legend()
    plt.show()
# Step 4: Compare Mean and Sum between subjects in each dataset
for label, stats in summary_stats.items():
    # Mean comparison
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Average"])), stats["Average"], color='green', label="Average per Subject")
    plt.xlabel('Subjects')
    plt.ylabel('Average Values')
    plt.title(f'Average in {label}')
    plt.legend()
    plt.show()

# Step 5: Statistical Comparison between Datasets
print("ANOVA Results (Mean Comparison):")
mean_anova_results = f_oneway(
    summary_stats["videodata"]["Mean"],
    summary_stats["videodata1"]["Mean"],
    summary_stats["videodata2"]["Mean"],
    summary_stats["videodata3"]["Mean"]
)
print(f"F-statistic: {mean_anova_results.statistic:.4f}, P-value: {mean_anova_results.pvalue:.4f}")

print("\nANOVA Results (Sum Comparison):")
sum_anova_results = f_oneway(
    summary_stats["videodata"]["Sum"],
    summary_stats["videodata1"]["Sum"],
    summary_stats["videodata2"]["Sum"],
    summary_stats["videodata3"]["Sum"]
)
print(f"F-statistic: {sum_anova_results.statistic:.4f}, P-value: {sum_anova_results.pvalue:.4f}")

# Pairwise Comparison for Means and Sums
print("\nPairwise T-tests (Mean Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Mean"],
            summary_stats[dataset_labels[j]]["Mean"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")

print("\nANOVA Results (Average Comparison):")
average_anova_results = f_oneway(
    summary_stats["videodata"]["Average"],
    summary_stats["videodata1"]["Average"],
    summary_stats["videodata2"]["Average"],
    summary_stats["videodata3"]["Average"]
)
print(f"F-statistic: {average_anova_results.statistic:.4f}, P-value: {average_anova_results.pvalue:.4f}")

print("\nPairwise T-tests (Sum Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Sum"],
            summary_stats[dataset_labels[j]]["Sum"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")
# Pairwise Comparison for Averages
print("\nPairwise T-tests (Average Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Average"],
            summary_stats[dataset_labels[j]]["Average"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")

# Ensure all datasets have the same number of rows
num_subjects = videodata.shape[0]
if any(dataset.shape[0] != num_subjects for dataset in [videodata1, videodata2, videodata3]):
    raise ValueError("All video datasets must have the same number of subjects (rows).")

# Calculate the sum for each subject in each video
sums_video1 = np.sum(videodata, axis=1)
sums_video2 = np.sum(videodata1, axis=1)
sums_video3 = np.sum(videodata2, axis=1)
sums_video4 = np.sum(videodata3, axis=1)

# Create a DataFrame to store the results
subjects = [f"Subject_{i+1}" for i in range(num_subjects)]  # Generate subject labels

sum_data = pd.DataFrame({
    "Subject": subjects,
    "Video1_Sum": sums_video1,
    "Video2_Sum": sums_video2,
    "Video3_Sum": sums_video3,
    "Video4_Sum": sums_video4
})

# Save the data to a CSV file
output_file = "subject_sums.csv"
sum_data.to_csv(output_file, index=False)

# Calculate correlation matrices using different methods
correlation_matrix_pearson = sum_data.drop("Subject", axis=1).corr(method="pearson")
correlation_matrix_spearman = sum_data.drop("Subject", axis=1).corr(method="spearman")
correlation_matrix_kendall = sum_data.drop("Subject", axis=1).corr(method="kendall")

# Print correlation matrices
print("Pearson Correlation Matrix:")
print(correlation_matrix_pearson)
print("\nSpearman Correlation Matrix:")
print(correlation_matrix_spearman)
print("\nKendall Correlation Matrix:")
print(correlation_matrix_kendall)

# Visualize the correlation matrices
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_pearson, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Pearson Correlation Matrix Between Videos (Based on Sums)")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_spearman, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Spearman Correlation Matrix Between Videos (Based on Sums)")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_kendall, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Kendall Correlation Matrix Between Videos (Based on Sums)")
plt.show()

# Determine the best model (pair with the highest correlation in Pearson)
best_pair_pearson = None
max_correlation_pearson = -1

for i, col1 in enumerate(correlation_matrix_pearson.columns):
    for j, col2 in enumerate(correlation_matrix_pearson.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_pearson.loc[col1, col2]
            if correlation > max_correlation_pearson:
                max_correlation_pearson = correlation
                best_pair_pearson = (col1, col2)

print(f"Best Correlated Pair (Pearson): {best_pair_pearson} with Correlation: {max_correlation_pearson:.2f}")

# Determine the best model (pair with the highest correlation in Spearman)
best_pair_spearman = None
max_correlation_spearman = -1

for i, col1 in enumerate(correlation_matrix_spearman.columns):
    for j, col2 in enumerate(correlation_matrix_spearman.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_spearman.loc[col1, col2]
            if correlation > max_correlation_spearman:
                max_correlation_spearman = correlation
                best_pair_spearman = (col1, col2)

print(f"Best Correlated Pair (Spearman): {best_pair_spearman} with Correlation: {max_correlation_spearman:.2f}")

# Determine the best model (pair with the highest correlation in Kendall)
best_pair_kendall = None
max_correlation_kendall = -1

for i, col1 in enumerate(correlation_matrix_kendall.columns):
    for j, col2 in enumerate(correlation_matrix_kendall.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_kendall.loc[col1, col2]
            if correlation > max_correlation_kendall:
                max_correlation_kendall = correlation
                best_pair_kendall = (col1, col2)

print(f"Best Correlated Pair (Kendall): {best_pair_kendall} with Correlation: {max_correlation_kendall:.2f}")

print(f"Sum for each subject saved to {output_file}")

"""NDARUL945XUU"""

videodata= np.load("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARUL945XUU/evaluation/baseline_eLORETA.npy")  #base relax
videodata1= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARUL945XUU/evaluation/video1_eLORETA.npy") #marowa's drive video1
videodata2= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARUL945XUU/evaluation/video2_eLORETA.npy") #video2
videodata3= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARUL945XUU/evaluation/video3_eLORETA.npy") #video3
#luca's drive
#pathLuca = "/content/drive/MyDrive/TU Dublin-NN/research/PHD CANDIDATES - AICL research LAB/CURRENT/Carlos Gomez Tapia - PB04482/dataset - full source activation space (31000x11250) for 35 subjects/"
#fileName = "video1_eLORETA.npy"
#videodata= np.load (pathLuca + fileName)
print(videodata1.shape)
print(len(videodata)) #len of points in brain (31553)
print(videodata[0])
print(len(videodata[0]))  #timestamps  (11250=125hz x 90 seconds)
print(type(videodata))
#video2data is a list of list (first list 31553 lenghth,each of this cell contains 11250 point,31553 is represented the number of the brain and the 11250 is the point in time  )
import numpy as np
import pandas as pd
from scipy.stats import f_oneway, ttest_ind
import matplotlib.pyplot as plt


datasets = [videodata, videodata1, videodata2, videodata3]
dataset_labels = ["videodata", "videodata1", "videodata2", "videodata3"]

# Step 2: Calculate Mean, SD, Sum, and Average for each column
summary_stats = {}
for i, dataset in enumerate(datasets):
    means = np.mean(dataset, axis=0)
    sds = np.std(dataset, axis=0)
    sums = np.sum(dataset, axis=0)
    averages = np.mean(dataset, axis=1)

    summary_stats[dataset_labels[i]] = {
        "Mean": means,
        "SD": sds,
        "Sum": sums,
        "Average": averages
    }

# Step 3: Visualize the Mean and Sum for each dataset
for label, stats in summary_stats.items():
    # Mean Visualization
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Mean"])), stats["Mean"], color='skyblue', label="Mean")
    plt.xlabel('Columns')
    plt.ylabel('Mean Values')
    plt.title(f'Mean for {label}')
    plt.legend()
    plt.show()

    # Sum Visualization
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Sum"])), stats["Sum"], color='orange', label="Sum")
    plt.xlabel('Columns')
    plt.ylabel('Sum Values')
    plt.title(f'Sum for {label}')
    plt.legend()
    plt.show()

    # Average Visualization (per subject)
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Average"])), stats["Average"], color='green', label="Average per Subject")
    plt.xlabel('Subjects')
    plt.ylabel('Average Values')
    plt.title(f'Average per Subject in {label}')
    plt.legend()
    plt.show()
# Step 4: Compare Mean and Sum between subjects in each dataset
for label, stats in summary_stats.items():
    # Mean comparison
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Average"])), stats["Average"], color='green', label="Average per Subject")
    plt.xlabel('Subjects')
    plt.ylabel('Average Values')
    plt.title(f'Average in {label}')
    plt.legend()
    plt.show()

# Step 5: Statistical Comparison between Datasets
print("ANOVA Results (Mean Comparison):")
mean_anova_results = f_oneway(
    summary_stats["videodata"]["Mean"],
    summary_stats["videodata1"]["Mean"],
    summary_stats["videodata2"]["Mean"],
    summary_stats["videodata3"]["Mean"]
)
print(f"F-statistic: {mean_anova_results.statistic:.4f}, P-value: {mean_anova_results.pvalue:.4f}")

print("\nANOVA Results (Sum Comparison):")
sum_anova_results = f_oneway(
    summary_stats["videodata"]["Sum"],
    summary_stats["videodata1"]["Sum"],
    summary_stats["videodata2"]["Sum"],
    summary_stats["videodata3"]["Sum"]
)
print(f"F-statistic: {sum_anova_results.statistic:.4f}, P-value: {sum_anova_results.pvalue:.4f}")

# Pairwise Comparison for Means and Sums
print("\nPairwise T-tests (Mean Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Mean"],
            summary_stats[dataset_labels[j]]["Mean"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")

print("\nANOVA Results (Average Comparison):")
average_anova_results = f_oneway(
    summary_stats["videodata"]["Average"],
    summary_stats["videodata1"]["Average"],
    summary_stats["videodata2"]["Average"],
    summary_stats["videodata3"]["Average"]
)
print(f"F-statistic: {average_anova_results.statistic:.4f}, P-value: {average_anova_results.pvalue:.4f}")

print("\nPairwise T-tests (Sum Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Sum"],
            summary_stats[dataset_labels[j]]["Sum"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")
# Pairwise Comparison for Averages
print("\nPairwise T-tests (Average Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Average"],
            summary_stats[dataset_labels[j]]["Average"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")

# Ensure all datasets have the same number of rows
num_subjects = videodata.shape[0]
if any(dataset.shape[0] != num_subjects for dataset in [videodata1, videodata2, videodata3]):
    raise ValueError("All video datasets must have the same number of subjects (rows).")

# Calculate the sum for each subject in each video
sums_video1 = np.sum(videodata, axis=1)
sums_video2 = np.sum(videodata1, axis=1)
sums_video3 = np.sum(videodata2, axis=1)
sums_video4 = np.sum(videodata3, axis=1)

# Create a DataFrame to store the results
subjects = [f"Subject_{i+1}" for i in range(num_subjects)]  # Generate subject labels

sum_data = pd.DataFrame({
    "Subject": subjects,
    "Video1_Sum": sums_video1,
    "Video2_Sum": sums_video2,
    "Video3_Sum": sums_video3,
    "Video4_Sum": sums_video4
})

# Save the data to a CSV file
output_file = "subject_sums.csv"
sum_data.to_csv(output_file, index=False)

# Calculate correlation matrices using different methods
correlation_matrix_pearson = sum_data.drop("Subject", axis=1).corr(method="pearson")
correlation_matrix_spearman = sum_data.drop("Subject", axis=1).corr(method="spearman")
correlation_matrix_kendall = sum_data.drop("Subject", axis=1).corr(method="kendall")

# Print correlation matrices
print("Pearson Correlation Matrix:")
print(correlation_matrix_pearson)
print("\nSpearman Correlation Matrix:")
print(correlation_matrix_spearman)
print("\nKendall Correlation Matrix:")
print(correlation_matrix_kendall)

# Visualize the correlation matrices
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_pearson, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Pearson Correlation Matrix Between Videos (Based on Sums)")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_spearman, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Spearman Correlation Matrix Between Videos (Based on Sums)")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_kendall, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Kendall Correlation Matrix Between Videos (Based on Sums)")
plt.show()

# Determine the best model (pair with the highest correlation in Pearson)
best_pair_pearson = None
max_correlation_pearson = -1

for i, col1 in enumerate(correlation_matrix_pearson.columns):
    for j, col2 in enumerate(correlation_matrix_pearson.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_pearson.loc[col1, col2]
            if correlation > max_correlation_pearson:
                max_correlation_pearson = correlation
                best_pair_pearson = (col1, col2)

print(f"Best Correlated Pair (Pearson): {best_pair_pearson} with Correlation: {max_correlation_pearson:.2f}")

# Determine the best model (pair with the highest correlation in Spearman)
best_pair_spearman = None
max_correlation_spearman = -1

for i, col1 in enumerate(correlation_matrix_spearman.columns):
    for j, col2 in enumerate(correlation_matrix_spearman.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_spearman.loc[col1, col2]
            if correlation > max_correlation_spearman:
                max_correlation_spearman = correlation
                best_pair_spearman = (col1, col2)

print(f"Best Correlated Pair (Spearman): {best_pair_spearman} with Correlation: {max_correlation_spearman:.2f}")

# Determine the best model (pair with the highest correlation in Kendall)
best_pair_kendall = None
max_correlation_kendall = -1

for i, col1 in enumerate(correlation_matrix_kendall.columns):
    for j, col2 in enumerate(correlation_matrix_kendall.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_kendall.loc[col1, col2]
            if correlation > max_correlation_kendall:
                max_correlation_kendall = correlation
                best_pair_kendall = (col1, col2)

print(f"Best Correlated Pair (Kendall): {best_pair_kendall} with Correlation: {max_correlation_kendall:.2f}")

print(f"Sum for each subject saved to {output_file}")

"""NDARUY529FNK"""

videodata= np.load("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARUY529FNK/evaluation/baseline_eLORETA.npy")  #base relax
videodata1= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARUY529FNK/evaluation/video1_eLORETA.npy") #marowa's drive video1
videodata2= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARUY529FNK/evaluation/video2_eLORETA.npy") #video2
videodata3= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARUY529FNK/evaluation/video3_eLORETA.npy") #video3
#luca's drive
#pathLuca = "/content/drive/MyDrive/TU Dublin-NN/research/PHD CANDIDATES - AICL research LAB/CURRENT/Carlos Gomez Tapia - PB04482/dataset - full source activation space (31000x11250) for 35 subjects/"
#fileName = "video1_eLORETA.npy"
#videodata= np.load (pathLuca + fileName)
print(videodata1.shape)
print(len(videodata)) #len of points in brain (31553)
print(videodata[0])
print(len(videodata[0]))  #timestamps  (11250=125hz x 90 seconds)
print(type(videodata))
#video2data is a list of list (first list 31553 lenghth,each of this cell contains 11250 point,31553 is represented the number of the brain and the 11250 is the point in time  )
import numpy as np
import pandas as pd
from scipy.stats import f_oneway, ttest_ind
import matplotlib.pyplot as plt


datasets = [videodata, videodata1, videodata2, videodata3]
dataset_labels = ["videodata", "videodata1", "videodata2", "videodata3"]

# Step 2: Calculate Mean, SD, Sum, and Average for each column
summary_stats = {}
for i, dataset in enumerate(datasets):
    means = np.mean(dataset, axis=0)
    sds = np.std(dataset, axis=0)
    sums = np.sum(dataset, axis=0)
    averages = np.mean(dataset, axis=1)

    summary_stats[dataset_labels[i]] = {
        "Mean": means,
        "SD": sds,
        "Sum": sums,
        "Average": averages
    }

# Step 3: Visualize the Mean and Sum for each dataset
for label, stats in summary_stats.items():
    # Mean Visualization
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Mean"])), stats["Mean"], color='skyblue', label="Mean")
    plt.xlabel('Columns')
    plt.ylabel('Mean Values')
    plt.title(f'Mean for {label}')
    plt.legend()
    plt.show()

    # Sum Visualization
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Sum"])), stats["Sum"], color='orange', label="Sum")
    plt.xlabel('Columns')
    plt.ylabel('Sum Values')
    plt.title(f'Sum for {label}')
    plt.legend()
    plt.show()

    # Average Visualization (per subject)
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Average"])), stats["Average"], color='green', label="Average per Subject")
    plt.xlabel('Subjects')
    plt.ylabel('Average Values')
    plt.title(f'Average per Subject in {label}')
    plt.legend()
    plt.show()
# Step 4: Compare Mean and Sum between subjects in each dataset
for label, stats in summary_stats.items():
    # Mean comparison
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Average"])), stats["Average"], color='green', label="Average per Subject")
    plt.xlabel('Subjects')
    plt.ylabel('Average Values')
    plt.title(f'Average in {label}')
    plt.legend()
    plt.show()

# Step 5: Statistical Comparison between Datasets
print("ANOVA Results (Mean Comparison):")
mean_anova_results = f_oneway(
    summary_stats["videodata"]["Mean"],
    summary_stats["videodata1"]["Mean"],
    summary_stats["videodata2"]["Mean"],
    summary_stats["videodata3"]["Mean"]
)
print(f"F-statistic: {mean_anova_results.statistic:.4f}, P-value: {mean_anova_results.pvalue:.4f}")

print("\nANOVA Results (Sum Comparison):")
sum_anova_results = f_oneway(
    summary_stats["videodata"]["Sum"],
    summary_stats["videodata1"]["Sum"],
    summary_stats["videodata2"]["Sum"],
    summary_stats["videodata3"]["Sum"]
)
print(f"F-statistic: {sum_anova_results.statistic:.4f}, P-value: {sum_anova_results.pvalue:.4f}")

# Pairwise Comparison for Means and Sums
print("\nPairwise T-tests (Mean Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Mean"],
            summary_stats[dataset_labels[j]]["Mean"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")

print("\nANOVA Results (Average Comparison):")
average_anova_results = f_oneway(
    summary_stats["videodata"]["Average"],
    summary_stats["videodata1"]["Average"],
    summary_stats["videodata2"]["Average"],
    summary_stats["videodata3"]["Average"]
)
print(f"F-statistic: {average_anova_results.statistic:.4f}, P-value: {average_anova_results.pvalue:.4f}")

print("\nPairwise T-tests (Sum Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Sum"],
            summary_stats[dataset_labels[j]]["Sum"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")
# Pairwise Comparison for Averages
print("\nPairwise T-tests (Average Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Average"],
            summary_stats[dataset_labels[j]]["Average"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")

# Ensure all datasets have the same number of rows
num_subjects = videodata.shape[0]
if any(dataset.shape[0] != num_subjects for dataset in [videodata1, videodata2, videodata3]):
    raise ValueError("All video datasets must have the same number of subjects (rows).")

# Calculate the sum for each subject in each video
sums_video1 = np.sum(videodata, axis=1)
sums_video2 = np.sum(videodata1, axis=1)
sums_video3 = np.sum(videodata2, axis=1)
sums_video4 = np.sum(videodata3, axis=1)

# Create a DataFrame to store the results
subjects = [f"Subject_{i+1}" for i in range(num_subjects)]  # Generate subject labels

sum_data = pd.DataFrame({
    "Subject": subjects,
    "Video1_Sum": sums_video1,
    "Video2_Sum": sums_video2,
    "Video3_Sum": sums_video3,
    "Video4_Sum": sums_video4
})

# Save the data to a CSV file
output_file = "subject_sums.csv"
sum_data.to_csv(output_file, index=False)

# Calculate correlation matrices using different methods
correlation_matrix_pearson = sum_data.drop("Subject", axis=1).corr(method="pearson")
correlation_matrix_spearman = sum_data.drop("Subject", axis=1).corr(method="spearman")
correlation_matrix_kendall = sum_data.drop("Subject", axis=1).corr(method="kendall")

# Print correlation matrices
print("Pearson Correlation Matrix:")
print(correlation_matrix_pearson)
print("\nSpearman Correlation Matrix:")
print(correlation_matrix_spearman)
print("\nKendall Correlation Matrix:")
print(correlation_matrix_kendall)

# Visualize the correlation matrices
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_pearson, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Pearson Correlation Matrix Between Videos (Based on Sums)")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_spearman, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Spearman Correlation Matrix Between Videos (Based on Sums)")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_kendall, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Kendall Correlation Matrix Between Videos (Based on Sums)")
plt.show()

# Determine the best model (pair with the highest correlation in Pearson)
best_pair_pearson = None
max_correlation_pearson = -1

for i, col1 in enumerate(correlation_matrix_pearson.columns):
    for j, col2 in enumerate(correlation_matrix_pearson.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_pearson.loc[col1, col2]
            if correlation > max_correlation_pearson:
                max_correlation_pearson = correlation
                best_pair_pearson = (col1, col2)

print(f"Best Correlated Pair (Pearson): {best_pair_pearson} with Correlation: {max_correlation_pearson:.2f}")

# Determine the best model (pair with the highest correlation in Spearman)
best_pair_spearman = None
max_correlation_spearman = -1

for i, col1 in enumerate(correlation_matrix_spearman.columns):
    for j, col2 in enumerate(correlation_matrix_spearman.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_spearman.loc[col1, col2]
            if correlation > max_correlation_spearman:
                max_correlation_spearman = correlation
                best_pair_spearman = (col1, col2)

print(f"Best Correlated Pair (Spearman): {best_pair_spearman} with Correlation: {max_correlation_spearman:.2f}")

# Determine the best model (pair with the highest correlation in Kendall)
best_pair_kendall = None
max_correlation_kendall = -1

for i, col1 in enumerate(correlation_matrix_kendall.columns):
    for j, col2 in enumerate(correlation_matrix_kendall.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_kendall.loc[col1, col2]
            if correlation > max_correlation_kendall:
                max_correlation_kendall = correlation
                best_pair_kendall = (col1, col2)

print(f"Best Correlated Pair (Kendall): {best_pair_kendall} with Correlation: {max_correlation_kendall:.2f}")

print(f"Sum for each subject saved to {output_file}")

"""NDARWX173EFW"""

videodata= np.load("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARWX173EFW/evaluation/baseline_eLORETA.npy")  #base relax
videodata1= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARWX173EFW/evaluation/video1_eLORETA.npy") #marowa's drive video1
videodata2= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARWX173EFW/evaluation/video2_eLORETA.npy") #video2
videodata3= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARWX173EFW/evaluation/video3_eLORETA.npy") #video3
#luca's drive
#pathLuca = "/content/drive/MyDrive/TU Dublin-NN/research/PHD CANDIDATES - AICL research LAB/CURRENT/Carlos Gomez Tapia - PB04482/dataset - full source activation space (31000x11250) for 35 subjects/"
#fileName = "video1_eLORETA.npy"
#videodata= np.load (pathLuca + fileName)
print(videodata1.shape)
print(len(videodata)) #len of points in brain (31553)
print(videodata[0])
print(len(videodata[0]))  #timestamps  (11250=125hz x 90 seconds)
print(type(videodata))
#video2data is a list of list (first list 31553 lenghth,each of this cell contains 11250 point,31553 is represented the number of the brain and the 11250 is the point in time  )
import numpy as np
import pandas as pd
from scipy.stats import f_oneway, ttest_ind
import matplotlib.pyplot as plt


datasets = [videodata, videodata1, videodata2, videodata3]
dataset_labels = ["videodata", "videodata1", "videodata2", "videodata3"]

# Step 2: Calculate Mean, SD, Sum, and Average for each column
summary_stats = {}
for i, dataset in enumerate(datasets):
    means = np.mean(dataset, axis=0)
    sds = np.std(dataset, axis=0)
    sums = np.sum(dataset, axis=0)
    averages = np.mean(dataset, axis=1)

    summary_stats[dataset_labels[i]] = {
        "Mean": means,
        "SD": sds,
        "Sum": sums,
        "Average": averages
    }

# Step 3: Visualize the Mean and Sum for each dataset
for label, stats in summary_stats.items():
    # Mean Visualization
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Mean"])), stats["Mean"], color='skyblue', label="Mean")
    plt.xlabel('Columns')
    plt.ylabel('Mean Values')
    plt.title(f'Mean for {label}')
    plt.legend()
    plt.show()

    # Sum Visualization
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Sum"])), stats["Sum"], color='orange', label="Sum")
    plt.xlabel('Columns')
    plt.ylabel('Sum Values')
    plt.title(f'Sum for {label}')
    plt.legend()
    plt.show()

    # Average Visualization (per subject)
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Average"])), stats["Average"], color='green', label="Average per Subject")
    plt.xlabel('Subjects')
    plt.ylabel('Average Values')
    plt.title(f'Average per Subject in {label}')
    plt.legend()
    plt.show()
# Step 4: Compare Mean and Sum between subjects in each dataset
for label, stats in summary_stats.items():
    # Mean comparison
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Average"])), stats["Average"], color='green', label="Average per Subject")
    plt.xlabel('Subjects')
    plt.ylabel('Average Values')
    plt.title(f'Average in {label}')
    plt.legend()
    plt.show()

# Step 5: Statistical Comparison between Datasets
print("ANOVA Results (Mean Comparison):")
mean_anova_results = f_oneway(
    summary_stats["videodata"]["Mean"],
    summary_stats["videodata1"]["Mean"],
    summary_stats["videodata2"]["Mean"],
    summary_stats["videodata3"]["Mean"]
)
print(f"F-statistic: {mean_anova_results.statistic:.4f}, P-value: {mean_anova_results.pvalue:.4f}")

print("\nANOVA Results (Sum Comparison):")
sum_anova_results = f_oneway(
    summary_stats["videodata"]["Sum"],
    summary_stats["videodata1"]["Sum"],
    summary_stats["videodata2"]["Sum"],
    summary_stats["videodata3"]["Sum"]
)
print(f"F-statistic: {sum_anova_results.statistic:.4f}, P-value: {sum_anova_results.pvalue:.4f}")

# Pairwise Comparison for Means and Sums
print("\nPairwise T-tests (Mean Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Mean"],
            summary_stats[dataset_labels[j]]["Mean"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")

print("\nANOVA Results (Average Comparison):")
average_anova_results = f_oneway(
    summary_stats["videodata"]["Average"],
    summary_stats["videodata1"]["Average"],
    summary_stats["videodata2"]["Average"],
    summary_stats["videodata3"]["Average"]
)
print(f"F-statistic: {average_anova_results.statistic:.4f}, P-value: {average_anova_results.pvalue:.4f}")

print("\nPairwise T-tests (Sum Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Sum"],
            summary_stats[dataset_labels[j]]["Sum"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")
# Pairwise Comparison for Averages
print("\nPairwise T-tests (Average Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Average"],
            summary_stats[dataset_labels[j]]["Average"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")

# Ensure all datasets have the same number of rows
num_subjects = videodata.shape[0]
if any(dataset.shape[0] != num_subjects for dataset in [videodata1, videodata2, videodata3]):
    raise ValueError("All video datasets must have the same number of subjects (rows).")

# Calculate the sum for each subject in each video
sums_video1 = np.sum(videodata, axis=1)
sums_video2 = np.sum(videodata1, axis=1)
sums_video3 = np.sum(videodata2, axis=1)
sums_video4 = np.sum(videodata3, axis=1)

# Create a DataFrame to store the results
subjects = [f"Subject_{i+1}" for i in range(num_subjects)]  # Generate subject labels

sum_data = pd.DataFrame({
    "Subject": subjects,
    "Video1_Sum": sums_video1,
    "Video2_Sum": sums_video2,
    "Video3_Sum": sums_video3,
    "Video4_Sum": sums_video4
})

# Save the data to a CSV file
output_file = "subject_sums.csv"
sum_data.to_csv(output_file, index=False)

# Calculate correlation matrices using different methods
correlation_matrix_pearson = sum_data.drop("Subject", axis=1).corr(method="pearson")
correlation_matrix_spearman = sum_data.drop("Subject", axis=1).corr(method="spearman")
correlation_matrix_kendall = sum_data.drop("Subject", axis=1).corr(method="kendall")

# Print correlation matrices
print("Pearson Correlation Matrix:")
print(correlation_matrix_pearson)
print("\nSpearman Correlation Matrix:")
print(correlation_matrix_spearman)
print("\nKendall Correlation Matrix:")
print(correlation_matrix_kendall)

# Visualize the correlation matrices
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_pearson, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Pearson Correlation Matrix Between Videos (Based on Sums)")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_spearman, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Spearman Correlation Matrix Between Videos (Based on Sums)")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_kendall, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Kendall Correlation Matrix Between Videos (Based on Sums)")
plt.show()

# Determine the best model (pair with the highest correlation in Pearson)
best_pair_pearson = None
max_correlation_pearson = -1

for i, col1 in enumerate(correlation_matrix_pearson.columns):
    for j, col2 in enumerate(correlation_matrix_pearson.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_pearson.loc[col1, col2]
            if correlation > max_correlation_pearson:
                max_correlation_pearson = correlation
                best_pair_pearson = (col1, col2)

print(f"Best Correlated Pair (Pearson): {best_pair_pearson} with Correlation: {max_correlation_pearson:.2f}")

# Determine the best model (pair with the highest correlation in Spearman)
best_pair_spearman = None
max_correlation_spearman = -1

for i, col1 in enumerate(correlation_matrix_spearman.columns):
    for j, col2 in enumerate(correlation_matrix_spearman.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_spearman.loc[col1, col2]
            if correlation > max_correlation_spearman:
                max_correlation_spearman = correlation
                best_pair_spearman = (col1, col2)

print(f"Best Correlated Pair (Spearman): {best_pair_spearman} with Correlation: {max_correlation_spearman:.2f}")

# Determine the best model (pair with the highest correlation in Kendall)
best_pair_kendall = None
max_correlation_kendall = -1

for i, col1 in enumerate(correlation_matrix_kendall.columns):
    for j, col2 in enumerate(correlation_matrix_kendall.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_kendall.loc[col1, col2]
            if correlation > max_correlation_kendall:
                max_correlation_kendall = correlation
                best_pair_kendall = (col1, col2)

print(f"Best Correlated Pair (Kendall): {best_pair_kendall} with Correlation: {max_correlation_kendall:.2f}")

print(f"Sum for each subject saved to {output_file}")

"""NDARXJ696AMX"""

videodata= np.load("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARXJ696AMX/evaluation/baseline_eLORETA.npy")  #base relax
videodata1= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARXJ696AMX/evaluation/video1_eLORETA.npy") #marowa's drive video1
videodata2= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARXJ696AMX/evaluation/video2_eLORETA.npy") #video2
videodata3= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARXJ696AMX/evaluation/video3_eLORETA.npy") #video3
#luca's drive
#pathLuca = "/content/drive/MyDrive/TU Dublin-NN/research/PHD CANDIDATES - AICL research LAB/CURRENT/Carlos Gomez Tapia - PB04482/dataset - full source activation space (31000x11250) for 35 subjects/"
#fileName = "video1_eLORETA.npy"
#videodata= np.load (pathLuca + fileName)
print(videodata1.shape)
print(len(videodata)) #len of points in brain (31553)
print(videodata[0])
print(len(videodata[0]))  #timestamps  (11250=125hz x 90 seconds)
print(type(videodata))
#video2data is a list of list (first list 31553 lenghth,each of this cell contains 11250 point,31553 is represented the number of the brain and the 11250 is the point in time  )
import numpy as np
import pandas as pd
from scipy.stats import f_oneway, ttest_ind
import matplotlib.pyplot as plt


datasets = [videodata, videodata1, videodata2, videodata3]
dataset_labels = ["videodata", "videodata1", "videodata2", "videodata3"]

# Step 2: Calculate Mean, SD, Sum, and Average for each column
summary_stats = {}
for i, dataset in enumerate(datasets):
    means = np.mean(dataset, axis=0)
    sds = np.std(dataset, axis=0)
    sums = np.sum(dataset, axis=0)
    averages = np.mean(dataset, axis=1)

    summary_stats[dataset_labels[i]] = {
        "Mean": means,
        "SD": sds,
        "Sum": sums,
        "Average": averages
    }

# Step 3: Visualize the Mean and Sum for each dataset
for label, stats in summary_stats.items():
    # Mean Visualization
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Mean"])), stats["Mean"], color='skyblue', label="Mean")
    plt.xlabel('Columns')
    plt.ylabel('Mean Values')
    plt.title(f'Mean for {label}')
    plt.legend()
    plt.show()

    # Sum Visualization
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Sum"])), stats["Sum"], color='orange', label="Sum")
    plt.xlabel('Columns')
    plt.ylabel('Sum Values')
    plt.title(f'Sum for {label}')
    plt.legend()
    plt.show()

    # Average Visualization (per subject)
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Average"])), stats["Average"], color='green', label="Average per Subject")
    plt.xlabel('Subjects')
    plt.ylabel('Average Values')
    plt.title(f'Average per Subject in {label}')
    plt.legend()
    plt.show()
# Step 4: Compare Mean and Sum between subjects in each dataset
for label, stats in summary_stats.items():
    # Mean comparison
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Average"])), stats["Average"], color='green', label="Average per Subject")
    plt.xlabel('Subjects')
    plt.ylabel('Average Values')
    plt.title(f'Average in {label}')
    plt.legend()
    plt.show()

# Step 5: Statistical Comparison between Datasets
print("ANOVA Results (Mean Comparison):")
mean_anova_results = f_oneway(
    summary_stats["videodata"]["Mean"],
    summary_stats["videodata1"]["Mean"],
    summary_stats["videodata2"]["Mean"],
    summary_stats["videodata3"]["Mean"]
)
print(f"F-statistic: {mean_anova_results.statistic:.4f}, P-value: {mean_anova_results.pvalue:.4f}")

print("\nANOVA Results (Sum Comparison):")
sum_anova_results = f_oneway(
    summary_stats["videodata"]["Sum"],
    summary_stats["videodata1"]["Sum"],
    summary_stats["videodata2"]["Sum"],
    summary_stats["videodata3"]["Sum"]
)
print(f"F-statistic: {sum_anova_results.statistic:.4f}, P-value: {sum_anova_results.pvalue:.4f}")

# Pairwise Comparison for Means and Sums
print("\nPairwise T-tests (Mean Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Mean"],
            summary_stats[dataset_labels[j]]["Mean"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")

print("\nANOVA Results (Average Comparison):")
average_anova_results = f_oneway(
    summary_stats["videodata"]["Average"],
    summary_stats["videodata1"]["Average"],
    summary_stats["videodata2"]["Average"],
    summary_stats["videodata3"]["Average"]
)
print(f"F-statistic: {average_anova_results.statistic:.4f}, P-value: {average_anova_results.pvalue:.4f}")

print("\nPairwise T-tests (Sum Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Sum"],
            summary_stats[dataset_labels[j]]["Sum"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")
# Pairwise Comparison for Averages
print("\nPairwise T-tests (Average Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Average"],
            summary_stats[dataset_labels[j]]["Average"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")

# Ensure all datasets have the same number of rows
num_subjects = videodata.shape[0]
if any(dataset.shape[0] != num_subjects for dataset in [videodata1, videodata2, videodata3]):
    raise ValueError("All video datasets must have the same number of subjects (rows).")

# Calculate the sum for each subject in each video
sums_video1 = np.sum(videodata, axis=1)
sums_video2 = np.sum(videodata1, axis=1)
sums_video3 = np.sum(videodata2, axis=1)
sums_video4 = np.sum(videodata3, axis=1)

# Create a DataFrame to store the results
subjects = [f"Subject_{i+1}" for i in range(num_subjects)]  # Generate subject labels

sum_data = pd.DataFrame({
    "Subject": subjects,
    "Video1_Sum": sums_video1,
    "Video2_Sum": sums_video2,
    "Video3_Sum": sums_video3,
    "Video4_Sum": sums_video4
})

# Save the data to a CSV file
output_file = "subject_sums.csv"
sum_data.to_csv(output_file, index=False)

# Calculate correlation matrices using different methods
correlation_matrix_pearson = sum_data.drop("Subject", axis=1).corr(method="pearson")
correlation_matrix_spearman = sum_data.drop("Subject", axis=1).corr(method="spearman")
correlation_matrix_kendall = sum_data.drop("Subject", axis=1).corr(method="kendall")

# Print correlation matrices
print("Pearson Correlation Matrix:")
print(correlation_matrix_pearson)
print("\nSpearman Correlation Matrix:")
print(correlation_matrix_spearman)
print("\nKendall Correlation Matrix:")
print(correlation_matrix_kendall)

# Visualize the correlation matrices
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_pearson, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Pearson Correlation Matrix Between Videos (Based on Sums)")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_spearman, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Spearman Correlation Matrix Between Videos (Based on Sums)")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_kendall, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Kendall Correlation Matrix Between Videos (Based on Sums)")
plt.show()

# Determine the best model (pair with the highest correlation in Pearson)
best_pair_pearson = None
max_correlation_pearson = -1

for i, col1 in enumerate(correlation_matrix_pearson.columns):
    for j, col2 in enumerate(correlation_matrix_pearson.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_pearson.loc[col1, col2]
            if correlation > max_correlation_pearson:
                max_correlation_pearson = correlation
                best_pair_pearson = (col1, col2)

print(f"Best Correlated Pair (Pearson): {best_pair_pearson} with Correlation: {max_correlation_pearson:.2f}")

# Determine the best model (pair with the highest correlation in Spearman)
best_pair_spearman = None
max_correlation_spearman = -1

for i, col1 in enumerate(correlation_matrix_spearman.columns):
    for j, col2 in enumerate(correlation_matrix_spearman.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_spearman.loc[col1, col2]
            if correlation > max_correlation_spearman:
                max_correlation_spearman = correlation
                best_pair_spearman = (col1, col2)

print(f"Best Correlated Pair (Spearman): {best_pair_spearman} with Correlation: {max_correlation_spearman:.2f}")

# Determine the best model (pair with the highest correlation in Kendall)
best_pair_kendall = None
max_correlation_kendall = -1

for i, col1 in enumerate(correlation_matrix_kendall.columns):
    for j, col2 in enumerate(correlation_matrix_kendall.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_kendall.loc[col1, col2]
            if correlation > max_correlation_kendall:
                max_correlation_kendall = correlation
                best_pair_kendall = (col1, col2)

print(f"Best Correlated Pair (Kendall): {best_pair_kendall} with Correlation: {max_correlation_kendall:.2f}")

print(f"Sum for each subject saved to {output_file}")

"""NDARYE850ZVD"""

videodata= np.load("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARYE850ZVD/evaluation/baseline_eLORETA.npy")  #base relax
videodata1= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARYE850ZVD/evaluation/video1_eLORETA.npy") #marowa's drive video1
videodata2= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARYE850ZVD/evaluation/video2_eLORETA.npy") #video2
videodata3= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/Subject  Data 2/NDARYE850ZVD/evaluation/video3_eLORETA.npy") #video3
#luca's drive
#pathLuca = "/content/drive/MyDrive/TU Dublin-NN/research/PHD CANDIDATES - AICL research LAB/CURRENT/Carlos Gomez Tapia - PB04482/dataset - full source activation space (31000x11250) for 35 subjects/"
#fileName = "video1_eLORETA.npy"
#videodata= np.load (pathLuca + fileName)
print(videodata1.shape)
print(len(videodata)) #len of points in brain (31553)
print(videodata[0])
print(len(videodata[0]))  #timestamps  (11250=125hz x 90 seconds)
print(type(videodata))
#video2data is a list of list (first list 31553 lenghth,each of this cell contains 11250 point,31553 is represented the number of the brain and the 11250 is the point in time  )
import numpy as np
import pandas as pd
from scipy.stats import f_oneway, ttest_ind
import matplotlib.pyplot as plt


datasets = [videodata, videodata1, videodata2, videodata3]
dataset_labels = ["videodata", "videodata1", "videodata2", "videodata3"]

# Step 2: Calculate Mean, SD, Sum, and Average for each column
summary_stats = {}
for i, dataset in enumerate(datasets):
    means = np.mean(dataset, axis=0)
    sds = np.std(dataset, axis=0)
    sums = np.sum(dataset, axis=0)
    averages = np.mean(dataset, axis=1)

    summary_stats[dataset_labels[i]] = {
        "Mean": means,
        "SD": sds,
        "Sum": sums,
        "Average": averages
    }

# Step 3: Visualize the Mean and Sum for each dataset
for label, stats in summary_stats.items():
    # Mean Visualization
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Mean"])), stats["Mean"], color='skyblue', label="Mean")
    plt.xlabel('Columns')
    plt.ylabel('Mean Values')
    plt.title(f'Mean for {label}')
    plt.legend()
    plt.show()

    # Sum Visualization
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Sum"])), stats["Sum"], color='orange', label="Sum")
    plt.xlabel('Columns')
    plt.ylabel('Sum Values')
    plt.title(f'Sum for {label}')
    plt.legend()
    plt.show()

    # Average Visualization (per subject)
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Average"])), stats["Average"], color='green', label="Average per Subject")
    plt.xlabel('Subjects')
    plt.ylabel('Average Values')
    plt.title(f'Average per Subject in {label}')
    plt.legend()
    plt.show()
# Step 4: Compare Mean and Sum between subjects in each dataset
for label, stats in summary_stats.items():
    # Mean comparison
    plt.figure(figsize=(10, 4))
    plt.bar(range(len(stats["Average"])), stats["Average"], color='green', label="Average per Subject")
    plt.xlabel('Subjects')
    plt.ylabel('Average Values')
    plt.title(f'Average in {label}')
    plt.legend()
    plt.show()

# Step 5: Statistical Comparison between Datasets
print("ANOVA Results (Mean Comparison):")
mean_anova_results = f_oneway(
    summary_stats["videodata"]["Mean"],
    summary_stats["videodata1"]["Mean"],
    summary_stats["videodata2"]["Mean"],
    summary_stats["videodata3"]["Mean"]
)
print(f"F-statistic: {mean_anova_results.statistic:.4f}, P-value: {mean_anova_results.pvalue:.4f}")

print("\nANOVA Results (Sum Comparison):")
sum_anova_results = f_oneway(
    summary_stats["videodata"]["Sum"],
    summary_stats["videodata1"]["Sum"],
    summary_stats["videodata2"]["Sum"],
    summary_stats["videodata3"]["Sum"]
)
print(f"F-statistic: {sum_anova_results.statistic:.4f}, P-value: {sum_anova_results.pvalue:.4f}")

# Pairwise Comparison for Means and Sums
print("\nPairwise T-tests (Mean Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Mean"],
            summary_stats[dataset_labels[j]]["Mean"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")

print("\nANOVA Results (Average Comparison):")
average_anova_results = f_oneway(
    summary_stats["videodata"]["Average"],
    summary_stats["videodata1"]["Average"],
    summary_stats["videodata2"]["Average"],
    summary_stats["videodata3"]["Average"]
)
print(f"F-statistic: {average_anova_results.statistic:.4f}, P-value: {average_anova_results.pvalue:.4f}")

print("\nPairwise T-tests (Sum Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Sum"],
            summary_stats[dataset_labels[j]]["Sum"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")
# Pairwise Comparison for Averages
print("\nPairwise T-tests (Average Comparison):")
for i in range(len(datasets)):
    for j in range(i + 1, len(datasets)):
        t_stat, t_pval = ttest_ind(
            summary_stats[dataset_labels[i]]["Average"],
            summary_stats[dataset_labels[j]]["Average"]
        )
        print(f"{dataset_labels[i]} vs {dataset_labels[j]}: t-stat={t_stat:.4f}, p-value={t_pval:.4f}")

# Ensure all datasets have the same number of rows
num_subjects = videodata.shape[0]
if any(dataset.shape[0] != num_subjects for dataset in [videodata1, videodata2, videodata3]):
    raise ValueError("All video datasets must have the same number of subjects (rows).")

# Calculate the sum for each subject in each video
sums_video1 = np.sum(videodata, axis=1)
sums_video2 = np.sum(videodata1, axis=1)
sums_video3 = np.sum(videodata2, axis=1)
sums_video4 = np.sum(videodata3, axis=1)

# Create a DataFrame to store the results
subjects = [f"Subject_{i+1}" for i in range(num_subjects)]  # Generate subject labels

sum_data = pd.DataFrame({
    "Subject": subjects,
    "Video1_Sum": sums_video1,
    "Video2_Sum": sums_video2,
    "Video3_Sum": sums_video3,
    "Video4_Sum": sums_video4
})

# Save the data to a CSV file
output_file = "subject_sums.csv"
sum_data.to_csv(output_file, index=False)

# Calculate correlation matrices using different methods
correlation_matrix_pearson = sum_data.drop("Subject", axis=1).corr(method="pearson")
correlation_matrix_spearman = sum_data.drop("Subject", axis=1).corr(method="spearman")
correlation_matrix_kendall = sum_data.drop("Subject", axis=1).corr(method="kendall")

# Print correlation matrices
print("Pearson Correlation Matrix:")
print(correlation_matrix_pearson)
print("\nSpearman Correlation Matrix:")
print(correlation_matrix_spearman)
print("\nKendall Correlation Matrix:")
print(correlation_matrix_kendall)

# Visualize the correlation matrices
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_pearson, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Pearson Correlation Matrix Between Videos (Based on Sums)")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_spearman, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Spearman Correlation Matrix Between Videos (Based on Sums)")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_kendall, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Kendall Correlation Matrix Between Videos (Based on Sums)")
plt.show()

# Determine the best model (pair with the highest correlation in Pearson)
best_pair_pearson = None
max_correlation_pearson = -1

for i, col1 in enumerate(correlation_matrix_pearson.columns):
    for j, col2 in enumerate(correlation_matrix_pearson.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_pearson.loc[col1, col2]
            if correlation > max_correlation_pearson:
                max_correlation_pearson = correlation
                best_pair_pearson = (col1, col2)

print(f"Best Correlated Pair (Pearson): {best_pair_pearson} with Correlation: {max_correlation_pearson:.2f}")

# Determine the best model (pair with the highest correlation in Spearman)
best_pair_spearman = None
max_correlation_spearman = -1

for i, col1 in enumerate(correlation_matrix_spearman.columns):
    for j, col2 in enumerate(correlation_matrix_spearman.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_spearman.loc[col1, col2]
            if correlation > max_correlation_spearman:
                max_correlation_spearman = correlation
                best_pair_spearman = (col1, col2)

print(f"Best Correlated Pair (Spearman): {best_pair_spearman} with Correlation: {max_correlation_spearman:.2f}")

# Determine the best model (pair with the highest correlation in Kendall)
best_pair_kendall = None
max_correlation_kendall = -1

for i, col1 in enumerate(correlation_matrix_kendall.columns):
    for j, col2 in enumerate(correlation_matrix_kendall.columns):
        if i < j:  # Avoid duplicates and diagonal
            correlation = correlation_matrix_kendall.loc[col1, col2]
            if correlation > max_correlation_kendall:
                max_correlation_kendall = correlation
                best_pair_kendall = (col1, col2)

print(f"Best Correlated Pair (Kendall): {best_pair_kendall} with Correlation: {max_correlation_kendall:.2f}")

print(f"Sum for each subject saved to {output_file}")

"""# VISUALIZE THE FIRST FIVE BRAIN FROM RESOURCE"""

# Creating a dictionary

my_dict = {
    'Resource CubeID': [1,1,1,1,2,3,4,4,4,4,4,4,4,4,4,4,4,5,5,5,5,5,5,5
                 ,5,5,5,5,5,6,6,7,7,7,7,8,9,10,10,10,10,10,10,10,
                 10,10,10,10,11,11,11,11,11,11,11,11,11,11,11,11,12,12],
    'BrainRegion': [54,57,85,94,96,64,61,63,65,69,74,75,79,84
                    ,87,98,102,52,58,59,66,67,73,81,83,86,89,93,
                    95,60,82,3,6,34,43,45,13,10,12,14,18,23,24,28,
                    33,36,47,51,1,7,8,15,16,22,30,32,35,38,42,44,9,31],
    'Mindboggle ID': [2009,2021,2011,2005,2030,2022,2008,2013,2034,
                      2016,2035,2007,2015,2010,2006,2023,2031,2027,
                      2012,2026,2014,2017,2020,2002,2018,2024,2028,
                      2003,2019,2029,2025,1009,1021,1011,1005,1030,
                      1022,1008,1013,1034,1016,1035,1007,1015,1010,1006,1023,
                      1031,1027,1012,1026,1014,1017,1020,1002,1018,1024,1028,1003,
                      1019,1029,1025]
}
for key, value in my_dict.items():
    print(key, value)
# To print the values associated with the key "Resource", do the following:
print(my_dict['Resource CubeID'])

# To print the values associated with the key "Mindboggle", do the following:
print(my_dict['BrainRegion'])

# To print the values associated with the key "Cube ID", do the following:
print(my_dict['Mindboggle ID'])
print (len(my_dict['BrainRegion']))
print (len(my_dict['Resource CubeID']))
print (len(my_dict['Mindboggle ID']))
print (len(my_dict))

#Read CSV with label
CerebrLabel = pd.read_csv("/content/drive/MyDrive/Data TU PHD DUBLIN/LABEL DETAILS/CerebrA_LabelDetails.csv")
#CerebrLabel

!pip install nibabel
import nibabel as nib
import matplotlib.pyplot as plt
import os

# Get the absolute path to the file (replace with your actual file path)
file_path = '/content/drive/MyDrive/Data TU PHD DUBLIN/LABEL DETAILS/CerebrA_in_t1.mgz' #This can be relative or abosulte path.

# Load the .mgz file
img = nib.load('/content/drive/MyDrive/Data TU PHD DUBLIN/LABEL DETAILS/CerebrA_in_t1.mgz')
# Get the data as a numpy array
data = img.get_fdata()

# Display basic information about the image
print(f"Data shape: {data.shape}")
print(f"Data type: {data.dtype}")
# Plot a slice of the image
plt.imshow(data[:, :, data.shape[2] // 2], cmap='gray')
plt.title('Middle Slice')
plt.show()

"""download the cereber a backage from Github"""

print(videodata.shape)
print(videodata[0])
print(len(videodata[0]))  #timestamps  (11250=125hz x 90 seconds)

import numpy as np
from Cerebra_aatlas_python.cerebra_atlas_python.cerebra import CerebrA
cerebra = CerebrA()
cerebra.orthoview()

cerebra.src_space_labels
print(cerebra.src_space_labels.shape)

import numpy as np
np.save('map_voxel.npy', cerebra.src_space_labels) #This command saves the src_space_labels attribute from the cerebra object to a file named map_voxel.npy in NumPy's binary format.

cerebra.src_space_labels[0]

cerebra._label_details

#from Cerebra_aatlas_python import CerebrA
#!pip install cerebra-atlas-python --upgrade #Upgrade the library
#from Cerebra_aatlas_python.cerebra_atlas_python.cerebra import CerebrA #Import CerebrA correctly
import matplotlib.pyplot as plt
n_points = 31553
colors = np.random.rand(n_points, 3)
cerebra = CerebrA()

colors = [[0.0, 0.0, 1.0]] * len(cerebra.src_space_labels)
cerebra.plot3d(colors=colors)
plt.show()

!pip install cerebra-atlas-python --upgrade
#from cerebra_atlas_python.cerebra_atlas_python.cerebra import CerebrA
from Cerebra_aatlas_python.cerebra_atlas_python.plotting.cerebra_o3d import create_plot
import numpy as np
import matplotlib.pyplot as plt

#cerebra = create_plot()
print(f"CerebrA: {cerebra.src_space_labels} {cerebra.src_space_labels.shape=}")
print(cerebra._label_details)

colors = [[0.0, 0.0, 1.0]] * len(cerebra.src_space_labels)
print(f"{colors=}")
cerebra.plot3d (colors=colors)

plt.show()

import matplotlib.pyplot as plt
from Cerebra_aatlas_python.cerebra_atlas_python.plotting.plotting_3d import  Plots3D

cerebra = Plots3D()
cerebra.plot_data_3d(colors=colors)
plt.show()

from cerebra_atlas_python.cerebra_atlas_python import CerebrA
import matplotlib.pyplot as plt
cerebra = CerebrA()
colors = [[0.0, 0.0, 1.0]] * len(cerebra.src_space_labels)
cerebra.plot3d(colors=colors)
plt.show()

import matplotlib.pyplot as plt # Import matplotlib.pyplot
videodata= np.load ("/content/drive/MyDrive/Data TU PHD DUBLIN/First data/video1_eLORETA.npy")#marowa's drive
meanOfVideoData= np.mean(videodata ,axis=0 )#mean for whole data
plt.plot(meanOfVideoData)
plt.xlabel('X Axis')
plt.ylabel('Y Axis')
plt.title('Mean of video data')
plt.show()

"""## For each region, I generated line plots showing the mean measures over time for all voxels within that region. The X-axis represents voxel indices, and the Y-axis represents the mean measure values. Visualized the data for each region as line plots showing voxel-wise measures."""

import pandas as pd
import matplotlib.pyplot as plt
map_voxel = np.load("/content/drive/MyDrive/Data TU PHD DUBLIN/LABEL DETAILS/map_voxel.npy")  # Shape: (31553,)
videodata = pd.read_csv("/content/drive/MyDrive/Data TU PHD DUBLIN/LABEL DETAILS/CerebrA_LabelDetails.csv")
# Create a DataFrame to map indices to regions
index_to_region = pd.DataFrame({
    "Index": range(len(map_voxel)),
    "Region": map_voxel
})

#sorted_index_to_region = index_to_region.sort_values(by="Region")
# Calculate the average of indices per region
average_indices_per_region = index_to_region.groupby("Region")["Index"].mean().reset_index()

# Save or display the results
average_indices_per_region.to_csv("average_indices_per_region.csv", index=False)  # Save to CSV
print(average_indices_per_region.head())
print(len(average_indices_per_region))

# Scatter plot for all regions
plt.figure(figsize=(12, 6))
plt.scatter(average_indices_per_region["Region"], average_indices_per_region["Index"], alpha=0.7, edgecolor="k")
plt.title("Average Index Per Region (All Regions)", fontsize=14)
plt.xlabel("Region", fontsize=12)
plt.ylabel("Average Index", fontsize=12)
plt.grid(True, linestyle="--", alpha=0.6)
plt.show()

# Scatter plots for individual regions
#for region in average_indices_per_region["Region"].unique():
 #   region_data = average_indices_per_region[average_indices_per_region["Region"] == region]
 #   plt.figure(figsize=(6, 4))
 #  plt.scatter(region_data["Region"], region_data["Index"], alpha=0.7, color="blue", edgecolor="k")
#  plt.title(f"Average Index for Region {region}", fontsize=14)
 #   plt.xlabel("Region", fontsize=12)
 #  plt.ylabel("Average Index", fontsize=12)
#  plt.grid(True, linestyle="--", alpha=0.6)
#  plt.show()
# Calculate the average of indices per region

average_indices_per_region = index_to_region.groupby("Region")["Index"].mean().reset_index()

# Create a pivot table for heat map visualization
heatmap_data = average_indices_per_region.pivot_table(
    values="Index",  # Value to plot
    index="Region",  # Rows
    aggfunc=np.mean  # Aggregation function
)

#  heat map using seaborn
plt.figure(figsize=(10, 8))
sns.heatmap(heatmap_data, cmap="coolwarm", annot=True, fmt=".2f", cbar=True)

plt.title("Heat Map of Average Index Per Region", fontsize=14)
plt.xlabel("Region", fontsize=12)
plt.ylabel("Region ID", fontsize=12)

# Show the plot
plt.show()

# prompt: Transform the data to dataframe to see it and how is it,,

# Assuming 'videodata' is your NumPy array and 'map_voxel' is loaded as before
videodata = np.load("/content/drive/MyDrive/Data TU PHD DUBLIN/First data/video1_eLORETA.npy")
map_voxel = np.load("/content/drive/MyDrive/Data TU PHD DUBLIN/LABEL DETAILS/map_voxel.npy")

# Create a DataFrame
df = pd.DataFrame(videodata)

# Add a column for brain regions
df['BrainRegion'] = map_voxel

# Display the DataFrame
print(df.head())
df.shape

"""# Visualise the mean/ RMS of each 31553 TO  102/62 regions according to the time

Transfer the 31553 to 102 region according to # Compute mean over time for each region

---
"""

videodata=videodata.T
# Debugging CSV column names
print("Region names CSV columns:", region_data.columns)

if "Cerebra_ID" not in region_data.columns or "Region_name" not in region_data.columns:
    raise ValueError("CSV file must contain 'Cerebra_ID' and 'Region_name' columns.")

# Create a dictionary for region ID to region name mapping
region_id_to_name = region_data.set_index("Cerebra_ID")["Region_name"].to_dict()

# Compute mean over time for each region
unique_regions = np.unique(map_voxel)
region_time_means = {}

for region in unique_regions:
    # Get indices for all voxels in the region
    region_indices = map_voxel == region
    # Compute the mean over time for voxels in this region
    region_time_means[region] = np.mean(videodata[:, region_indices], axis=1)

# Plot mean over time for each region
for region, time_means in region_time_means.items():
    region_name = region_id_to_name.get(region, f"Unknown Region {region}")  # Handle missing names
    plt.figure(figsize=(10, 5))
    plt.plot(
        range(len(time_means)),  # X-axis: Time steps
        time_means,             # Y-axis: Mean values over time
        marker='o', linestyle='-', linewidth=1.5, alpha=0.8, label=f"{region_name}"
    )
    plt.xlabel("Time Step")
    plt.ylabel("Mean Measure")
    plt.title(f"Mean Measures Over Time for {region_name} ({region})")
    plt.grid(axis="y", linestyle="--", alpha=0.6)
    plt.legend()
    plt.tight_layout()
    plt.show()

"""Transfer the 31553 to 102 region according to RMS for each columns  where aggregrate the RMS according to region map voxel"""

videodata=videodata.T
# Debugging CSV column names
#  print CSV column names to verify expected structure
print("Region names CSV columns:", region_data.columns)

if "Cerebra_ID" not in region_data.columns or "Region_name" not in region_data.columns:
    raise ValueError("CSV file must contain 'Cerebra_ID' and 'Region_name' columns.")

# Create a dictionary mapping from Cerebra_ID to Region_name
region_id_to_name = region_data.set_index("Cerebra_ID")["Region_name"].to_dict()

# --------------------- Compute RMS for Each Voxel ---------------------
# Each column in videodata represents a time series for one voxel.
# Compute the RMS (Root Mean Square) for each voxel over time.
voxel_rms = np.sqrt(np.mean(videodata**2, axis=0))
print(f"Computed RMS for {len(voxel_rms)} voxels.")
print(sum(voxel_rms))

# --------------------- Group and Sort RMS Values by Region ---------------------
unique_regions = np.unique(map_voxel)
region_rms_dict = {}

for region in unique_regions:
    # Get indices for voxels that belong to the current region
    region_indices = np.where(map_voxel == region)[0]
    # Extract the RMS values for these voxels
    rms_values = voxel_rms[region_indices]
    # Sort the RMS values in ascending order
    sorted_rms = np.sort(rms_values)
    region_rms_dict[region] = sorted_rms
    region_name = region_id_to_name.get(region, f"Unknown Region {region}")
    print(f"Region '{region}' ({region_id_to_name.get(region, 'Unknown')}): "
          f"{len(sorted_rms)} voxels with RMS values ranging from {sorted_rms[0]:.2f} to {sorted_rms[-1]:.2f}")

# --------------------- Aggregate All RMS Values into One Table ---------------------
# We'll create a table with two columns:
#   "x": the region name
#   "y": the RMS value for a given voxel
data_rows = []
for region, sorted_rms in region_rms_dict.items():
    region_name = region_id_to_name.get(region, f"Unknown Region {region}")
    for rms in sorted_rms:
        data_rows.append({"x": region_name, "y": rms})

rms_table = pd.DataFrame(data_rows)
print("\nAggregated RMS Table (first few rows):")
print(rms_table.head())

# Optionally, save the aggregated table to a CSV file
rms_table.to_csv("aggregated_rms_table.csv", index=False)

# --------------------- Plot the Aggregated RMS Values ---------------------
# Plot 1: Box Plot (shows distribution per region)
plt.figure(figsize=(16, 6))
sns.boxplot(x="x", y="y", data=rms_table)
plt.xlabel("Region")
plt.ylabel("RMS Value")
plt.title("Distribution of RMS Values by Region")
plt.xticks(rotation=45)
plt.grid(axis="y", linestyle="--", alpha=0.6)
plt.tight_layout()
plt.show()

# Plot 2: Strip Plot (shows individual RMS values)
plt.figure(figsize=(16, 6))
sns.stripplot(x="x", y="y", data=rms_table, jitter=True, size=4)
plt.xlabel("Region")
plt.ylabel("RMS Value")
plt.title("Aggregated RMS Values by Region")
plt.xticks(rotation=45)
plt.grid(axis="y", linestyle="--", alpha=0.6)
plt.tight_layout()
plt.show()

"""✅ Sort 31,553 points into regions based on Cerebra ID
✅ Compute RMS per time activation for each region
✅ Visualize each region's RMS separately and plot them
"""

# --------------------- Execution ---------------------
while True:
    subject_id = input("Enter Subject ID (or type 'exit' to quit): ").strip()
    if subject_id.lower() == "exit":
        break

    print("\n🔹 Step 1: Load Video Data 🔹")
    videodata = load_subject_video_data1()

    print("\n🔹 Step 2: Load Region Mapping Data (CSV) 🔹")
    region_data = load_region_data1()

    map_voxel = np.random.randint(1, 103, size=31335)  # Simulated voxel-to-region mapping

    try:
        avg_rms_time_series, region_name_map = compute_rms_time_series2(videodata, map_voxel, region_data)
        print("\n✅ RMS Computation and Sorting Complete.")
        plot_rms_time_series2(avg_rms_time_series, region_name_map)
    except (KeyError, FileNotFoundError, ValueError) as e:
        print(f"❌ Error: {e}")

    print("\n🔄 Execution completed. Restarting for a new subject.\n")

"""✅ Sort 31,553 points into regions based on Cerebra ID ✅ Compute RMS per time activation for each region,  sort   according to  the cerebra id to the cube id (Multiple resource theory ID) calculate the region RMS' average and  and connect the Region_name to MRT ID Name from csv file., I want to visualize as time series the MRT where the plot title has the name of all regions which is sorted to same the MRT"""

# --------------------- Execution ---------------------
while True:
    subject_id = input("Enter Subject ID (or type 'exit' to quit): ").strip()
    if subject_id.lower() == "exit":
        break

    video_type = input("Enter Video Type (baseline_eLORETA, video1_eLORETA, video2_eLORETA, video3_eLORETA): ").strip()
    if video_type not in ["baseline_eLORETA", "video1_eLORETA", "video2_eLORETA", "video3_eLORETA"]:
        print("❌ Invalid video type. Please enter a valid option.")
        continue

    videodata = load_subject_video_data(subject_id, video_type)
    map_voxel = np.random.randint(1, 103, size=31553)
    region_data = pd.read_csv("/content/drive/MyDrive/Data TU PHD DUBLIN/LABEL DETAILS/results_per_region (MY 101124).csv")

    try:
        avg_rms_per_mrt, mrt_regions_map, mrt_name_map = compute_rms_time_series3(videodata, map_voxel, region_data)
        print("RMS Computation and Sorting Complete.")
        plot_avg_mrt_time_series3(avg_rms_per_mrt, mrt_regions_map, mrt_name_map, subject_id, video_type)
    except (KeyError, FileNotFoundError, ValueError) as e:
        print(f"❌ Error: {e}")

    print("\nExecution completed. Restarting for a new subject.")

"""    Compute the RMS for each region along all 11250 time activations,
    sort each region to its corresponding MRT based on the CSV file,
    compute the average RMS per MRT, and prepare data for visualization.
    '''
"""

def load_video_data():
    """
    Manually enter the file path to load brain activation data.
    """
    file_path = input("Enter the full file path for the video data (.npy file): ").strip()
    if not os.path.exists(file_path):
        print(f"❌ Warning: File not found at {file_path}. Returning dummy data.")
        return np.random.randn(31335, 11250)  # Dummy fallback data
    return np.load(file_path)

def load_csv_data():
    """
    Manually enter the file path to load the CSV file.
    """
    file_path = input("Enter the full file path for the CSV file: ").strip()
    if not os.path.exists(file_path):
        print(f"❌ Warning: File not found at {file_path}. Please enter a valid path.")
        return None
    return pd.read_csv(file_path)

def compute_rms_time_series3(videodata, map_voxel, region_data):
    """
    Compute the RMS for each region along all 11250 time activations,
    sort each region to its corresponding MRT based on the CSV file,
    compute the average RMS per MRT, and prepare data for visualization.
    """
    required_columns = {"Cerebra_ID", "Region_name", "Multiple resource theory ID", "MRT ID Name"}
    missing_columns = required_columns - set(region_data.columns)
    if missing_columns:
        raise ValueError(f"CSV file must contain the following columns: {missing_columns}")

    valid_cerebra_ids = set(region_data["Cerebra_ID"].unique())
    region_name_map = region_data.set_index("Cerebra_ID")["Region_name"].to_dict()
    mrt_id_map = region_data.set_index("Cerebra_ID")["Multiple resource theory ID"].to_dict()
    mrt_name_map = region_data.set_index("Multiple resource theory ID")["MRT ID Name"].to_dict()

    mrt_regions_map = {}
    rms_per_mrt = {}
    avg_rms_per_mrt = {}

    for cerebra_id, mrt_id in mrt_id_map.items():
        region_name = region_name_map.get(cerebra_id, f"Region {cerebra_id}")
        if mrt_id not in mrt_regions_map:
            mrt_regions_map[mrt_id] = []
        mrt_regions_map[mrt_id].append(region_name)

    rms_per_region = {cerebra_id: np.sqrt(np.mean(videodata[map_voxel == cerebra_id]**2, axis=0)) for cerebra_id in valid_cerebra_ids}

    for cerebra_id, mrt_id in mrt_id_map.items():
        if mrt_id not in rms_per_mrt:
            rms_per_mrt[mrt_id] = []
        rms_per_mrt[mrt_id].append(rms_per_region.get(cerebra_id, np.zeros(11250)))

    for mrt_id, rms_values_list in rms_per_mrt.items():
        avg_rms_per_mrt[mrt_id] = np.mean(rms_values_list, axis=0)

    mwl_index = np.sum(list(avg_rms_per_mrt.values()), axis=0)

    return avg_rms_per_mrt, mwl_index, mrt_regions_map, mrt_name_map

def plot_mrt_time_series3(rms_per_mrt, mwl_index, mrt_regions_map, mrt_name_map):
    """
    Plot the average time series for each MRT ID separately in red,
    and plot the overall average RMS in green.
    """
    for mrt_id, avg_rms_values in rms_per_mrt.items():
        mrt_name = mrt_name_map.get(mrt_id, "Unknown MRT Name")
        regions_str = ", ".join(mrt_regions_map.get(mrt_id, []))

        plt.figure(figsize=(16, 5))
        plt.plot(range(11250), avg_rms_values, color='red', label=f"{mrt_name} (MRT ID {mrt_id})")
        plt.xlabel("Time (0 - 11250)")
        plt.ylabel("Average RMS Value")
        plt.title(f"Average RMS for {mrt_name} (MRT ID {mrt_id})\nRegions: {regions_str}")
        plt.legend(loc='upper right', fontsize=8)
        plt.grid(axis="y", linestyle="--", alpha=0.6)
        plt.show()

    plt.figure(figsize=(16, 5))
    plt.plot(range(11250), mwl_index, color='green', label="MWL Index ")
    plt.xlabel("Time (0 - 11250)")
    plt.ylabel("Sum of Averages of RMS of cortical regions")
    plt.title("Mental Workload Index")
    plt.legend()
    plt.grid(axis="y", linestyle="--", alpha=0.6)
    plt.show()

# --------------------- Execution ---------------------
while True:
    videodata = load_video_data()
    region_data = load_csv_data()
    if region_data is None:
        continue  # Retry if no valid CSV file was provided

    map_voxel = np.random.randint(1, 103, size=31553)  # Simulated voxel-to-region mapping

    try:
        avg_rms_per_mrt, mwl_index, mrt_regions_map, mrt_name_map = compute_rms_time_series3(videodata, map_voxel, region_data)
        print("RMS Computation and Sorting Complete.")
        plot_mrt_time_series3(avg_rms_per_mrt, mwl_index, mrt_regions_map, mrt_name_map)
    except (KeyError, FileNotFoundError, ValueError) as e:
        print(f"❌ Error: {e}")

    cont = input("Do you want to process another dataset? (yes/no): ").strip().lower()
    if cont != 'yes':
        break

# @title Default title text
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

def load_video_data():
    """
    Manually enter the file path to load brain activation video data.
    Returns:
        np.ndarray: Loaded video data (if not found, dummy data is returned).
    """
    file_path = input("Enter the full file path for the video data (.npy file): ").strip()
    if not os.path.exists(file_path):
        print(f"❌ Warning: File not found at {file_path}. Returning dummy data.")
        return np.random.randn(31335, 11250)  # Dummy fallback data
    return np.load(file_path)

def load_csv_data():
    """
    Manually enter the file path to load the region mapping CSV file.
    Returns:
        pd.DataFrame: Loaded CSV data, or None if file is not found.
    """
    file_path = input("Enter the full file path for the CSV file: ").strip()
    if not os.path.exists(file_path):
        print(f"❌ Warning: File not found at {file_path}. Please enter a valid path.")
        return None
    return pd.read_csv(file_path)

def compute_rms_time_series3(videodata, map_voxel, region_data):
    """
    Sort the voxels to their cortical regions and then compute the RMS for each region along
    all 11250 time activations. Next, sort each region to its corresponding MRT based on the CSV file,
    compute the average RMS per MRT, and prepare data for visualization.

    Parameters:
        videodata (np.ndarray): Brain activation data with shape (voxels, time).
        map_voxel (np.ndarray): 1D array mapping each voxel to a cortical region (Cerebra_ID).
        region_data (pd.DataFrame): DataFrame with region mapping data. Required columns are:
            'Cerebra_ID', 'Region_name', 'Multiple resource theory ID', 'MRT ID Name'

    Returns:
        avg_rms_per_mrt (dict): Dictionary mapping each MRT ID to its average RMS time series.
        mwl_index (np.ndarray): Overall index (sum of all MRT time series) over time.
        mrt_regions_map (dict): Mapping from MRT ID to list of region names.
        mrt_name_map (dict): Mapping from MRT ID to MRT ID Name.
    """
    required_columns = {"Cerebra_ID", "Region_name", "Multiple resource theory ID", "MRT ID Name"}
    missing_columns = required_columns - set(region_data.columns)
    if missing_columns:
        raise ValueError(f"CSV file must contain the following columns: {missing_columns}")

    valid_cerebra_ids = set(region_data["Cerebra_ID"].unique())
    region_name_map = region_data.set_index("Cerebra_ID")["Region_name"].to_dict()
    mrt_id_map = region_data.set_index("Cerebra_ID")["Multiple resource theory ID"].to_dict()
    mrt_name_map = region_data.set_index("Multiple resource theory ID")["MRT ID Name"].to_dict()

    # Build mapping from MRT to its constituent regions
    mrt_regions_map = {}
    for cerebra_id, mrt_id in mrt_id_map.items():
        region_name = region_name_map.get(cerebra_id, f"Region {cerebra_id}")
        mrt_regions_map.setdefault(mrt_id, []).append(region_name)

    # Ensure the voxel mapping length matches the number of voxels in videodata.
    if len(map_voxel) != videodata.shape[0]:
        print(f"Warning: map_voxel length ({len(map_voxel)}) does not match videodata voxel count ({videodata.shape[0]}). Trimming map_voxel.")
        map_voxel = map_voxel[:videodata.shape[0]]

    # Compute RMS per cortical region (for each Cerebra_ID)
    # For each region, compute RMS over voxels (for each time point).
    rms_per_region = {
        cerebra_id: np.sqrt(np.mean(videodata[map_voxel == cerebra_id]**2, axis=0))
        for cerebra_id in valid_cerebra_ids
    }

    # Now, for each MRT, aggregate the RMS time series of its constituent regions.
    rms_per_mrt = {}
    for cerebra_id, mrt_id in mrt_id_map.items():
        rms_series = rms_per_region.get(cerebra_id, np.zeros(11250))
        rms_per_mrt.setdefault(mrt_id, []).append(rms_series)

    # Compute the average RMS time series for each MRT.
    avg_rms_per_mrt = {mrt_id: np.mean(rms_list, axis=0) for mrt_id, rms_list in rms_per_mrt.items()}

    # Compute overall Mental Workload Index (MWL Index) as the sum over MRT time series.
    mwl_index = np.sum(list(avg_rms_per_mrt.values()), axis=0)

    return avg_rms_per_mrt, mwl_index, mrt_regions_map, mrt_name_map

def plot_mrt_time_series3(avg_rms_per_mrt, mwl_index, mrt_regions_map, mrt_name_map):
    """
    Plot the average RMS time series for each MRT ID separately in red,
    and then plot the overall MWL Index in green.
    """
    # Plot each MRT time series
    for mrt_id, avg_rms_values in avg_rms_per_mrt.items():
        mrt_name = mrt_name_map.get(mrt_id, "Unknown MRT Name")
        regions_str = ", ".join(mrt_regions_map.get(mrt_id, []))

        plt.figure(figsize=(16, 5))
        plt.plot(range(11250), avg_rms_values, color='red', label=f"{mrt_name} (MRT ID {mrt_id})")
        plt.xlabel("Time (0 - 11250)")
        plt.ylabel("Average RMS Value")
        plt.title(f"Average RMS for {mrt_name} (MRT ID {mrt_id})\nRegions: {regions_str}")
        plt.legend(loc='upper right', fontsize=8)
        plt.grid(axis="y", linestyle="--", alpha=0.6)
        plt.tight_layout()
        plt.show()

    # Plot overall MWL Index
    plt.figure(figsize=(16, 5))
    plt.plot(range(11250), mwl_index, color='green', label="MWL Index")
    plt.xlabel("Time (0 - 11250)")
    plt.ylabel("Sum of Average RMS Values")
    plt.title("Mental Workload Index")
    plt.legend()
    plt.grid(axis="y", linestyle="--", alpha=0.6)
    plt.tight_layout()
    plt.show()

# --------------------- Execution ---------------------
while True:
    # Load video data
    videodata = load_video_data()
    # Load region mapping CSV
    region_data = load_csv_data()
    if region_data is None:
        continue  # Retry if no valid CSV file was provided

    # Simulated voxel-to-region mapping
    map_voxel = np.random.randint(1, 103, size=31553)

    try:
        avg_rms_per_mrt, mwl_index, mrt_regions_map, mrt_name_map = compute_rms_time_series3(videodata, map_voxel, region_data)
        print("RMS Computation and Sorting Complete.")
        plot_mrt_time_series3(avg_rms_per_mrt, mwl_index, mrt_regions_map, mrt_name_map)
    except (KeyError, FileNotFoundError, ValueError) as e:
        print(f"❌ Error: {e}")

    cont = input("Do you want to process another dataset? (yes/no): ").strip().lower()
    if cont != 'yes':
        break

print("\nThank you for using the program. Run ended.")

"""# to be deleted if not used

Transfer the 31553 to 102 region according to RMS where it had been calculated for each columns then aggregreate to Group by region name and sum the RMS values for each region.
"""

videodata=videodata.T
# Debugging CSV column names
#print CSV column names to verify expected structure
print("Region names CSV columns:", region_data.columns)

if "Cerebra_ID" not in region_data.columns or "Region_name" not in region_data.columns:
    raise ValueError("CSV file must contain 'Cerebra_ID' and 'Region_name' columns.")

# Create a dictionary mapping from Cerebra_ID to Region_name
region_id_to_name = region_data.set_index("Cerebra_ID")["Region_name"].to_dict()

# --------------------- Compute RMS for Each Voxel ---------------------
# Each column in videodata represents a time series for one voxel.
# Compute the RMS (Root Mean Square) for each voxel over time.
voxel_rms = np.sqrt(np.mean(videodata**2, axis=0))
print(f"Computed RMS for {len(voxel_rms)} voxels.")
print(sum(voxel_rms))
# --------------------- Group and Sort RMS Values by Region ---------------------
unique_regions = np.unique(map_voxel)
region_rms_dict = {}

for region in unique_regions:
    # Get indices for voxels that belong to the current region
    region_indices = np.where(map_voxel == region)[0]
    # Extract the RMS values for these voxels
    rms_values = voxel_rms[region_indices]
    # Sort the RMS values in ascending order (if desired)
    sorted_rms = np.sort(rms_values)
    region_rms_dict[region] = sorted_rms
    region_name = region_id_to_name.get(region, f"Unknown Region {region}")
    print(f"Region '{region}' ({region_name}): {len(sorted_rms)} voxels with RMS values ranging from {sorted_rms[0]:.2f} to {sorted_rms[-1]:.2f}")

# --------------------- Aggregate All RMS Values into One Table ---------------------
# Create a table with two columns:
#   "x": the region name
#   "y": the RMS value for a given voxel
data_rows = []
for region, sorted_rms in region_rms_dict.items():
    region_name = region_id_to_name.get(region, f"Unknown Region {region}")
    for rms in sorted_rms:
        data_rows.append({"x": region_name, "y": rms})

rms_table = pd.DataFrame(data_rows)
print("\nAggregated RMS Table (first few rows):")
print(rms_table.head())

# Optionally, save the aggregated table to a CSV file
rms_table.to_csv("aggregated_rms_table.csv", index=False)

# --------------------- Aggregate by Summing RMS for Each Region ---------------------
# Group the table by region name and sum the RMS values for each region.
aggregated_series = rms_table.groupby("x")["y"].sum().sort_values(ascending=False)
print("\nAggregated RMS Sum per Region:")
print(aggregated_series)

# --------------------- Plot the Aggregated RMS Sums ---------------------
plt.figure(figsize=(16, 6))
sns.barplot(x=aggregated_series.index, y=aggregated_series.values, palette="viridis")
plt.xlabel("Region")
plt.ylabel("Summed RMS Value")
plt.title("Summed RMS Values by Region")
plt.xticks(rotation=45)
plt.grid(axis="y", linestyle="--", alpha=0.6)
plt.tight_layout()
plt.show()



# Plot 2: Strip Plot (shows individual RMS values)
plt.figure(figsize=(16, 6))
sns.stripplot(x="x", y="y", data=rms_table, jitter=True, size=4)
plt.xlabel("Region")
plt.ylabel("Summed RMS Value")
plt.title("Summed RMS Values by Region")
plt.xticks(rotation=45)
plt.grid(axis="y", linestyle="--", alpha=0.6)
plt.tight_layout()
plt.show()

"""calculate the RMS after sorting each voxel to its regions 31553 to 102

"""

videodata=videodata.T
#  print column names to verify expected structure
print("Region names CSV columns:", region_data.columns)

if "Cerebra_ID" not in region_data.columns or "Region_name" not in region_data.columns:
    raise ValueError("CSV file must contain 'Cerebra_ID' and 'Region_name' columns.")

# Create a dictionary mapping Cerebra_ID to Region_name
region_id_to_name = region_data.set_index("Cerebra_ID")["Region_name"].to_dict()

# --------------------- Compute RMS Over Time for Each Region ---------------------

unique_regions = np.unique(map_voxel)
region_time_rms = {}

for region in unique_regions:
    # Get a boolean index for all voxels belonging to the current region
    region_indices = map_voxel == region

    # Select the data for these voxels.
    # videodata has shape (time, voxels) after transposing
    # So, videodata[:, region_indices] gives an array of shape (time, number_of_voxels_in_region)
    region_data = videodata[:, region_indices]

    # Compute RMS over time for each time point across the selected voxels.
    # For each time step (i.e., each row), calculate:
    #   rms = sqrt(mean(x^2))
    # This produces an array of shape (time,)
    rms_values = np.sqrt(np.mean(region_data**2, axis=1))

    region_time_rms[region] = rms_values

# --------------------- Plot RMS Over Time for Each Region ---------------------

for region, time_rms in region_time_rms.items():
    region_name = region_id_to_name.get(region, f"Unknown Region {region}")

    plt.figure(figsize=(10, 5))
    plt.plot(
        range(len(time_rms)),  # X-axis: Time steps
        time_rms,              # Y-axis: RMS values over time
        marker='o', linestyle='-', linewidth=1.5, alpha=0.8, label=f"{region_name}"
    )
    plt.xlabel("Time Step")
    plt.ylabel("RMS Value")
    plt.title(f"RMS Over Time for {region_name} (Region ID: {region})")
    plt.grid(axis="y", linestyle="--", alpha=0.6)
    plt.legend()
    plt.tight_layout()
    plt.show()

"""# Visualise the RMS of each 62 regions according to the time TO 12
 caluculate the RMS foe voxel
then group
 by Transformed Region to 12
"""

region_data = pd.read_csv("/content/drive/MyDrive/Data TU PHD DUBLIN/LABEL DETAILS/results_per_region (MY 101124).csv")
videodata=videodata.T
#if "Cerebra_ID" not in region_data.columns or "Multiple resource theory ID & Name" not in region_data.columns:
#   raise ValueError("CSV file must contain 'Cerebra_ID' and 'Multiple resource theory ID & Name' columns.")

# Create a dictionary mapping from the original 62 regions to the 12-region names/IDs
mapping_62_to_12 = region_data.set_index("Cerebra_ID")["Multiple resource theory ID & Name"].to_dict()

# Transform map_voxel to the new 12-region labels using vectorized mapping
transformed_map_voxel = np.vectorize(mapping_62_to_12.get)(map_voxel)

# --------------------- Compute RMS for Each Voxel ---------------------
# Each column of videodata corresponds to a voxel's time series.
# Calculate RMS (Root Mean Square) for each voxel:
#   RMS = sqrt(mean(x^2)) over the time dimension.
voxel_rms = np.sqrt(np.mean(videodata**2, axis=0))
print(f"Computed RMS for {len(voxel_rms)} voxels.")
print(sum(voxel_rms))
# --------------------- Group and Sort RMS Values by Transformed Region ---------------------
unique_regions = np.unique(transformed_map_voxel)
region_rms_dict = {}

for region in unique_regions:
    # Find voxel indices that belong to the current transformed region
    indices = np.where(transformed_map_voxel == region)[0]
    # Extract RMS values for these voxels
    rms_values = voxel_rms[indices]
    # Sort the RMS values in ascending order
    sorted_rms = np.sort(rms_values)
    region_rms_dict[region] = sorted_rms
    print(f"Region '{region}': {len(sorted_rms)} voxels with RMS values ranging from {sorted_rms[0]:.2f} to {sorted_rms[-1]:.2f}")


# --------------------- Aggregate All RMS Values into One Table ---------------------
#  table with two columns: "x" (region) and "y" (RMS value).
data_rows = []
for region, sorted_rms in region_rms_dict.items():
    for rms in sorted_rms:
        data_rows.append({"x": region, "y": rms})

rms_table = pd.DataFrame(data_rows)
print("\nAggregated RMS Table (first few rows):")
print(rms_table.head())

# save the aggregated table to a CSV file
rms_table.to_csv("aggregated_rms_table.csv", index=False)
# --------------------- (Optional) Plot Sorted RMS Values for Each Region ---------------------
for region, sorted_rms in region_rms_dict.items():
    plt.figure(figsize=(10, 5))
    plt.plot(sorted_rms, marker='o', linestyle='-', linewidth=1.5, alpha=0.8)
    plt.xlabel("Voxel Index (sorted)")
    plt.ylabel("RMS Value")
    plt.title(f"Sorted RMS Values for Transformed Region {region}")
    plt.grid(axis="y", linestyle="--", alpha=0.6)
    plt.tight_layout()
    plt.show()

# --------------------- Plot the Aggregation ---------------------
# Using Seaborn to create a box plot of RMS values by region.
plt.figure(figsize=(16, 6))
sns.boxplot(x="x", y="y", data=rms_table)
plt.xlabel("Transformed Region")
plt.ylabel("RMS Value")
plt.title("Distribution of RMS Values by Transformed Region")
plt.grid(axis="y", linestyle="--", alpha=0.6)
plt.tight_layout()
plt.show()

# Alternatively, if you want a scatter plot of all aggregated RMS values:
plt.figure(figsize=(16, 6))
sns.stripplot(x="x", y="y", data=rms_table, jitter=True, size=4)
plt.xlabel("Transformed Region")
plt.ylabel("RMS Value")
plt.title("Aggregated RMS Values by Transformed Region")
plt.grid(axis="y", linestyle="--", alpha=0.6)
plt.tight_layout()
plt.show()

# --------------------- Aggregate RMS Values by Region ---------------------
# 'rms_table' is your aggregated DataFrame with columns "x" (region) and "y" (RMS value).
#                        x         y
# 0  Cognition Spatial   4  0.519202
# 1  Cognition Spatial   4  0.523675
# 2  Cognition Spatial   4  0.524115
# Group by the "x" column (region) and sum the RMS values for each region.
aggregated_series = rms_table.groupby("x")["y"].sum().sort_values(ascending=False)
print("Summed RMS Values per Region:")
print(aggregated_series)

# --------------------- Plot the Aggregated RMS Sums as a Bar Chart ---------------------
plt.figure(figsize=(16, 6))
sns.barplot(x=aggregated_series.index, y=aggregated_series.values, palette="viridis")
plt.xlabel("Region")
plt.ylabel("Summed RMS Value")
plt.title("Summed RMS Values by Region")
plt.xticks(rotation=45)
plt.grid(axis="y", linestyle="--", alpha=0.6)
plt.tight_layout()
plt.show()

# --------------------- Aggregate Region Time Series into One Overall Time Series ---------------------
# 'region_time_rms' is  a region
# and each value is a 1D NumPy array representing the time series of RMS values for that region.
# Convert the dictionary to a DataFrame (each column corresponds to one region's time series)
region_time_df = pd.DataFrame(region_time_rms)

#  sum the RMS values from all regions at each time step
overall_time_series = region_time_df.sum(axis=1)
print(overall_time_series)
print(sum(overall_time_series))
# --------------------- Plot the Overall Time Series as a Line Plot ---------------------
plt.figure(figsize=(12, 6))
plt.plot(overall_time_series, marker='o', linestyle='-', color='blue')
plt.xlabel("Time Step", fontsize=12)
plt.ylabel("Summed RMS Value", fontsize=12)
plt.title("Overall Summed RMS Time Series Across Regions", fontsize=14)
plt.grid(True, linestyle="--", alpha=0.6)
plt.tight_layout()
plt.show()

"""Group the voxel in each region and then calculate  CV  of the region

"""

#      Region1   Region2  ...  Region12
# 0    0.52      0.51     ...  0.55
# 1    0.53      0.52     ...  0.56
# ...   ...      ...      ...  ...

# --------------------- Compute Overall CV Time Series ---------------------
# For each time step (row), compute the coefficient of variation (CV) across the 12 regions.
overall_cv_time_series = region_time_df.apply(lambda row: row.std() / row.mean(), axis=1)

print("Overall CV Time Series (first few values):")
print(overall_cv_time_series.head())

# --------------------- Plot the Overall CV Time Series as a Line Plot ---------------------
plt.figure(figsize=(16, 6))
plt.plot(overall_cv_time_series.index, overall_cv_time_series.values, marker='o', linestyle='-', color='blue')
plt.xlabel("Time Step", fontsize=12)
plt.ylabel("Coefficient of Variation (CV)", fontsize=12)
plt.title("Overall CV Time Series Across Regions", fontsize=14)
plt.grid(True, linestyle="--", alpha=0.6)
plt.tight_layout()
plt.show()

# --------------------- Plot the CV Values as a Bar Plot ---------------------
plt.figure(figsize=(16, 6))
bar_plot = sns.barplot(x=overall_cv_time_series.index, y=overall_cv_time_series.values, palette="viridis")
plt.xlabel("Time Step", fontsize=12)
plt.ylabel("Coefficient of Variation (CV)", fontsize=12)
plt.title("Overall CV Time Series Across Region", fontsize=14)
plt.xticks(rotation=45)
plt.grid(axis="y", linestyle="--", alpha=0.6)
plt.tight_layout()

"""Compute   CV over time for each region  102 TO 1 LINE

"""

#   Region1    Region2   ...  Region12
# 0  0.52       0.51     ...   0.55
# 1  0.53       0.52     ...   0.56
# ... etc.

# --------------------- Calculate CV for Each Region ---------------------
# The coefficient of variation (CV) is defined as: CV = standard deviation / mean.
cv_series = region_time_df.apply(lambda col: np.std(col) / np.mean(col), axis=0)
print("Coefficient of Variation (CV) for each region:")
print(cv_series)

# --------------------- Plot the CV Values as a Line Plot ---------------------
plt.figure(figsize=(16, 6))
plt.plot(cv_series.index, cv_series.values, marker='o', linestyle='-', color='blue')
plt.xlabel("Region", fontsize=12)
plt.ylabel("Coefficient of Variation (CV)", fontsize=12)
plt.title("Line Plot of CV for Each Region", fontsize=14)
plt.xticks(rotation=45)
plt.grid(True, linestyle="--", alpha=0.6)
plt.tight_layout()
plt.show()

# --------------------- Plot the CV Values as a Bar Plot ---------------------
plt.figure(figsize=(12, 6))
bar_plot = sns.barplot(x=cv_series.index, y=cv_series.values, palette="viridis")
plt.xlabel("Region", fontsize=12)
plt.ylabel("Coefficient of Variation (CV)", fontsize=12)
plt.title("Bar Plot of CV for Each Region", fontsize=14)
plt.xticks(rotation=45)
plt.grid(axis="y", linestyle="--", alpha=0.6)
plt.tight_layout()

# Annotate each bar with its CV value
for p in bar_plot.patches:
    height = p.get_height()
    bar_plot.annotate(f'{height:.2f}',
                      (p.get_x() + p.get_width() / 2., height),
                      ha='center', va='bottom',
                      fontsize=10,
                      color='black',
                      xytext=(0, 5),
                      textcoords='offset points')
plt.show()

videodata=videodata.T
#print CSV column names to verify expected structure
print("Region names CSV columns:", region_data.columns)
if "Cerebra_ID" not in region_data.columns or "Multiple resource theory ID & Name" not in region_data.columns:
    raise ValueError("CSV file must contain 'Cerebra_ID' and 'Multiple resource theory ID & Name' columns.")

# Create a dictionary mapping from the original 62 regions to the new 12-region labels/IDs
mapping_62_to_12 = region_data.set_index("Cerebra_ID")["Multiple resource theory ID & Name"].to_dict()

# Transform map_voxel using vectorized mapping so that each voxel now has a 12-region label.
transformed_map_voxel = np.vectorize(mapping_62_to_12.get)(map_voxel)

# --------------------- Compute CV Over Time for Each of the 12 Regions ---------------------
# 'videodata' should be transposed so that its shape is (time, voxels)
unique_regions = np.unique(transformed_map_voxel)
region_time_cv = {}

for region in unique_regions:
    # Get boolean indices for voxels belonging to the current transformed region.
    region_indices = (transformed_map_voxel == region)

    # Extract voxel data for these voxels; region_data shape: (time, number_of_voxels_in_region)
    region_data = videodata[:, region_indices]

    # Compute the mean and standard deviation across the voxels (axis=1: for each time step)
    mean_values = np.mean(region_data, axis=1)
    std_values = np.std(region_data, axis=1)

    # Compute CV for each time step, avoiding division by zero.
    cv_values = np.where(mean_values != 0, std_values / mean_values, 0)

    region_time_cv[region] = cv_values
    print(f"Region '{region}': {len(cv_values)} time points with CV values ranging from {cv_values.min():.2f} to {cv_values.max():.2f}")

# --------------------- Aggregate the 12 CV Time Series into One Overall Time Series ---------------------
# Convert the dictionary to a DataFrame (each column corresponds to one region's CV time series)
region_time_df = pd.DataFrame(region_time_cv)
print("Region CV Time Series DataFrame (first few rows):")
print(region_time_df.head())

# For each time step (row), sum the CV values from all 12 regions.
overall_cv_time_series = region_time_df.sum(axis=1)
print("Overall Summed CV Time Series (first few values):")
print(overall_cv_time_series.head())
print("Total sum over all time steps:", overall_cv_time_series.sum())

# --------------------- Plot the Overall CV Time Series as a Line Plot ---------------------
plt.figure(figsize=(12, 6))
plt.plot(overall_cv_time_series.index, overall_cv_time_series.values, marker='o', linestyle='-', color='blue')
plt.xlabel("Time Step", fontsize=12)
plt.ylabel("Summed CV Value", fontsize=12)
plt.title("Overall Summed CV Time Series Across Regions", fontsize=14)
plt.grid(True, linestyle="--", alpha=0.6)
plt.tight_layout()
plt.show()


# --------------------- Plot the CV Values as a Line Plot ---------------------
plt.figure(figsize=(16, 6))
plt.plot(overall_cv_time_series.index, overall_cv_time_series.values, marker='o', linestyle='-', color='blue')
plt.xlabel("Time Step", fontsize=12)
plt.ylabel("Summed CV Value", fontsize=12)
plt.title("Overall Summed CV Time Series Across Regions", fontsize=14)
plt.xticks(rotation=45)
plt.grid(True, linestyle="--", alpha=0.6)
plt.tight_layout()
plt.show()

"""  # Compute RMS over time for each region"""

videodata=videodata.T
#print CSV column names to verify expected structure
print("Region names CSV columns:", region_data.columns)
#Check that the required columns exist in the CSV
if "Cerebra_ID" not in region_data.columns or "Multiple resource theory ID & Name" not in region_data.columns:
   raise ValueError("CSV file must contain 'Cerebra_ID' and 'Multiple resource theory ID & Name' columns.")

# Create a dictionary mapping from the original region IDs (62 regions) to the new 12-region IDs/names
mapping_62_to_12 = region_data.set_index("Cerebra_ID")["Multiple resource theory ID & Name"].to_dict()

# Transform map_voxel: for each voxel (originally with a 62-region ID), get its corresponding 12-region ID/name
transformed_map_voxel = np.vectorize(mapping_62_to_12.get)(map_voxel)

# --------------------- Compute RMS Over Time for Each of the 12 Regions ---------------------
unique_transformed_regions = np.unique(transformed_map_voxel)
region_time_rms = {}

for region in unique_transformed_regions:
    # Boolean index for voxels that belong to the current transformed region
    region_indices = transformed_map_voxel == region

    # Extract voxel data for the region (videodata shape: [time, voxels_in_region])
    region_data = videodata[:, region_indices]

    # Compute RMS over time for each time step:
    rms_values = np.sqrt(np.mean(region_data**2, axis=1))

    region_time_rms[region] = rms_values
print(region_time_rms)
#print(sum (int(region_time_rms)))
# --------------------- Plot RMS Over Time for Each Transformed Region ---------------------
for region, time_rms in region_time_rms.items():
    plt.figure(figsize=(10, 5))
    plt.plot(
        range(len(time_rms)),  # X-axis: Time steps
        time_rms,              # Y-axis: RMS values over time
        marker='o', linestyle='-', linewidth=1.5, alpha=0.8, label=f"Region {region}"
    )
    plt.xlabel("Time Step")
    plt.ylabel("RMS Value")
    plt.title(f"RMS Over Time for Transformed Region {region}")
    plt.grid(axis="y", linestyle="--", alpha=0.6)
    plt.legend()
    plt.tight_layout()
    plt.show()

"""# Compute mean over time for each of the 12 regions"""

if "Cerebra_ID" not in region_mapping.columns or "Multiple resource theory ID & Name" not in region_mapping.columns:
    raise ValueError("CSV file must contain 'Cerebra_ID' and 'Multiple resource theory ID & Name' columns.")

# Create a mapping from 62 regions to 12 regions
region_mapping = region_mapping.set_index("Cerebra_ID")["Multiple resource theory ID & Name"].to_dict()

# Transform map_voxel to use the 12-region IDs
transformed_map_voxel = np.vectorize(region_mapping.get)(map_voxel)

# Compute mean over time for each of the 12 regions
unique_transformed_regions = np.unique(transformed_map_voxel)
region_time_means = {}

for transformed_region in unique_transformed_regions:
    # Get indices for all voxels in the transformed region
    region_indices = transformed_map_voxel == transformed_region
    # Compute the mean over time for voxels in this region
    region_time_means[transformed_region] = np.mean(videodata[:, region_indices], axis=1)

# Plot mean over time for each transformed region
for region, time_means in region_time_means.items():
    plt.figure(figsize=(10, 5))
    plt.plot(
        range(len(time_means)),  # X-axis: Time steps
        time_means,             # Y-axis: Mean values over time
        marker='o', linestyle='-', linewidth=1.5, alpha=0.8, label=f"Region {region}"
    )
    plt.xlabel("Time Step")
    plt.ylabel("Mean Measure")
    plt.title(f"Mean Measures Over Time for Transformed Region {region}")
    plt.grid(axis="y", linestyle="--", alpha=0.6)
    plt.legend()
    plt.tight_layout()
    plt.show()

"""# Visualize the Brain 3d  and 2d**"""

# Commented out IPython magic to ensure Python compatibility.
# %cd Cerebra_aatlas_python
!pip install .

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from cerebra_atlas_python.plotting.plotting_3d import Plots3D

# Clone and install the repository first!
# !git clone https://github.com/kdotdot/cerebra_atlas_python.git
# %cd cerebra_atlas_python
# !pip install .

# Load video eLORETA data
try:
    videodata = np.load("/content/drive/MyDrive/Data TU PHD DUBLIN/First data/video1_eLORETA.npy")  # Shape: (time, 11250)
    print(f"Loaded video data with shape: {videodata.shape}")
except FileNotFoundError:
    print("Error: video1_eLORETA.npy file not found. Please check the file path.")
    raise
videodata = videodata.T

# Load the region mapping
try:
    map_voxel = np.load("/content/drive/MyDrive/Data TU PHD DUBLIN/LABEL DETAILS/map_voxel.npy")  # Shape: (31553,)
    print(f"Loaded map_voxel data with shape: {map_voxel.shape}")
except FileNotFoundError:
    print("Error: map_voxel.npy file not found. Please check the file path.")
    raise

# Align map_voxel with videodata
if len(map_voxel) != videodata.shape[1]:
    map_voxel = map_voxel[:videodata.shape[1]]  # Trim map_voxel to match videodata's voxels
    print(f"Aligned map_voxel shape: {map_voxel.shape}")

# Load region names from a CSV file
try:
    region_names = pd.read_csv("/content/drive/MyDrive/Data TU PHD DUBLIN/LABEL DETAILS/results_per_region (MY 101124).csv")
    print(f"Loaded region names with shape: {region_names.shape}")
except FileNotFoundError:
    print("Error: results_per_region (MY 101124).csv file not found. Please check the file path.")
    raise

# Debugging CSV column names
print("Region names CSV columns:", region_names.columns)

# Ensure region names CSV contains 'Cerebra_ID' and 'Region_name' columns
if "Cerebra_ID" not in region_names.columns or "Region_name" not in region_names.columns:
    raise ValueError("CSV file must contain 'Cerebra_ID' and 'Region_name' columns.")

# Create a dictionary for region ID to region name mapping
region_id_to_name = region_names.set_index("Cerebra_ID")["Region_name"].to_dict()

# Compute mean over time for each region
unique_regions = np.unique(map_voxel)
region_time_means = {}

for region in unique_regions:
    # Get indices for all voxels in the region
    region_indices = map_voxel == region
    # Compute the mean over time for voxels in this region
    region_time_means[region] = np.mean(videodata[:, region_indices], axis=1)

# Prepare data for 3D visualization
voxel_colors = np.zeros((map_voxel.shape[0], 3))  # Initialize voxel colors as black

for region in unique_regions:
    region_indices = map_voxel == region
    mean_intensity = np.mean(region_time_means[region])  # Mean intensity for the region
    color_value = plt.cm.viridis(mean_intensity / max(region_time_means[region]))[:3]  # Normalize and use colormap
    voxel_colors[region_indices] = color_value  # Assign colors to voxels



#Plots3D.plot_data_3d(plot_data=plot_data)
# Visualize 3D brain with Plots3D
try:
    Plots3D.plot_data_3d(region_time_means)
    print("3D visualization generated successfully.")
except Exception as e:
    print(f"Error generating 3D visualization: {e}")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.spatial import Delaunay
from cerebra_atlas_python.plotting.plotting_3d import Plots3D

# Load video eLORETA data
try:
    videodata = np.load("/content/drive/MyDrive/Data TU PHD DUBLIN/First data/video1_eLORETA.npy")  # Shape: (31553, 11250)
    print(f"Loaded video data with shape: {videodata.shape}")
except FileNotFoundError:
    print("Error: video1_eLORETA.npy file not found. Please check the file path.")
    raise

videodata = videodata.T  # Transpose to match dimensions if necessary

# Load the region mapping
try:
    map_voxel = np.load("/content/drive/MyDrive/Data TU PHD DUBLIN/LABEL DETAILS/map_voxel.npy")  # Shape: (31553,)
    print(f"Loaded map_voxel data with shape: {map_voxel.shape}")
except FileNotFoundError:
    print("Error: map_voxel.npy file not found. Please check the file path.")
    raise

# Align map_voxel with videodata
if len(map_voxel) != videodata.shape[1]:
    map_voxel = map_voxel[:videodata.shape[1]]  # Trim map_voxel to match videodata's voxels
    print(f"Aligned map_voxel shape: {map_voxel.shape}")

# Generate random 3D positions for src_space_points (replace with actual positions if available)
voxel_positions = np.random.rand(map_voxel.shape[0], 3) * 255  # Random positions in a 3D space

# Generate triangles for visualization
tri = Delaunay(voxel_positions[:, :2])  # Triangulate based on x, y coordinates
bem_triangles = tri.simplices  # Triangle connectivity

# Compute voxel colors based on mean intensity
voxel_colors = np.zeros((map_voxel.shape[0], 3))  # Initialize colors as black
for region in np.unique(map_voxel):
    region_indices = map_voxel == region
    mean_intensity = np.mean(videodata[:, region_indices])  # Mean intensity for the region
    color_value = plt.cm.viridis(mean_intensity / np.max(videodata))[:3]  # Normalize intensity
    voxel_colors[region_indices] = color_value

# Generate bem_normals_vox_ras (dummy for now)
bem_normals_vox_ras = np.zeros_like(voxel_positions)  # Replace with actual normals if available

# Define optional parameters
info = {}  # Replace with real info if available
fiducials = []  # Replace with real fiducials if available
rotate_mode = None  # Adjust rotation mode as needed
save_path = None  # Path to save the visualization (set None to disable saving)
# Define background color
background_color = [1, 1, 1]  # White color

# Prepare plot_data for visualization
plot_data = {
    "src_space_points": voxel_positions,       # 3D positions of voxels
    "colors": voxel_colors,                    # RGB colors for voxels
    "cerebra_volume": videodata.mean(axis=0),  # Mean intensity for each voxel
    "src_space_labels": map_voxel,             # Region mapping for voxels
    "cortical_color": voxel_colors,            # Color map for cortical regions
    "bem_colors": voxel_colors,                # Color map for regions
    "bem_vertices_vox_ras": voxel_positions,   # 3D coordinates in RAS space
    "bem_triangles": bem_triangles,            # Triangle connectivity
    "bem_normals_vox_ras": bem_normals_vox_ras,  # Normals for visualization
    "info": info,                              # Info dictionary
    "fiducials": fiducials,                    # Fiducial data
    "rotate_mode": rotate_mode,                # Rotation settings
    "save_path": save_path,                    # Path to save the visualization
    "background_color":background_color,
}
# Define background color
background_color = [1, 1, 1]  # White color

# Prepare plot_data for visualization
plot_data = {
    # ... (other data remains the same)
    "background_color": background_color,  # Include background color in plot_data
}

# Initialize Plots3D instance
plots_3d = Plots3D()
# Call show method to initialize the visualization
plots_3d.show()
plots_3d.vis.get_render_option().background_color = [1, 1, 1]
# Visualize 3D brain with Plots3D
try:
    plots_3d.plot_data_3d(plot_data=plot_data)
    print("3D visualization generated successfully.")
except Exception as e:
    print(f"Error generating 3D visualization: {e}")







